[
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html",
    "title": "Take-home Exercise 3",
    "section": "",
    "text": "In this exercise, we’ll apply visual interactivity and visualizing uncertainty methods to validate either of the claims on future climate projection in the infographic above that:\n\nDaily mean temperature are projected to increase by 1.4 to 4.6 C, and\nThe contrast between the wet months (November to January) and dry month (February and June to September) is likely to be more pronounced.\n\nWe will choose to validate the first claim that “Daily mean temperature are projected to increase by 1.4 to 4.6 C”.\nTo do so, we will use the daily temperature records of June of the years 1983, 1993, 2003, 2013 and 2023 collected at the Changi climate station, to create an analytics-driven data visualisation.\nWe will apply appropriate interactive techniques to enhance the user experience in data discovery and/or visual story-telling."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#load-packages",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#load-packages",
    "title": "Take-home Exercise 3",
    "section": "2.1 Load packages",
    "text": "2.1 Load packages\nFirst, we load packages required:\n\nreadr: for reading in CSV files\ndplyr: for manipulating, concatenating dataframes\npurrr: for handling lists and functional programming\nnaniar: for using miss_vis() function to check data for missing values\n\n\npacman::p_load(readr, dplyr, purrr, naniar,\n               tidyverse, DT, ggplot2, patchwork, \n               ggridges, ggdist, ggstatsplot,\n               plotly, ggiraph, DT, ggdist, \n               ungeviz, crosstalk, \n               viridis)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#import-data",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#import-data",
    "title": "Take-home Exercise 3",
    "section": "3.1 Import data",
    "text": "3.1 Import data\n\n\n\n\n\n\nNote\n\n\n\nThis section includes code on how data was retrieved. To skip ahead to the preparation of the retrieved data, you can skip ahead to Section 3.2.\n\n\nNext, we load the Climate Historical Daily Records for the Changi weather station, for the five years 1983, 1993, 2003, 2013 and 2023. The data for each month of the five years was downloaded using a python script. The data, in the form of .CSV files, will be imported as a list of CSV files, then compiled into a single dataframe using R.\nFirst, we read in all .CSV files into a list object of dataframes. As the .CSV files may be ended with either UTF-8 or ISO-8859-1 encoding, we will create a function to read in the .CSV files that encompasses both types of encoding.\nNext, we identify columns with inconsistent data types across all the dataframes:\n\n# Define a function to get column types of a data frame\nget_column_types &lt;- function(df) {\n  sapply(df, class)\n}\n\n# Get column types for each data frame\ncolumnTypesList &lt;- map(all_data, get_column_types)\n\n# Identify columns with different types across data frames\nuniqueTypes &lt;- unique(unlist(columnTypesList))\ncolumnsWithDifferentTypes &lt;- list()\n\nfor (typeName in uniqueTypes) {\n  columnsOfType &lt;- names(Filter(function(x) typeName %in% x, columnTypesList[[1]]))\n  for (colName in columnsOfType) {\n    typesFound &lt;- unique(sapply(columnTypesList, `[`, colName))\n    if (length(typesFound) &gt; 1) {\n      columnsWithDifferentTypes[[colName]] &lt;- typesFound\n    }\n  }\n}\n\n# Display columns with different types\ncolumnsWithDifferentTypes\n\nFrom the code chunk above, we observe that:\n\nThe ‘Mean Wind Speed (km/h)’ and ‘Max Wind Speed (km/h)’ columns exhibit data type inconsistencies.\n\nThese columns will be uniformly converted to numeric type.\n\nThe columns ‘Highest 30 Min Rainfall (mm)’, ‘Highest 60 Min Rainfall (mm)’, and ‘Highest 120 Min Rainfall (mm)’ are type character in some dataframes, and NA type in other dataframes.\n\nThis suggests that these columns are present in some dataframes but absent in others. We will address the handling of these columns later on.\n\n\nWe hence convert the ‘Mean Wind Speed (km/h)’ and ‘Max Wind Speed (km/h)’ columns to numeric type:\n\nall_data &lt;- map(all_data, ~ .x %&gt;%\n  mutate(\n    `Mean Wind Speed (km/h)` = as.numeric(as.character(`Mean Wind Speed (km/h)`)),\n    `Max Wind Speed (km/h)` = as.numeric(as.character(`Max Wind Speed (km/h)`))\n  )\n)\n\nThe list of dataframes is then combined into a single dataframe, weather_data:\n\nweather_data &lt;- bind_rows(all_data)\n\nWe will save this dataframe as an rds object for faster loading of data in the future.\n\nwrite_rds(weather_data, \"data/weather_data.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#sec-loaddataset",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#sec-loaddataset",
    "title": "Take-home Exercise 3",
    "section": "3.2 Load dataset",
    "text": "3.2 Load dataset\nTo load this file in the future, follow the code in the code chunk below:\n\nweather_data &lt;- read_rds(\"data/weather_data.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#check-data-health",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#check-data-health",
    "title": "Take-home Exercise 3",
    "section": "3.3 Check data health",
    "text": "3.3 Check data health\nNow that we have a single dataframe, we first check the health of the dataframe by:\n\nusing glimpse() to look at the structure of the dataframe, data types of the columns, and some values of the dataframe,\nusing datatable() from the DT package to view the dataframe more interactively,\nusing duplicate() to check the dataframe for any duplicated entries using duplicate(), and\nusing vis_miss() to check the state of missing values in the dataset.\n\n\nglimpse()datatable()duplicated()vis_miss()\n\n\n\nglimpse(weather_data)\n\nRows: 1,825\nColumns: 16\n$ Station                         &lt;chr&gt; \"Changi\", \"Changi\", \"Changi\", \"Changi\"…\n$ Year                            &lt;dbl&gt; 1983, 1983, 1983, 1983, 1983, 1983, 19…\n$ Month                           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Day                             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,…\n$ `Daily Rainfall Total (mm)`     &lt;dbl&gt; 0.3, 0.4, 2.9, 0.0, 0.0, 0.0, 22.3, 0.…\n$ `Highest 30 Min Rainfall (mm)`  &lt;chr&gt; \"\\u0097\", \"\\u0097\", \"\\u0097\", \"\\u0097\"…\n$ `Highest 60 Min Rainfall (mm)`  &lt;chr&gt; \"\\u0097\", \"\\u0097\", \"\\u0097\", \"\\u0097\"…\n$ `Highest 120 Min Rainfall (mm)` &lt;chr&gt; \"\\u0097\", \"\\u0097\", \"\\u0097\", \"\\u0097\"…\n$ `Mean Temperature (°C)`         &lt;dbl&gt; 26.5, 26.8, 27.0, 27.3, 27.1, 27.2, 26…\n$ `Maximum Temperature (°C)`      &lt;dbl&gt; 28.7, 30.6, 31.3, 30.8, 31.8, 32.1, 31…\n$ `Minimum Temperature (°C)`      &lt;dbl&gt; 25.1, 24.8, 24.5, 25.0, 23.7, 23.7, 24…\n$ `Mean Wind Speed (km/h)`        &lt;dbl&gt; 5.5, 9.4, 10.7, 12.6, 10.3, 8.4, 9.8, …\n$ `Max Wind Speed (km/h)`         &lt;dbl&gt; 29.9, 43.2, 42.8, 42.1, 34.6, 32.4, 38…\n$ `Highest 30 min Rainfall (mm)`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Highest 60 min Rainfall (mm)`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Highest 120 min Rainfall (mm)` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\n\n\ndatatable(weather_data, \n          class= \"compact\",\n          rownames = FALSE,\n          width=\"100%\", \n          options = list(pageLength = 10,scrollX=T))\n\n\n\n\n\n\n\n\nweather_data[duplicated(weather_data),]\n\n# A tibble: 0 × 16\n# ℹ 16 variables: Station &lt;chr&gt;, Year &lt;dbl&gt;, Month &lt;dbl&gt;, Day &lt;dbl&gt;,\n#   Daily Rainfall Total (mm) &lt;dbl&gt;, Highest 30 Min Rainfall (mm) &lt;chr&gt;,\n#   Highest 60 Min Rainfall (mm) &lt;chr&gt;, Highest 120 Min Rainfall (mm) &lt;chr&gt;,\n#   Mean Temperature (°C) &lt;dbl&gt;, Maximum Temperature (°C) &lt;dbl&gt;,\n#   Minimum Temperature (°C) &lt;dbl&gt;, Mean Wind Speed (km/h) &lt;dbl&gt;,\n#   Max Wind Speed (km/h) &lt;dbl&gt;, Highest 30 min Rainfall (mm) &lt;dbl&gt;,\n#   Highest 60 min Rainfall (mm) &lt;dbl&gt;, Highest 120 min Rainfall (mm) &lt;dbl&gt;\n\n\n\n\n\nvis_miss(weather_data)\n\n\n\n\n\n\n\n\n\n\n\nFrom the above, we observe these data quality issues and actionable insights:\n\nThe glimpse function shows three columns ‘Year’, ‘Month’, ‘Day’ representing the date of the record.\n\nThese columns can be used to create a ‘Date’ column of date type that would make visualizing the data for different time periods easier.\n\nThe glimpse function shows the presence of character values \"\\u0097\" in the columns ‘Highest 30 Min Rainfall (mm)’, ‘Highest 60 Min Rainfall (mm)’, and ‘Highest 120 Min Rainfall (mm)’.\n\nThese non-numeric placeholders represent missing values and should be converted to NA.\nThese columns should also be of type numeric instead of character.\n\nThe glimpse function also shows there is a possible naming inconsistency of the ‘Highest 30 Min Rainfall (mm)’, ‘Highest 60 Min Rainfall (mm)’, and ‘Highest 120 Min Rainfall (mm)’ character type columns, where similar columns ‘Highest 30 min Rainfall (mm)’, ‘Highest 60 min Rainfall (mm)’, and ‘Highest 120 min Rainfall (mm)’ of numeric type exist. The vis_miss() function confirms this naming inconsistency.\n\nThis can be resolved by merging the information in the character and double type columns."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#clean-data",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#clean-data",
    "title": "Take-home Exercise 3",
    "section": "3.4 Clean data",
    "text": "3.4 Clean data\n\n# First, create a new column Date\nweather_data &lt;- weather_data %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, Day, sep = \"-\")))\n\n# Next, convert character values \"\\u0097\" to NA.\nweather_data &lt;- weather_data %&gt;%\n  mutate(\n    `Highest 30 Min Rainfall (mm)` = na_if(`Highest 30 Min Rainfall (mm)`, \"\\u0097\"),\n    `Highest 60 Min Rainfall (mm)` = na_if(`Highest 60 Min Rainfall (mm)`, \"\\u0097\"),\n    `Highest 120 Min Rainfall (mm)` = na_if(`Highest 120 Min Rainfall (mm)`, \"\\u0097\")\n  )\n\n# Next, convert these columns to numeric type.\nweather_data &lt;- weather_data %&gt;%\n  mutate(\n    `Highest 30 Min Rainfall (mm)` = as.numeric(`Highest 30 Min Rainfall (mm)`),\n    `Highest 60 Min Rainfall (mm)` = as.numeric(`Highest 60 Min Rainfall (mm)`),\n    `Highest 120 Min Rainfall (mm)` = as.numeric(`Highest 120 Min Rainfall (mm)`)\n  )\n\n# Lastly, merge duplicate columns on the condition that the originally numeric column (e.g. Highest 30 min Rainfall (mm)) is not NA value.\n\nweather_data &lt;- weather_data %&gt;%\n  mutate(\n    `Highest 30 Min Rainfall (mm)` = coalesce(`Highest 30 min Rainfall (mm)`, `Highest 30 Min Rainfall (mm)`),\n    `Highest 60 Min Rainfall (mm)` = coalesce(`Highest 60 min Rainfall (mm)`, `Highest 60 Min Rainfall (mm)`),\n    `Highest 120 Min Rainfall (mm)` = coalesce(`Highest 120 min Rainfall (mm)`, `Highest 120 Min Rainfall (mm)`)\n  ) %&gt;%\n  # Optional: Remove the source columns if they are no longer needed\n  select(\n    -`Highest 30 min Rainfall (mm)`,\n    -`Highest 60 min Rainfall (mm)`,\n    -`Highest 120 min Rainfall (mm)`\n  )\n\nWe then check the health of the data again:\n\nglimpse()datatable()vis_miss()\n\n\n\nglimpse(weather_data)\n\nRows: 1,825\nColumns: 14\n$ Station                         &lt;chr&gt; \"Changi\", \"Changi\", \"Changi\", \"Changi\"…\n$ Year                            &lt;dbl&gt; 1983, 1983, 1983, 1983, 1983, 1983, 19…\n$ Month                           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Day                             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,…\n$ `Daily Rainfall Total (mm)`     &lt;dbl&gt; 0.3, 0.4, 2.9, 0.0, 0.0, 0.0, 22.3, 0.…\n$ `Highest 30 Min Rainfall (mm)`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Highest 60 Min Rainfall (mm)`  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Highest 120 Min Rainfall (mm)` &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Mean Temperature (°C)`         &lt;dbl&gt; 26.5, 26.8, 27.0, 27.3, 27.1, 27.2, 26…\n$ `Maximum Temperature (°C)`      &lt;dbl&gt; 28.7, 30.6, 31.3, 30.8, 31.8, 32.1, 31…\n$ `Minimum Temperature (°C)`      &lt;dbl&gt; 25.1, 24.8, 24.5, 25.0, 23.7, 23.7, 24…\n$ `Mean Wind Speed (km/h)`        &lt;dbl&gt; 5.5, 9.4, 10.7, 12.6, 10.3, 8.4, 9.8, …\n$ `Max Wind Speed (km/h)`         &lt;dbl&gt; 29.9, 43.2, 42.8, 42.1, 34.6, 32.4, 38…\n$ Date                            &lt;date&gt; 1983-01-01, 1983-01-02, 1983-01-03, 1…\n\n\n\n\n\ndatatable(weather_data, \n          class= \"compact\",\n          rownames = FALSE,\n          width=\"100%\", \n          options = list(pageLength = 10,scrollX=T))\n\n\n\n\n\n\n\n\nvis_miss(weather_data)\n\n\n\n\n\n\n\n\n\n\n\nInsights:\n\nThe dataset is now clean. It has 1825 rows and 14 columns of correct data types.\nThe missing values in the data for the three columns ‘Highest 30 Min Rainfall (mm)’, ‘Highest 60 Min Rainfall (mm)’, and ‘Highest 120 Min Rainfall (mm)’ correspond with information that these datapoints were not collected before 2014. Hence, we will leave the values for the years before 2014 as NA."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#rename-columns",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#rename-columns",
    "title": "Take-home Exercise 3",
    "section": "3.5 Rename columns",
    "text": "3.5 Rename columns\nWe will rename the following columns for easier handling while plotting:\n\nweather_data &lt;- weather_data %&gt;%\n  rename(\n    Mean_Temperature = `Mean Temperature (°C)`,\n    Max_Temperature = `Maximum Temperature (°C)`,\n    Min_Temperature = `Minimum Temperature (°C)`\n  )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#select-temperature-data-for-june",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#select-temperature-data-for-june",
    "title": "Take-home Exercise 3",
    "section": "3.6 Select temperature data for June",
    "text": "3.6 Select temperature data for June\nAs we have mentioned that we will be focusing on the month of June for this exercise, we will extract the data for this month. We will exclude the columns on rainfall and wind speed as well.\n\njune_weather_data &lt;- weather_data %&gt;%\n  filter(Month == 6) %&gt;%\n  select(Station, Year, Month, Day, Date, Mean_Temperature, Min_Temperature, Max_Temperature)\n\nWe can now proceed with the next steps.\n\ndatatable(june_weather_data, \n          class= \"compact\",\n          rownames = FALSE,\n          width=\"100%\", \n          options = list(pageLength = 10,scrollX=T))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#looking-at-mean_temperature",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#looking-at-mean_temperature",
    "title": "Take-home Exercise 3",
    "section": "4.1 Looking at ‘Mean_Temperature’",
    "text": "4.1 Looking at ‘Mean_Temperature’\n\n4.1.1 Distribution of values\nWe first observe the distributions of the ‘Mean_Temperature’ values in June for the five years, with a static ridge line plot. This plot will allow us to also visually compare the values across the years.\nWhile the ridge line plot gives a good view of distribution shape for the years, it lacks details such as median, and interquartiles.\nTo make the visualization of distribution more informative for a user, we choose to use plot_ly to plot an interactive violin plot and boxplot. The addition of a boxplot adds statistical detail to the distributions being plotted. The summary statistics will appear on hover as a tooltip, giving a user a handy view of some details on the data. Moreover, we will include the mean and median lines. On hover, a line is shown across the plots enabling for easy yet accurate comparison of mean and median values across the years.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Ridge plot\nridge_plot &lt;- ggplot(june_weather_data, aes(x = Mean_Temperature, y=factor(Year), fill = stat(x))) +\n                       geom_density_ridges_gradient(scale =2,rel_min_height = 0.01, gradient_lwd = 1.) +\n                       scale_y_discrete(limits = unique(rev(june_weather_data$Year)))+\n                       scale_fill_viridis_c(name = \"°C\", option = \"C\") +\n                       labs(title = 'June temperature profile',\n                        subtitle = 'Distribution of daily mean temperature recorded in June across years',\n                        x = \"Temperature (°C)\",\n                        y = \"Year\") +\n                        theme_ridges(font_size = 13, grid = TRUE) +\n  theme(plot.title = element_text(size = 14),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n# Violin with box plot\nviolin_plot &lt;- plot_ly(data = june_weather_data, x = ~Year, y = ~Mean_Temperature, type = 'violin', \n               split = ~Year, \n               box = list(visible = T),\n               meanline = list(visible = T)) %&gt;% # Adds a box plot inside the violin for summary statistics\n  layout(title = list(text = \"&lt;b&gt;Distribution of Mean Temperatures in June Across Years&lt;/b&gt;\", \n                                                   font = list(family = \"Arial\", size = 14),\n                                                   xanchor = \"center\", xref = \"paper\",\n                                                   yanchor = \"middle\", yref = \"paper\"),\n                      yaxis = list(title = list(text = \"Temperature (°C)\", \n                                                font = list(family = \"Arial\", size = 12)) ),\n                      xaxis = list(title = list(text = \"Year\", \n                                                font = list(family = \"Arial\", size = 12)) ))\n# Display plots\nridge_plot\nviolin_plot\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nThe rightward shift of peaks over the years suggests an increase in average temperature.\nDistribution shapes vary, with some resembling normal distributions (1983, 2003, 2023) and others showing non-normal distributions or multiple peaks (1993, 2013).\nThe distribution for years 2003, 2013, and 2023 exhibit more negative skewness compared to 1983 and 1993, supported by higher median than mean values, indicating a higher frequency of hotter days in those years.\nThe left tails of the distributions, are not extending as far left in more recent years as they do in earlier years. This suggests a decrease in the occurrence of lower temperature extremes, which could mean fewer cooling days in June in recent years.\nThe right tails have remained similar in position across the five years, indicating that the magnitude of warmer temperature extremes in June have stayed relatively consistent over the five years.\nFrom the violin plot, median temperatures show a slight increase over the years, but with a dip in 2013 and similarity between 2003 and 2023.\nThe interquartile ranges do not show a consistent trend in terms of becoming wider or narrower, implying that the variability around the median does not follow a clear pattern over the years.\nThe lower tails show an increasing trend. Similar to the ridge line plot, this indicates fewer cooler days in recent years.\nHowever, notably the year 2003 had the most extreme low mean temperature values.\n\n\n\n\n\n4.1.2 Visualizing uncertainty of point estimate, the mean\nNext, we want to explore the estimated mean of ‘Mean_Temperature’ values for June of each year, and visualize the uncertainty associated with that estimate.\n\n4.1.2.1 Derive summary statistics\nFirstly, we derive the necessary summary statistics:\n\n\nCode\nmy_sum &lt;- june_weather_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise( # used to compute the count of observations, mean, standard deviation\n    n=n(),\n    mean=mean(Mean_Temperature),\n    sd=sd(Mean_Temperature)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1)) %&gt;% # used to derive standard error of mean\nmutate(\n    ci_lower = mean - (1.96 * se),\n    ci_upper = mean + (1.96 * se)\n  )\n\n\n\n\n4.1.2.2 Plotting standard error bars and confidence interval\nNow we plot the standard error bars of the mean of the Mean_Temperature values by Year as shown below. Again, we choose plot_ly to make this plot interactive - on hover, a tooltip shows the year, point estimate of mean, and the standard error.\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code to plot side by side sub plots\nmy_sum$Year &lt;- as.numeric(as.character(my_sum$Year))\n\nmy_sum_d = highlight_key(my_sum)\n\n# Create the plot for mean with standard error\nfig1 &lt;- plot_ly(data = my_sum_d, x = ~Year, y = ~mean, type = 'scatter', mode = 'lines+markers',\n        error_y = ~list(array = se,\n                        color = '#000000')) %&gt;%\n  layout(title = list(text = '&lt;b&gt;Mean of Mean_Temperature values \\n with Standard Error&lt;/b&gt;', font = list(size = 14), xanchor = \"center\", xref = \"paper\"),\n         xaxis = list(title = list(text = 'Year', font = list(size = 11))),\n         yaxis = list(title = list(text = 'Temperature (°C)', font = list(size = 11))))\n\n\n# Add the second trace for mean with confidence intervals\nfig2 &lt;- plot_ly(data = my_sum_d, x = ~Year, y = ~mean, type = 'scatter', mode = 'lines+markers',\n               error_y = ~list(\n                 array = my_sum$ci_upper - my_sum$mean,\n                 arrayminus = my_sum$mean - my_sum$ci_lower,\n                 color = '#000000'\n               ),\n               line = list(color = 'blue')) %&gt;%\n  layout(title = list(text = '&lt;b&gt;Mean of Mean_Temperature values \\n with 95% Confidence Intervals&lt;/b&gt;', font = list(size = 14), xanchor = \"center\", xref = \"paper\"),\n         xaxis = list(title = list(text = 'Year', font = list(size = 11))), \n         yaxis = list(title = list(text = 'Temperature (°C)', font = list(size = 11))))\nbscols(list(fig1,\n       datatable(my_sum_d, \n                 width = \"200%\",\n                 colnames = c(\"Year\", \"Number of Days\", \"Mean\", \"Standard Deviation\", \"Standard Error\", \"Confidence Interval (Lower)\", \"Confidence Interval (Higher)\"),\n                 rownames = FALSE,\n                 height = 200)), \n        fig2)\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nFrom 1983 to 1993 to 2003, the mean showed significant and consistent increase, indicating the increasing trend in temperature.\nHowever, from 2003, this increase appeared to level off, suggesting that there was a decrease in rate of temperature change.\nThis is accompanied by the overlapping confidence intervals, which indicates the differences between the estimates for 2003, 2013 and 2023 are not significant.\nThe length of the error bars across the years are similar, indicating similar precision in estimating the mean. However, this could also be due to small sample size of only 30 points per year.\n\n\n\n\n\n\n4.1.3 Statistical testing for difference amongst years\nTo determine whether the mean daily temperatures in June have varied significantly over the span of five years, we will conduct a statistical test. We earlier observed that some of the data is non-normal, hence we will use a non-parametric test. Since we have multiple groups, it will be a Kruskal-Wallis one-way ANOVA test.\nThe hypotheses would be:\n\nNull Hypothesis (H0): There is no difference in the median values among the five years. This means that any observed difference in among the years is due to random chance.\nAlternative Hypothesis (H1): There is a difference in the median values among the five years. This means that the observed difference is not due to chance, and there is a true difference among the years.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = june_weather_data,\n  x = Year, \n  y = Mean_Temperature,\n  type = \"np\", # non-parametric text that compares median, \n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\", # displays only significant pairwise comparisons\n  p.adjust.method = \"fdr\", \n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nThe p-value is below 0.05. Hence we have sufficient evidence to reject the Null Hypothesis. We can conclude that there is evidence to indicate that there is a difference in the median temperatures for June across the years.\nHowever, closer inspection of pairwise differences reveals that no 10-year intervals (for example, comparing 1983 to 1993, comparing 2003 to 2013 etc.) has a p-value below 0.05. This indicates that there is not statistical evidence that the mean daily temperature for June is actually different for the 10-year intervals.\n\n\n\nIt may not be accurate to claim that the temperature increases each year. Moreover, we have shown that the magnitude of temperature increases across the years for the month of June is not very large - nowhere close to 1.4 to 4.6 C.\nLet’s look next at ‘Min_Temperature’."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#looking-at-min_temperature",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#looking-at-min_temperature",
    "title": "Take-home Exercise 3",
    "section": "4.2 Looking at ‘Min_Temperature’",
    "text": "4.2 Looking at ‘Min_Temperature’\n\n4.2.1 Distribution of values\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njune_weather_data$Year&lt;-factor(june_weather_data$Year,levels=c(\"1983\", \"1993\", \"2003\", \"2013\", \"2023\"))\n\n# Ridge plot\nridge_plot &lt;- ggplot(june_weather_data, aes(x = Min_Temperature, y=factor(Year), fill = stat(x))) +\n                       geom_density_ridges_gradient(scale =2,rel_min_height = 0.01, gradient_lwd = 1.) +\n                       scale_y_discrete(limits = unique(rev(june_weather_data$Year)))+\n                       scale_fill_viridis_c(name = \"°C\", option = \"C\") +\n                       labs(title = 'June temperature profile',\n                        subtitle = 'Distribution of daily minimum temperature recorded in June across years',\n                        x = \"Temperature (°C)\",\n                        y = \"Year\") +\n                        theme_ridges(font_size = 13, grid = TRUE) +\n  theme(plot.title = element_text(size = 14),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n# Violin with box plot\nviolin_plot &lt;- plot_ly(data = june_weather_data, x = ~Year, y = ~Min_Temperature, type = 'violin', \n               split = ~Year, \n               box = list(visible = T),\n               meanline = list(visible = T)) %&gt;% # Adds a box plot inside the violin for summary statistics\n  layout(title = list(text = \"&lt;b&gt;Distribution of Minimum Temperatures in June Across Years&lt;/b&gt;\", \n                                                   font = list(family = \"Arial\", size = 14),\n                                                   xanchor = \"center\", xref = \"paper\",\n                                                   yanchor = \"middle\", yref = \"paper\"),\n                      yaxis = list(title = list(text = \"Temperature (°C)\", \n                                                font = list(family = \"Arial\", size = 12)) ),\n                      xaxis = list(title = list(text = \"Year\", \n                                                font = list(family = \"Arial\", size = 12)) ))\n\nridge_plot\nviolin_plot\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nThe positions of the distributions look similar across the years, with no notable or consistent shift towards the left or right.\nThe left tails of the distributions are quite consistent in extending to the far left. This suggests consistent occurrence of extreme minimum temperatures across the years.\nThe shape of distributions are non-normal, except for the year 1983. Some of the non-normal distributions have multiple peaks, like 1993 and 2023.\nLooking at the violin plot, the median temperatures do appear to rise slightly from 1983 to 1993 to 2003, but seem to dip at 2013 before a slight rise in 2023.\nNotably the year 1983 had the most extreme low Min_Temperature values.\n\n\n\n\n\n4.2.2 Visualizing uncertainty of point estimate, the mean\nNext, we will explore the estimated mean of ‘Min_Temperature’ values for each year, and visualize the uncertainty associated with that estimate.\n\n4.2.2.1 Derive summary statistics\nFirstly, we derive the necessary summary statistics.\n\n\nCode\nmy_sum &lt;- june_weather_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise( # used to compute the count of observations, mean, standard deviation\n    n=n(),\n    mean=mean(Min_Temperature),\n    sd=sd(Min_Temperature)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1)) %&gt;% # used to derive standard error of mean\nmutate(\n    ci_lower = mean - (1.96 * se),\n    ci_upper = mean + (1.96 * se)\n  )\n\n\n\n\n4.2.2.2 Plotting standard error bars and confidence interval\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code to plot side by side sub plots\nmy_sum$Year &lt;- as.numeric(as.character(my_sum$Year))\n\nmy_sum_d = highlight_key(my_sum)\n\n# Create the plot for mean with standard error\nfig1 &lt;- plot_ly(data = my_sum_d, x = ~Year, y = ~mean, type = 'scatter', mode = 'lines+markers',\n        error_y = ~list(array = se,\n                        color = '#000000')) %&gt;%\n  layout(title = list(text = '&lt;b&gt;Mean of Min_Temperature values \\n with Standard Error&lt;/b&gt;', font = list(size = 14), xanchor = \"center\", xref = \"paper\"),\n         xaxis = list(title = list(text = 'Year', font = list(size = 11))),\n         yaxis = list(title = list(text = 'Temperature (°C)', font = list(size = 11))))\n\n\n# Add the second trace for mean with confidence intervals\nfig2 &lt;- plot_ly(data = my_sum_d, x = ~Year, y = ~mean, type = 'scatter', mode = 'lines+markers',\n               error_y = ~list(\n                 array = my_sum$ci_upper - my_sum$mean,\n                 arrayminus = my_sum$mean - my_sum$ci_lower,\n                 color = '#000000'\n               ),\n               line = list(color = 'blue')) %&gt;%\n  layout(title = list(text = '&lt;b&gt;Mean of Min_Temperature values \\n with 95% Confidence Intervals&lt;/b&gt;', font = list(size = 14), xanchor = \"center\", xref = \"paper\"),\n         xaxis = list(title = list(text = 'Year', font = list(size = 11))), \n         yaxis = list(title = list(text = 'Temperature (°C)', font = list(size = 11))))\nbscols(list(fig1,\n       datatable(my_sum_d, \n                 width = \"200%\",\n                 colnames = c(\"Year\", \"Number of Days\", \"Mean\", \"Standard Deviation\", \"Standard Error\", \"Confidence Interval (Lower)\", \"Confidence Interval (Higher)\"),\n                 rownames = FALSE,\n                 height = 200)), \n        fig2)\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nThe trend in magnitude of temperature increase is not consistent. From 1983 to 1993 there was a slight increase in temperature, and a larger increase from 1993 to 2003. However, the mean seemed to dip from 2003 to 2013 and slightly rise from 2013 to 2023.\nThe error bars and confidence intervals for the years 1983 and 1993 overlap almost completely, while the error bars for the years 2003, 2013 and 2023 overlap almost completely. This overlap suggests that the difference between the years is not statistically significant.\n\n\n\n\n\n\n4.2.3 Statistical testing for difference amongst years\nGiven that the data is non-normal and there are multiple groups, we will use a Kruskal-Wallis one-way ANOVA test to test whether the Min_Temperature varies significantly among the five years.\nThe hypotheses would be:\n\nNull Hypothesis (H0): There is no difference in the median values among the five years. This means that any observed difference in among the years is due to random chance.\nAlternative Hypothesis (H1): There is a difference in the median values among the five years. This means that the observed difference is not due to chance, and there is a true difference among the years.\n\n\n\nCode\nggbetweenstats(\n  data = june_weather_data,\n  x = Year, \n  y = Min_Temperature,\n  type = \"np\", # non-parametric text that compares median\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\", # displays only significant pairwise comparisons\n  p.adjust.method = \"fdr\", \n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nThe p-value is above 0.05. Hence we do not have sufficient evidence to reject the Null Hypothesis. We cannot conclude that there is evidence to indicate that there is a difference in the median Min_Temperature values for June across the years.\nMoreover, no pairwise comparisons between the years show any p-value below 0.05 - indicating that we cannot conclude that the difference in Min_Temperature values between any of the years is significantly different.\n\n\n\nLet’s look next at ‘Max_Temperature’."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#looking-at-max_temperature",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#looking-at-max_temperature",
    "title": "Take-home Exercise 3",
    "section": "4.3 Looking at ‘Max_Temperature’",
    "text": "4.3 Looking at ‘Max_Temperature’\n\n4.3.1 Distribution of values\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njune_weather_data$Year&lt;-factor(june_weather_data$Year,levels=c(\"1983\", \"1993\", \"2003\", \"2013\", \"2023\"))\n\n# Ridge plot\nridge_plot &lt;- ggplot(june_weather_data, aes(x = Max_Temperature, y=factor(Year), fill = stat(x))) +\n                       geom_density_ridges_gradient(scale =2,rel_min_height = 0.01, gradient_lwd = 1.) +\n                       scale_y_discrete(limits = unique(rev(june_weather_data$Year)))+\n                       scale_fill_viridis_c(name = \"°C\", option = \"C\") +\n                       labs(title = 'June temperature profile',\n                        subtitle = 'Distribution of daily maximum temperature recorded in June across years',\n                        x = \"Temperature (°C)\",\n                        y = \"Year\") +\n                        theme_ridges(font_size = 13, grid = TRUE) +\n  theme(plot.title = element_text(size = 14),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n# Violin with box plot\nviolin_plot &lt;- plot_ly(data = june_weather_data, x = ~Year, y = ~Max_Temperature, type = 'violin', \n               split = ~Year, \n               box = list(visible = T),\n               meanline = list(visible = T)) %&gt;% # Adds a box plot inside the violin for summary statistics\n  layout(title = list(text = \"&lt;b&gt;Distribution of Maximum Temperatures in June Across Years&lt;/b&gt;\", \n                                                   font = list(family = \"Arial\", size = 14),\n                                                   xanchor = \"center\", xref = \"paper\",\n                                                   yanchor = \"middle\", yref = \"paper\"),\n                      yaxis = list(title = list(text = \"Temperature (°C)\", \n                                                font = list(family = \"Arial\", size = 12)) ),\n                      xaxis = list(title = list(text = \"Year\", \n                                                font = list(family = \"Arial\", size = 12)) ))\n\nridge_plot\nviolin_plot\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nThe positions of the distributions look fluctuate slightly over the years. From 1983 to 1993, the distribution shifted left; from 1993 to 2003, the distribution shifted right. The distribution position remained quite constant from 2003 to 2013 to 2023.\nThe left tails of the distributions are quite consistent in extending to the far left.\nThe shape of distributions are non-normal, except for the year 1993 Some of the non-normal distributions have multiple peaks, like 2013 and 2023.\nThe median temperatures fluctuate across the years. From the years 2003 to 2023, the median ‘Max_Temperature’ value decreases.\nThe years also have very long lower tails, indicating days where the maximum temperature was not so high. These would have been more cooling days.\n\n\n\n\n\n4.3.2 Visualizing uncertainty of point estimate, the mean\nDeriving the necessary summary statistics:\n\n\nCode\nmy_sum &lt;- june_weather_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise( # used to compute the count of observations, mean, standard deviation\n    n=n(),\n    mean=mean(Max_Temperature),\n    sd=sd(Max_Temperature)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1)) %&gt;% # used to derive standard error\nmutate(\n    ci_lower = mean - (1.96 * se),\n    ci_upper = mean + (1.96 * se)\n  )\n\n\n\n4.3.2.1 Plotting standard error bars and confidence interval\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Code to plot side by side sub plots\nmy_sum$Year &lt;- as.numeric(as.character(my_sum$Year))\n\nmy_sum_d = highlight_key(my_sum)\n\n# Create the plot for mean with standard error\nfig1 &lt;- plot_ly(data = my_sum_d, x = ~Year, y = ~mean, type = 'scatter', mode = 'lines+markers',\n        error_y = ~list(array = se,\n                        color = '#000000')) %&gt;%\n  layout(title = list(text = '&lt;b&gt;Mean of Max_Temperature values \\n with Standard Error&lt;/b&gt;', font = list(size = 14), xanchor = \"center\", xref = \"paper\"),\n         xaxis = list(title = list(text = 'Year', font = list(size = 11))),\n         yaxis = list(title = list(text = 'Temperature (°C)', font = list(size = 11))))\n\n\n# Add the second trace for mean with confidence intervals\nfig2 &lt;- plot_ly(data = my_sum_d, x = ~Year, y = ~mean, type = 'scatter', mode = 'lines+markers',\n               error_y = ~list(\n                 array = my_sum$ci_upper - my_sum$mean,\n                 arrayminus = my_sum$mean - my_sum$ci_lower,\n                 color = '#000000'\n               ),\n               line = list(color = 'blue')) %&gt;%\n  layout(title = list(text = '&lt;b&gt;Mean of Max_Temperature values \\n with 95% Confidence Intervals&lt;/b&gt;', font = list(size = 14), xanchor = \"center\", xref = \"paper\"),\n         xaxis = list(title = list(text = 'Year', font = list(size = 11))), \n         yaxis = list(title = list(text = 'Temperature (°C)', font = list(size = 11))))\nbscols(list(fig1,\n       datatable(my_sum_d, \n                 width = \"200%\",\n                 colnames = c(\"Year\", \"Number of Days\", \"Mean\", \"Standard Deviation\", \"Standard Error\", \"Confidence Interval (Lower)\", \"Confidence Interval (Higher)\"),\n                 rownames = FALSE,\n                 height = 200)), \n        fig2)\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nThe trend in magnitude of temperature increase is not consistent. From 1983 to 1993 there was a slight decrease. From 1993 to 2003 there is a large increase, but this remains quite constant to 2013 before decreasing in 2023.\nThe length of the error bars across the years are similar, indicating similar precision in estimating the mean.\nHowever, the overlapping confidence intervals between 1983 and 1993, as well as among the years 2003, 2013, and 2023, suggest that the differences in the estimates across these periods lack statistical significance.\n\n\n\n\n\n\n4.3.3 Statistical testing for difference amongst years\nGiven that the data is non-normal and there are multiple groups, we will use a Kruskal-Wallis one-way ANOVA test to test whether the Max_Temperature varies significantly among the five years.\nThe hypotheses would be:\n\nNull Hypothesis (H0): There is no difference in the median values among the five years. This means that any observed difference in among the years is due to random chance.\nAlternative Hypothesis (H1): There is a difference in the median values among the five years. This means that the observed difference is not due to chance, and there is a true difference among the years.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = june_weather_data,\n  x = Year, \n  y = Max_Temperature,\n  type = \"np\", # non-parametric text that compares median\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\", # displays only significant pairwise comparisons\n  p.adjust.method = \"fdr\", \n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nThe p-value is below 0.05. Hence we have sufficient evidence to reject the Null Hypothesis. We can conclude that there is evidence to indicate that there is a difference in the median Max_Temperature values for June across the years.\nHowever on closer inspection of the pairwise comparisons, only 1 pair of consecutive 10-year intervals, 1993 and 2003, have a p-value below 0.05."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#looking-at-difference-in-min-and-max",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#looking-at-difference-in-min-and-max",
    "title": "Take-home Exercise 3",
    "section": "4.4 Looking at difference in Min and Max",
    "text": "4.4 Looking at difference in Min and Max\nTo comprehensively evaluate the claim regarding future climate projections of a 1.4 to 4.6°C increase in daily mean temperatures, we will examine the daily temperature difference (the difference between ‘Max_Temperature’ and ‘Min_Temperature’). Analysing this will help validate if observed trends in mean temperatures are consistent with changes in daily temperature variability.\nWe will create a new column, ‘Temp_Difference’ in the code chunk below:\n\njune_weather_data$Temp_Difference &lt;- june_weather_data$Max_Temperature - june_weather_data$Min_Temperature\n\n\n4.4.1 Distribution of values\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\njune_weather_data$Year&lt;-factor(june_weather_data$Year,levels=c(\"1983\", \"1993\", \"2003\", \"2013\", \"2023\"))\n\n# Ridge plot\nridge_plot &lt;- ggplot(june_weather_data, aes(x = Temp_Difference, y=factor(Year), fill = stat(x))) +\n                       geom_density_ridges_gradient(scale =2,rel_min_height = 0.01, gradient_lwd = 1.) +\n                       scale_y_discrete(limits = unique(rev(june_weather_data$Year)))+\n                       scale_fill_viridis_c(name = \"°C\", option = \"C\") +\n                       labs(title = 'June temperature profile',\n                        subtitle = 'Distribution of daily temperature differences recorded in June across years',\n                        x = \"Temperature (°C)\",\n                        y = \"Year\") +\n                        theme_ridges(font_size = 13, grid = TRUE) +\n  theme(plot.title = element_text(size = 14),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n# Violin with box plot\nviolin_plot &lt;- plot_ly(data = june_weather_data, x = ~Year, y = ~Temp_Difference, type = 'violin', \n               split = ~Year, \n               box = list(visible = T),\n               meanline = list(visible = T)) %&gt;% # Adds a box plot inside the violin for summary statistics\n  layout(title = list(text = \"&lt;b&gt;Distribution of daily temperature differences recorded in June across years&lt;/b&gt;\", \n                                                   font = list(family = \"Arial\", size = 14),\n                                                   xanchor = \"center\", xref = \"paper\",\n                                                   yanchor = \"middle\", yref = \"paper\"),\n                      yaxis = list(title = list(text = \"Temperature (°C)\", \n                                                font = list(family = \"Arial\", size = 12)) ),\n                      xaxis = list(title = list(text = \"Year\", \n                                                font = list(family = \"Arial\", size = 12)) ))\n\nridge_plot\nviolin_plot\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nThere appears to be no notable or consistent shift towards the left or right across the years.\nThe distributions for the years 1993 and 2013 appear to be flatter than the the other years, indicating greater temperature differences between minimum and maximum temperature.\nFor the years 1983, 1993 and 2013, the right tails appear to extend more to the right, suggesting more days where daily temperature difference is higher.\nThe shape of distributions are non-normal, except for the year 2003.\nWhile there appears to be no notable trend in median values across the years, the year 2023 has the lowest median value suggesting that there were more days where the daily temperature difference was smaller. It could suggest the occurrence of fewer extreme temperature differences during 2023.\n\n\n\n\n\n4.4.2 Visualizing uncertainty of point estimate, the mean\nDeriving the necessary summary statistics:\n\n\nCode\nmy_sum &lt;- june_weather_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise( # used to compute the count of observations, mean, standard deviation\n    n=n(),\n    mean=mean(Temp_Difference),\n    sd=sd(Temp_Difference)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1)) %&gt;% # used to derive standard error of mean\nmutate(\n    ci_lower = mean - (1.96 * se),\n    ci_upper = mean + (1.96 * se)\n  )\n\n\n\n4.4.2.1 Plotting standard error bars and confidence interval\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Change Year back to numeric type\nmy_sum$Year &lt;- as.numeric(as.character(my_sum$Year))\n\n# Create crosstalk object\nmy_sum_d = highlight_key(my_sum)\n\n# Create the plot for mean with standard error\nfig1 &lt;- plot_ly(data = my_sum_d, x = ~Year, y = ~mean, type = 'scatter', mode = 'lines+markers',\n        error_y = ~list(array = se,\n                        color = '#000000')) %&gt;%\n  layout(title = list(text = '&lt;b&gt;Mean of Temp_Difference values \\n with Standard Error&lt;/b&gt;', font = list(size = 14), xanchor = \"center\", xref = \"paper\"),\n         xaxis = list(title = list(text = 'Year', font = list(size = 11))),\n         yaxis = list(title = list(text = 'Temperature (°C)', font = list(size = 11))))\n\n\n# Add the second trace for mean with confidence intervals\nfig2 &lt;- plot_ly(data = my_sum_d, x = ~Year, y = ~mean, type = 'scatter', mode = 'lines+markers',\n               error_y = ~list(\n                 array = my_sum$ci_upper - my_sum$mean,\n                 arrayminus = my_sum$mean - my_sum$ci_lower,\n                 color = '#000000'\n               ),\n               line = list(color = 'blue')) %&gt;%\n  layout(title = list(text = '&lt;b&gt;Mean of Temp_Difference values \\n with 95% Confidence Intervals&lt;/b&gt;', font = list(size = 14), xanchor = \"center\", xref = \"paper\"),\n         xaxis = list(title = list(text = 'Year', font = list(size = 11))), \n         yaxis = list(title = list(text = 'Temperature (°C)', font = list(size = 11))))\n\n# Layout of plots\nbscols(list(fig1,\n       datatable(my_sum_d, \n                 width = \"200%\",\n                 colnames = c(\"Year\", \"Number of Days\", \"Mean\", \"Standard Deviation\", \"Standard Error\", \"Confidence Interval (Lower)\", \"Confidence Interval (Higher)\"),\n                 rownames = FALSE,\n                 height = 200)), \n        fig2)\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nThe trend in magnitude of temperature difference is not consistent.\nHowever, the overlapping error bars and confidence intervals suggest that the differences in the estimates across these periods lack statistical significance.\n\n\n\n\n\n\n4.4.3 Statistical testing for difference amongst years\nGiven that the data is non-normal and there are multiple groups, we will use a Kruskal-Wallis one-way ANOVA test to test whether the Temp_Difference varies significantly among the five years.\nThe hypotheses would be:\n\nNull Hypothesis (H0): There is no difference in the median values among the five years. This means that any observed difference in among the years is due to random chance.\nAlternative Hypothesis (H1): There is a difference in the median values among the five years. This means that the observed difference is not due to chance, and there is a true difference among the years.\n\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggbetweenstats(\n  data = june_weather_data,\n  x = Year, \n  y = Temp_Difference,\n  type = \"np\", # non-parametric text that compares median\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\", # displays only significant pairwise comparisons\n  p.adjust.method = \"fdr\", \n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\n\nThe p-value is above 0.05. Hence we do not have sufficient evidence to reject the Null Hypothesis. We cannot conclude that there is evidence to indicate that there is a difference in the median Temp_Difference values for June across the years.\nMoreover, no pairwise comparisons between the years show any p-value below 0.05 - indicating that we cannot conclude that the difference in Temp_Difference values between any of the years is significantly different."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#summary-and-conclusion",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#summary-and-conclusion",
    "title": "Take-home Exercise 3",
    "section": "4.5 Summary and conclusion",
    "text": "4.5 Summary and conclusion\nIn conclusion, we used visualizations to explore the various temperature measures (including daily mean, daily minimum, daily maximum, and daily difference) for the Changi weather station for the month of June across years of 1983, 1993, 2003, 2013 and 2023. Despite the claim projecting an increase in temperature magnitudes ranging from 1.4 to 4.6°C, our visual and statistical analyses of historical values did not support this projection.\nVisually, our distribution plots depicted a consistent upward trajectory in temperature magnitudes for daily mean temperature across the years. However, statistical tests conducted showed that that the differences in the point estimate of daily mean temperature across these periods lacked statistical significance for 10-year intervals.\nThis was supported by the analysis of the daily minimum, daily maximum, and daily difference as well. This exercise suggests that while there could be increase in daily mean temperature over the years, more can be done to explore if the projected magnitude of increase is indeed as drastic as 1.4 to 4.6°C."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#references",
    "href": "Take-home_Ex/Take-home_Ex3/Take-home_Ex3.html#references",
    "title": "Take-home Exercise 3",
    "section": "4.6 References",
    "text": "4.6 References\n\nR for Visual Analytics: Programming Interactive Data Visualisation with R.\nR for Visual Analytics: Visualising Distribution.\nR for Visual Analytics: Visual Statistical Analysis.\nR for Visual Analytics: Visualising Uncertainty."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "The PISA global education survey is the OECD’s Programme for International Student Assessment, that measures 15-year-olds’ ability to use their reading, mathematics and science knowledge and skills to meet real-life challenges.\n\n\n\nIn this exercise, Exploratory Data Analysis (EDA) methods and ggplot functions are used to explore:\n\nthe distribution of Singapore students’ performance in mathematics, reading, and science, and\nthe relationship between these performances with schools, gender and socioeconomic status of the students."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#overview",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#overview",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "The PISA global education survey is the OECD’s Programme for International Student Assessment, that measures 15-year-olds’ ability to use their reading, mathematics and science knowledge and skills to meet real-life challenges.\n\n\n\nIn this exercise, Exploratory Data Analysis (EDA) methods and ggplot functions are used to explore:\n\nthe distribution of Singapore students’ performance in mathematics, reading, and science, and\nthe relationship between these performances with schools, gender and socioeconomic status of the students."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#getting-started",
    "title": "Take-home Exercise 1",
    "section": "Getting started",
    "text": "Getting started\n\nLoad packages\nWe load the following R packages using the pacman::p_load() function:\n\ntidyverse: Core collection of R packages designed for data science\nhaven: To read in data formats such as SAS and SPSS\nggrepel: to provides geoms for ggplot2 to repel overlapping text labels\nggthemes: to use additional themes for ggplot2\npatchwork: to prepare composite figure created using ggplot2\nggridges: to plot ridgeline plots\nggdist: for visualizations of distributions and uncertainty\nscales: provides the internal scaling infrastructure used by ggplot2\n\n\npacman::p_load(tidyverse, haven,\n               ggrepel, ggthemes,\n               ggridges, ggdist,\n               patchwork, scales)\n\n\n\nImport data\nThe dataset (data/stu_qqq_SG.rds) used in the exercise is a subset of the original Student questionnaire data file retrieved from the PISA 2022 database. It has been filtered to include only the rows where the country code (CNT) matches ‘Singapore’ (SGP).\nWe import this dataset as stu_qqq_SG.\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\n\n\n\nData pre-processing\nWe first take a look at the data, and check if there are any duplicate entries.\n\nGlimpse of data\nUsing the glimpse() function, we see that the dataset consists of 6,606 rows and 1,279 columns. It also shows the column names, column type, and the first few entries of each column.\n\n\nglimpse(stu_qqq_SG)\n\nRows: 6,606\nColumns: 1,279\n$ CNT          &lt;chr&gt; \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"SGP\", \"…\n$ CNTRYID      &lt;dbl&gt; 702, 702, 702, 702, 702, 702, 702, 702, 702, 702, 702, 70…\n$ CNTSCHID     &lt;dbl&gt; 70200052, 70200134, 70200112, 70200004, 70200152, 7020004…\n$ CNTSTUID     &lt;dbl&gt; 70200001, 70200002, 70200003, 70200004, 70200005, 7020000…\n$ CYC          &lt;chr&gt; \"08MS\", \"08MS\", \"08MS\", \"08MS\", \"08MS\", \"08MS\", \"08MS\", \"…\n$ NatCen       &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"070200…\n$ STRATUM      &lt;chr&gt; \"SGP01\", \"SGP01\", \"SGP01\", \"SGP01\", \"SGP01\", \"SGP01\", \"SG…\n$ SUBNATIO     &lt;chr&gt; \"7020000\", \"7020000\", \"7020000\", \"7020000\", \"7020000\", \"7…\n$ REGION       &lt;dbl&gt; 70200, 70200, 70200, 70200, 70200, 70200, 70200, 70200, 7…\n$ OECD         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ADMINMODE    &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ LANGTEST_QQQ &lt;dbl&gt; 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 31…\n$ LANGTEST_COG &lt;dbl&gt; 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 31…\n$ LANGTEST_PAQ &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Option_CT    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Option_FL    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Option_ICTQ  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Option_WBQ   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Option_PQ    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Option_TQ    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Option_UH    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ BOOKID       &lt;dbl&gt; 4, 45, 8, 40, 42, 15, 13, 39, 14, 7, 20, 17, 38, 24, 19, …\n$ ST001D01T    &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ ST003D02T    &lt;dbl&gt; 10, 6, 7, 2, 9, 9, 3, 4, 8, 6, 10, 7, 9, 11, 5, 10, 11, 4…\n$ ST003D03T    &lt;dbl&gt; 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 2006, 200…\n$ ST004D01T    &lt;dbl&gt; 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, …\n$ ST250Q01JA   &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250Q02JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250Q03JA   &lt;dbl&gt; 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250Q04JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250Q05JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST250D06JA   &lt;chr&gt; \"7020002\", \"7020001\", \"7020001\", \"7020002\", \"7020002\", \"7…\n$ ST250D07JA   &lt;chr&gt; \"7020002\", \"7020001\", \"7020002\", \"7020002\", \"7020002\", \"7…\n$ ST251Q01JA   &lt;dbl&gt; 2, 1, 2, 1, 2, 2, 2, 1, 3, 3, 1, 2, 2, 1, 2, 2, 1, 2, 3, …\n$ ST251Q02JA   &lt;dbl&gt; 1, 4, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, …\n$ ST251Q03JA   &lt;dbl&gt; 3, 3, 3, 3, 2, 2, 3, 3, 4, 3, 2, 2, 3, 2, 3, 3, 2, 3, 4, …\n$ ST251Q04JA   &lt;dbl&gt; 3, 3, 3, 3, 2, 3, 3, 3, 4, 3, 2, 2, 3, 2, 3, 3, 2, 3, 4, …\n$ ST251Q06JA   &lt;dbl&gt; 3, 4, 2, 2, 1, 2, 2, 3, 4, 1, 3, 3, 1, 2, 2, 4, 4, 1, 2, …\n$ ST251Q07JA   &lt;dbl&gt; 3, 2, 1, 1, 4, 1, 4, 1, 4, 3, 1, 4, 1, 1, 4, 4, 1, 4, 1, …\n$ ST251D08JA   &lt;chr&gt; \"9999997\", \"9999997\", \"9999997\", \"9999997\", \"9999997\", \"9…\n$ ST251D09JA   &lt;chr&gt; \"9999997\", \"9999997\", \"9999997\", \"9999997\", \"9999997\", \"9…\n$ ST253Q01JA   &lt;dbl&gt; 7, 8, 7, 6, 7, 7, 8, 8, 8, 7, 7, 8, 5, 7, 7, 8, 5, 7, 7, …\n$ ST254Q01JA   &lt;dbl&gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 1, 3, 2, 2, 2, 2, …\n$ ST254Q02JA   &lt;dbl&gt; 1, 2, 2, 1, 3, 2, 2, 5, 2, 2, 1, 3, 2, 1, 2, 2, 2, 2, 2, …\n$ ST254Q03JA   &lt;dbl&gt; 3, 2, 2, 2, 2, 2, 3, 3, 3, 4, 2, 3, 2, 2, 3, 3, 2, 3, 2, …\n$ ST254Q04JA   &lt;dbl&gt; 2, 3, 2, 1, 1, 2, 2, 3, 3, 2, 1, 3, 2, 2, 3, 3, 2, 3, 2, …\n$ ST254Q05JA   &lt;dbl&gt; 1, 5, 1, 1, NA, 1, 1, 5, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1,…\n$ ST254Q06JA   &lt;dbl&gt; 3, 2, 3, 3, 4, 2, 4, 3, 4, 4, 3, 3, 2, 3, 3, 4, 3, 4, 3, …\n$ ST255Q01JA   &lt;dbl&gt; 7, 4, 4, 3, 2, 2, 4, 5, 7, 4, 3, 7, 4, 4, 2, 4, 5, 4, 4, …\n$ ST256Q01JA   &lt;dbl&gt; 2, 4, 5, 2, 4, 1, 1, 3, 4, 4, 1, 4, 2, 2, 2, 3, 2, 2, 1, …\n$ ST256Q02JA   &lt;dbl&gt; 2, 5, 2, 1, 1, 2, 1, 5, 4, 2, 2, 4, 1, 2, 2, 2, 3, 5, 1, …\n$ ST256Q03JA   &lt;dbl&gt; 4, 5, 2, 1, 1, 2, 2, 5, 5, 1, 2, 4, 1, 3, 1, 4, 4, 5, 1, …\n$ ST256Q06JA   &lt;dbl&gt; 4, 3, 3, 2, 2, 5, 2, 4, 4, 1, 2, 4, 2, 2, 2, 3, 2, 2, 3, …\n$ ST256Q07JA   &lt;dbl&gt; 3, 5, 5, 2, 2, 5, 1, 4, 3, 1, 2, 4, 1, 2, 2, 2, 1, 5, 1, …\n$ ST256Q08JA   &lt;dbl&gt; 3, 3, 3, 1, 1, 5, 1, 5, 5, 2, 2, 4, 1, 1, 1, 3, 1, 5, 2, …\n$ ST256Q09JA   &lt;dbl&gt; 2, 2, 4, 2, 2, 2, 2, 2, 2, 3, 2, 4, 2, 2, 2, 2, 2, 2, NA,…\n$ ST256Q10JA   &lt;dbl&gt; 4, 4, 5, 2, 4, 1, 4, 4, 4, 3, 2, 4, 4, 1, 2, 4, 3, 3, 3, …\n$ ST230Q01JA   &lt;dbl&gt; 4, 4, 2, 4, 4, 3, 2, 2, 3, 4, 1, 3, 4, 1, 4, 3, 2, 3, 2, …\n$ ST005Q01JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST006Q01JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, NA, 2, 2, NA, 2, 1, 2, 2, 2, 2, 2, 2…\n$ ST006Q02JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, NA, 1, 1, 2, 2, 1, 2, 2, 2,…\n$ ST006Q03JA   &lt;dbl&gt; 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, NA, 1, 1, 2, 2, 1, 1, 1, 2,…\n$ ST006Q04JA   &lt;dbl&gt; 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, NA, 2, 1, 2, 2, 2, 1, 2, 2,…\n$ ST006Q05JA   &lt;dbl&gt; 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, NA, 1, 2, 1, 2, 1, 1, 2, 1,…\n$ ST007Q01JA   &lt;dbl&gt; 2, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST008Q01JA   &lt;dbl&gt; 2, 2, 2, NA, 2, 2, 2, NA, 2, 2, NA, 2, 1, 2, 2, 2, 2, 2, …\n$ ST008Q02JA   &lt;dbl&gt; 2, 2, 2, NA, 2, 1, 2, NA, 2, 2, NA, 1, 1, 1, 2, 2, 1, 2, …\n$ ST008Q03JA   &lt;dbl&gt; 2, 2, 2, NA, 2, 1, 2, 1, 1, 2, NA, 1, 1, 2, 2, 2, 1, 1, 2…\n$ ST008Q04JA   &lt;dbl&gt; 1, 1, 2, NA, 1, 1, 2, 1, 1, 1, NA, 2, 1, 2, 2, 2, 1, 2, 2…\n$ ST008Q05JA   &lt;dbl&gt; 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, NA, 1, 2, 2, 1, 2, 1, 2, 1,…\n$ ST258Q01JA   &lt;dbl&gt; 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, …\n$ ST259Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST259Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST019AQ01T   &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, …\n$ ST019BQ01T   &lt;dbl&gt; 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, …\n$ ST019CQ01T   &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, …\n$ ST021Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, 1, NA, 1, NA, NA, NA, NA, NA, 7, NA, …\n$ ST022Q01TA   &lt;dbl&gt; 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, …\n$ ST226Q01JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST125Q01NA   &lt;dbl&gt; 3, 5, 8, 4, 8, 3, 4, 8, 8, 4, 8, 2, 7, 2, 3, 5, 8, 8, 3, …\n$ ST126Q01TA   &lt;dbl&gt; 4, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, …\n$ ST127Q01TA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST127Q02TA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST127Q03TA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST260Q01JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, …\n$ ST260Q02JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST260Q03JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST261Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA…\n$ ST261Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA…\n$ ST261Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST261Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, NA, NA…\n$ ST062Q01TA   &lt;dbl&gt; 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST062Q02TA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, …\n$ ST062Q03TA   &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST267Q01JA   &lt;dbl&gt; NA, 4, 3, 3, NA, 4, 3, NA, 4, NA, NA, 4, 3, 3, 3, NA, 3, …\n$ ST267Q02JA   &lt;dbl&gt; 2, 3, 3, NA, 3, 3, 1, NA, 4, NA, 3, 4, 1, NA, 4, 2, 1, 4,…\n$ ST267Q03JA   &lt;dbl&gt; NA, NA, 3, 2, NA, 3, 1, 3, NA, 3, NA, 4, NA, 3, NA, 3, NA…\n$ ST267Q04JA   &lt;dbl&gt; 2, 2, NA, NA, 3, NA, 1, 3, 2, 2, 2, NA, 3, 1, 4, 1, 2, NA…\n$ ST267Q05JA   &lt;dbl&gt; 3, 3, 3, 3, 3, 3, NA, 3, NA, NA, 3, 4, NA, NA, NA, 4, 3, …\n$ ST267Q06JA   &lt;dbl&gt; NA, NA, NA, 3, 4, 4, NA, 3, 4, 3, NA, NA, 2, 3, 3, 4, 4, …\n$ ST267Q07JA   &lt;dbl&gt; 3, 4, 4, NA, NA, NA, NA, NA, 4, 3, 3, 4, 1, 3, 4, NA, NA,…\n$ ST267Q08JA   &lt;dbl&gt; 2, NA, NA, 1, 1, NA, 1, 3, NA, 2, 2, NA, NA, NA, NA, NA, …\n$ ST034Q01TA   &lt;dbl&gt; 4, 4, 3, 2, 4, 4, 2, 1, 3, 3, 3, 4, 2, 4, 3, 4, 3, 4, 3, …\n$ ST034Q02TA   &lt;dbl&gt; NA, 2, 2, 3, 1, 2, 2, NA, 1, 2, NA, 1, 3, 1, 4, 2, 3, 2, …\n$ ST034Q03TA   &lt;dbl&gt; 2, NA, NA, 3, 1, 2, 3, 3, 2, NA, 2, 1, 3, 1, 2, NA, 2, NA…\n$ ST034Q04TA   &lt;dbl&gt; 4, 3, 2, 2, 4, NA, 3, 2, NA, 3, 3, 4, 2, 4, 2, 4, NA, 4, …\n$ ST034Q05TA   &lt;dbl&gt; 2, 2, 2, 2, NA, 2, NA, 3, 2, 2, 2, NA, 3, 1, NA, 2, 2, 3,…\n$ ST034Q06TA   &lt;dbl&gt; 3, 3, 3, NA, 4, 4, 3, 4, 3, 3, 3, 4, NA, NA, 3, 4, 3, 4, …\n$ ST038Q03NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, …\n$ ST038Q04NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 4, 3, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, …\n$ ST038Q05NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST038Q06NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, …\n$ ST038Q07NA   &lt;dbl&gt; 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, …\n$ ST038Q08NA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, …\n$ ST038Q09JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST038Q10JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, …\n$ ST038Q11JA   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST265Q01JA   &lt;dbl&gt; 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 2, 1, 2, …\n$ ST265Q02JA   &lt;dbl&gt; 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 2, 1, 2, …\n$ ST265Q03JA   &lt;dbl&gt; 2, 1, 2, 2, 1, 2, 3, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, …\n$ ST265Q04JA   &lt;dbl&gt; 2, 1, 2, 2, 1, 2, 3, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, …\n$ ST266Q01JA   &lt;dbl&gt; 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, …\n$ ST266Q02JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST266Q03JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST266Q04JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST266Q05JA   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ ST294Q01JA   &lt;dbl&gt; 5, 4, 6, 1, 1, 1, 1, 6, 6, 1, 5, 1, 1, 1, 4, 3, 6, 6, 1, …\n$ ST294Q02JA   &lt;dbl&gt; 1, 5, 2, 1, 4, 6, 1, 6, 1, 1, 1, 1, 6, 1, 5, 1, 1, 1, 6, …\n$ ST294Q03JA   &lt;dbl&gt; 6, 2, 1, 6, 1, 3, 4, 1, 1, 1, 1, 1, 6, 1, 6, 1, 3, 1, 1, …\n$ ST294Q04JA   &lt;dbl&gt; 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST294Q05JA   &lt;dbl&gt; 1, 3, 1, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, …\n$ ST295Q01JA   &lt;dbl&gt; 6, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, 3, 6, 6, …\n$ ST295Q02JA   &lt;dbl&gt; 5, 4, 3, 6, 5, 6, 1, 6, 6, 4, 6, 6, 6, 1, 5, 6, 6, 6, 6, …\n$ ST295Q03JA   &lt;dbl&gt; 6, 2, 1, 6, 6, 4, 5, 1, 1, 5, 3, 3, 6, 1, 6, 1, 4, 6, 1, …\n$ ST295Q04JA   &lt;dbl&gt; 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST295Q05JA   &lt;dbl&gt; 2, 3, 3, 1, 5, 2, 3, 1, 4, 6, 2, 3, 6, 3, 3, 1, 3, 4, 1, …\n$ ST326Q01JA   &lt;dbl&gt; 3, 2, 3, 2, 6, 3, 3, 8, 4, 3, 4, 4, 3, 7, 5, 3, 7, 6, 2, …\n$ ST326Q02JA   &lt;dbl&gt; 4, 2, 2, 3, 5, 2, 2, 5, 5, 5, 5, 3, 5, 2, 3, 3, 6, 4, 4, …\n$ ST326Q03JA   &lt;dbl&gt; 2, 3, 2, 2, 4, 1, 1, NA, 4, 4, 6, 5, 8, 1, 4, 3, 9, 4, 2,…\n$ ST326Q04JA   &lt;dbl&gt; 1, 1, 2, 1, 5, 1, 2, 3, 1, 2, 1, 2, 1, 2, 3, 2, 3, 2, 1, …\n$ ST326Q05JA   &lt;dbl&gt; NA, 1, 5, 1, 5, 2, 4, 5, 3, 5, 5, 5, 1, 9, 3, 1, 4, 6, 5,…\n$ ST326Q06JA   &lt;dbl&gt; 8, 1, 7, 1, 2, 5, 9, 7, 4, 5, 4, 7, 2, 9, 5, 1, 7, 7, 6, …\n$ ST326Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST326Q12JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST322Q01JA   &lt;dbl&gt; 5, 6, 3, 5, 4, 6, 5, 5, 4, NA, 4, 5, 5, 1, 2, 5, NA, 4, 5…\n$ ST322Q02JA   &lt;dbl&gt; 5, 1, 3, 5, 5, 5, 5, 5, NA, 4, 2, 5, 5, NA, NA, 5, 4, 3, …\n$ ST322Q03JA   &lt;dbl&gt; NA, 5, 2, 5, NA, 4, 1, 5, 3, 5, NA, 4, 3, 4, 4, NA, 4, 4,…\n$ ST322Q04JA   &lt;dbl&gt; 2, 5, 2, 1, 5, NA, NA, 5, 3, 3, 4, 5, NA, 5, 5, 2, 2, NA,…\n$ ST322Q06JA   &lt;dbl&gt; 1, 1, NA, NA, 4, 6, 1, NA, 6, 4, 1, NA, 1, 1, 4, 1, 3, 4,…\n$ ST322Q07JA   &lt;dbl&gt; 1, NA, 2, 1, 4, 2, 2, 1, 1, 5, 2, 3, 1, 1, 1, 1, 5, 3, 1,…\n$ ST307Q01JA   &lt;dbl&gt; 4, NA, NA, NA, 4, NA, 4, 5, NA, NA, 5, NA, 4, NA, NA, 4, …\n$ ST307Q02JA   &lt;dbl&gt; NA, 5, 4, NA, NA, 3, NA, NA, 5, NA, NA, 5, NA, 4, NA, NA,…\n$ ST307Q03JA   &lt;dbl&gt; 4, 4, 3, 4, 3, NA, NA, 4, 2, 3, 4, 5, NA, NA, 4, 5, NA, N…\n$ ST307Q04JA   &lt;dbl&gt; 2, NA, 3, 2, NA, NA, NA, 3, 4, 4, NA, NA, NA, 1, 4, 2, 3,…\n$ ST307Q05JA   &lt;dbl&gt; NA, 5, NA, NA, NA, NA, NA, NA, NA, NA, 3, 4, NA, 4, 4, NA…\n$ ST307Q06JA   &lt;dbl&gt; 2, NA, 2, 2, 5, 3, 1, 3, NA, NA, NA, NA, 2, NA, 3, NA, NA…\n$ ST307Q07JA   &lt;dbl&gt; 2, NA, NA, 2, NA, NA, 5, NA, 4, 4, 2, 1, 2, 1, NA, 2, NA,…\n$ ST307Q08JA   &lt;dbl&gt; NA, 3, NA, 4, NA, 4, 5, NA, NA, 4, NA, NA, NA, NA, NA, 4,…\n$ ST307Q09JA   &lt;dbl&gt; NA, NA, 3, NA, 4, 5, NA, NA, 4, NA, 4, 4, 4, 5, 3, NA, 3,…\n$ ST307Q10JA   &lt;dbl&gt; NA, 3, NA, NA, 5, 4, 2, 1, NA, 3, NA, NA, 3, NA, NA, NA, …\n$ ST309Q01JA   &lt;dbl&gt; NA, NA, 5, NA, 4, 4, 5, 4, 5, NA, 4, NA, 5, NA, 5, NA, 4,…\n$ ST309Q02JA   &lt;dbl&gt; 3, 5, NA, NA, 5, NA, NA, NA, 4, NA, NA, 2, 5, NA, NA, NA,…\n$ ST309Q03JA   &lt;dbl&gt; NA, 3, 3, NA, NA, NA, 2, 4, 1, 4, 2, 1, 1, 5, 2, NA, NA, …\n$ ST309Q04JA   &lt;dbl&gt; 3, NA, NA, NA, NA, 4, 5, NA, NA, NA, NA, NA, NA, 4, NA, N…\n$ ST309Q05JA   &lt;dbl&gt; NA, 4, 3, NA, 4, NA, NA, NA, 5, 4, NA, NA, NA, NA, 4, 1, …\n$ ST309Q06JA   &lt;dbl&gt; NA, NA, 4, 3, NA, 4, NA, NA, 4, NA, NA, 1, 3, NA, NA, 3, …\n$ ST309Q07JA   &lt;dbl&gt; 3, NA, NA, 4, NA, NA, 2, 1, NA, 4, 3, 1, NA, 4, 4, 3, NA,…\n$ ST309Q08JA   &lt;dbl&gt; 4, 3, NA, 4, 5, NA, NA, 4, NA, 4, 4, NA, NA, 2, 5, NA, 3,…\n$ ST309Q09JA   &lt;dbl&gt; 2, NA, 1, 4, NA, 2, NA, 2, NA, NA, 3, NA, 2, 4, NA, 4, NA…\n$ ST309Q10JA   &lt;dbl&gt; NA, 5, NA, 3, 4, 4, 4, NA, NA, 3, NA, 5, NA, NA, NA, 3, 4…\n$ ST301Q01JA   &lt;dbl&gt; 5, NA, NA, 4, 5, 4, 4, NA, 3, 5, NA, NA, 3, NA, NA, NA, N…\n$ ST301Q02JA   &lt;dbl&gt; NA, NA, NA, NA, 1, NA, NA, NA, NA, 4, 4, 5, NA, NA, NA, 5…\n$ ST301Q03JA   &lt;dbl&gt; NA, NA, 3, NA, 4, 4, 1, NA, 2, NA, 2, 4, NA, NA, 4, 2, 3,…\n$ ST301Q04JA   &lt;dbl&gt; 5, NA, NA, 4, 5, NA, NA, 5, NA, 4, NA, 5, NA, 4, NA, 4, N…\n$ ST301Q05JA   &lt;dbl&gt; NA, 4, NA, NA, 4, 4, NA, NA, 5, NA, 4, 4, 3, NA, NA, 4, 3…\n$ ST301Q06JA   &lt;dbl&gt; 5, 5, 3, 4, NA, NA, NA, NA, 3, 4, 4, 5, 2, 4, 5, NA, 3, 4…\n$ ST301Q07JA   &lt;dbl&gt; 5, NA, 3, 4, NA, 4, NA, 5, NA, NA, NA, NA, 2, 4, NA, 4, N…\n$ ST301Q08JA   &lt;dbl&gt; NA, 2, NA, 3, NA, NA, 4, 2, 3, 2, NA, NA, NA, 2, 2, NA, 3…\n$ ST301Q09JA   &lt;dbl&gt; NA, 3, 4, NA, NA, 4, 5, 5, NA, NA, 5, NA, NA, 4, 5, NA, N…\n$ ST301Q10JA   &lt;dbl&gt; 5, 4, 3, NA, NA, NA, 4, 5, NA, NA, NA, NA, 3, NA, 4, NA, …\n$ ST343Q01JA   &lt;dbl&gt; 4, NA, 3, NA, NA, NA, 5, NA, 5, NA, NA, 5, 4, NA, NA, NA,…\n$ ST343Q02JA   &lt;dbl&gt; NA, 2, NA, 3, NA, 1, NA, 4, NA, 3, NA, 2, NA, 4, NA, NA, …\n$ ST343Q03JA   &lt;dbl&gt; NA, NA, NA, NA, 4, NA, 3, NA, NA, NA, 4, 5, NA, NA, NA, N…\n$ ST343Q04JA   &lt;dbl&gt; 2, NA, NA, 2, NA, 1, 3, 1, NA, 2, NA, NA, 3, NA, 3, 2, 2,…\n$ ST343Q05JA   &lt;dbl&gt; 3, 3, 3, 2, 2, 1, NA, NA, 1, NA, 2, NA, NA, NA, 5, 2, 4, …\n$ ST343Q06JA   &lt;dbl&gt; 4, 4, 3, 4, 5, NA, NA, 1, 5, 5, 4, 5, 4, 5, NA, 5, 3, NA,…\n$ ST343Q07JA   &lt;dbl&gt; 1, 3, NA, 2, NA, NA, NA, 1, NA, 3, 3, NA, NA, 3, 1, 3, NA…\n$ ST343Q08JA   &lt;dbl&gt; NA, 5, NA, NA, 5, 5, NA, 2, 5, NA, NA, NA, NA, 2, 3, NA, …\n$ ST343Q09JA   &lt;dbl&gt; NA, NA, 4, NA, NA, 5, 2, NA, 5, NA, NA, 5, 3, 3, 3, NA, N…\n$ ST343Q10JA   &lt;dbl&gt; NA, NA, 2, NA, 2, NA, 2, NA, NA, 2, 3, NA, 3, NA, NA, 4, …\n$ ST311Q01JA   &lt;dbl&gt; NA, 2, NA, NA, 2, 1, 5, 2, NA, NA, 2, 1, 2, 2, NA, 2, NA,…\n$ ST311Q02JA   &lt;dbl&gt; 5, 4, 4, 4, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, 4, NA,…\n$ ST311Q03JA   &lt;dbl&gt; 5, NA, NA, NA, NA, NA, NA, 5, 5, 5, NA, NA, NA, NA, 5, NA…\n$ ST311Q04JA   &lt;dbl&gt; 5, 4, 4, 4, NA, 5, NA, NA, NA, NA, 4, 5, 4, NA, 4, NA, 3,…\n$ ST311Q05JA   &lt;dbl&gt; NA, 2, 2, 2, NA, 1, 3, NA, NA, 1, NA, NA, NA, 2, 1, 2, NA…\n$ ST311Q06JA   &lt;dbl&gt; NA, NA, NA, NA, 3, NA, 3, NA, 5, NA, 3, 5, NA, NA, NA, NA…\n$ ST311Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 1, 5, NA, NA, NA, 3, 3, 2, NA, 1,…\n$ ST311Q08JA   &lt;dbl&gt; NA, NA, 3, NA, 3, NA, 2, 5, 5, 4, 3, 5, 3, 4, 4, 5, NA, N…\n$ ST311Q09JA   &lt;dbl&gt; 3, 4, 3, 4, 4, NA, NA, NA, 5, 4, NA, NA, 4, NA, NA, 4, NA…\n$ ST311Q10JA   &lt;dbl&gt; 5, NA, NA, 3, 3, 3, NA, 5, 5, 4, 3, NA, NA, 4, NA, NA, 3,…\n$ ST315Q01JA   &lt;dbl&gt; NA, NA, NA, 4, NA, NA, 5, 4, NA, NA, 3, 4, 4, NA, NA, NA,…\n$ ST315Q02JA   &lt;dbl&gt; 4, 3, 3, 2, NA, 3, 1, NA, 5, NA, NA, 5, NA, NA, NA, 5, NA…\n$ ST315Q03JA   &lt;dbl&gt; 3, NA, NA, NA, NA, 5, NA, NA, NA, 4, NA, NA, NA, 2, 2, NA…\n$ ST315Q04JA   &lt;dbl&gt; 3, 3, 3, 4, 3, NA, NA, 1, NA, 4, 4, NA, 3, 2, NA, NA, NA,…\n$ ST315Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 1, 2, NA, 1, 2, 3, 4, NA,…\n$ ST315Q06JA   &lt;dbl&gt; 3, 3, 3, NA, NA, 5, 2, 1, NA, NA, 3, NA, NA, NA, NA, 5, N…\n$ ST315Q07JA   &lt;dbl&gt; NA, 2, NA, NA, 2, NA, 4, 5, 1, NA, 3, NA, NA, 3, NA, 1, N…\n$ ST315Q08JA   &lt;dbl&gt; NA, NA, 2, 1, 2, 3, NA, 1, 3, 4, NA, NA, 3, NA, 4, 5, 2, …\n$ ST315Q09JA   &lt;dbl&gt; NA, 3, NA, NA, 5, 5, 1, NA, 4, 4, 3, 5, NA, 3, 3, 5, 4, 3…\n$ ST315Q10JA   &lt;dbl&gt; 4, NA, 4, 4, 5, NA, NA, NA, NA, NA, NA, 5, 4, NA, 3, NA, …\n$ ST303Q01JA   &lt;dbl&gt; 5, NA, NA, NA, NA, NA, 4, NA, 5, 4, 4, 5, NA, NA, NA, NA,…\n$ ST303Q02JA   &lt;dbl&gt; 5, NA, 3, NA, 5, 4, NA, 5, NA, NA, 4, 5, NA, 4, NA, NA, 3…\n$ ST303Q03JA   &lt;dbl&gt; 5, NA, 2, 3, 4, NA, NA, 5, 5, 4, NA, NA, 3, 4, 3, 5, 3, N…\n$ ST303Q04JA   &lt;dbl&gt; NA, 4, 4, 4, 5, 5, NA, 5, 5, NA, 4, 5, 3, 4, 5, 5, NA, NA…\n$ ST303Q05JA   &lt;dbl&gt; NA, 3, NA, 2, NA, 1, 3, NA, NA, 4, NA, 1, NA, 2, 4, 1, NA…\n$ ST303Q06JA   &lt;dbl&gt; 5, 5, 4, 4, 5, NA, 4, 5, 5, 4, 4, NA, 3, NA, 5, 5, 3, 4, …\n$ ST303Q07JA   &lt;dbl&gt; 1, 4, 3, NA, NA, 2, 2, NA, 3, 3, 3, NA, 3, NA, NA, 1, NA,…\n$ ST303Q08JA   &lt;dbl&gt; NA, 4, NA, 3, 5, 5, 4, 5, NA, NA, NA, 5, 3, 4, 4, NA, 3, …\n$ ST305Q01JA   &lt;dbl&gt; NA, 2, NA, NA, 5, NA, 2, NA, 5, NA, NA, 5, NA, 5, 2, 5, 2…\n$ ST305Q02JA   &lt;dbl&gt; NA, NA, 2, NA, NA, NA, 4, 3, NA, 3, 4, 3, 2, NA, NA, 5, N…\n$ ST305Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 3, 2, 5, 4, 4, NA, 3, NA, 2, NA, …\n$ ST305Q04JA   &lt;dbl&gt; 3, NA, NA, NA, 5, 2, NA, NA, NA, NA, NA, 5, NA, NA, NA, 1…\n$ ST305Q05JA   &lt;dbl&gt; 4, 4, 3, 3, 2, 3, NA, NA, NA, 4, NA, NA, 2, 5, 2, NA, 3, …\n$ ST305Q06JA   &lt;dbl&gt; 3, NA, 3, 3, NA, 4, NA, NA, 5, NA, 4, NA, NA, 4, NA, 5, 3…\n$ ST305Q07JA   &lt;dbl&gt; 4, 4, NA, 3, NA, NA, NA, 4, 3, 4, 3, 1, NA, NA, 4, NA, 3,…\n$ ST305Q08JA   &lt;dbl&gt; 2, NA, NA, 3, NA, 5, 2, NA, NA, NA, NA, NA, 3, 3, NA, NA,…\n$ ST305Q09JA   &lt;dbl&gt; NA, 3, 2, NA, 3, 1, 4, 1, 3, NA, 3, 4, NA, NA, 2, 3, NA, …\n$ ST305Q10JA   &lt;dbl&gt; NA, 4, 3, 3, 3, NA, NA, 1, NA, 4, NA, NA, 3, 4, NA, NA, N…\n$ ST345Q01JA   &lt;dbl&gt; 3, 4, NA, 3, 5, NA, 3, 5, 4, 4, NA, 3, 5, NA, NA, NA, 4, …\n$ ST345Q02JA   &lt;dbl&gt; 5, NA, 4, 3, 3, NA, 5, NA, NA, NA, NA, 3, 2, NA, 4, NA, 3…\n$ ST345Q03JA   &lt;dbl&gt; 2, 4, NA, NA, NA, 4, NA, 4, NA, NA, NA, NA, 5, 2, NA, NA,…\n$ ST345Q04JA   &lt;dbl&gt; 1, NA, 4, NA, NA, 2, 3, NA, 4, 3, 4, 3, NA, NA, NA, 2, 3,…\n$ ST345Q05JA   &lt;dbl&gt; 3, NA, NA, 3, NA, NA, NA, 4, 4, NA, NA, 3, 1, NA, 3, NA, …\n$ ST345Q06JA   &lt;dbl&gt; NA, 2, NA, NA, 3, 5, NA, NA, 5, 4, NA, NA, NA, 5, 3, 4, N…\n$ ST345Q07JA   &lt;dbl&gt; NA, 5, 3, 3, 5, 5, NA, 3, NA, 5, 5, NA, NA, 1, 5, NA, 5, …\n$ ST345Q08JA   &lt;dbl&gt; NA, NA, 3, 3, NA, NA, NA, NA, NA, 3, 4, 5, NA, NA, NA, 5,…\n$ ST345Q09JA   &lt;dbl&gt; NA, NA, 3, NA, 2, NA, 5, NA, 5, NA, 4, NA, 1, 5, 2, 5, NA…\n$ ST345Q10JA   &lt;dbl&gt; NA, 4, NA, NA, NA, 3, 2, 4, NA, NA, 4, NA, NA, 2, NA, 1, …\n$ ST313Q01JA   &lt;dbl&gt; NA, NA, NA, 3, 2, NA, 5, NA, NA, 4, NA, 5, NA, 4, NA, 3, …\n$ ST313Q02JA   &lt;dbl&gt; 2, NA, 2, NA, NA, 1, 2, 1, 4, NA, NA, 1, 3, NA, NA, 3, NA…\n$ ST313Q03JA   &lt;dbl&gt; 2, 3, 3, 3, NA, NA, NA, NA, 3, NA, NA, NA, 2, 2, NA, NA, …\n$ ST313Q04JA   &lt;dbl&gt; 1, 4, NA, 3, 4, NA, 1, 2, 2, NA, 2, NA, NA, NA, 4, 3, 1, …\n$ ST313Q05JA   &lt;dbl&gt; 3, NA, 3, 3, NA, 1, NA, NA, 4, NA, 4, 4, 1, 5, 3, NA, 3, …\n$ ST313Q06JA   &lt;dbl&gt; NA, 4, 2, NA, 4, 1, 2, 2, NA, 2, 4, 2, 5, 2, 5, NA, 1, 2,…\n$ ST313Q07JA   &lt;dbl&gt; NA, NA, NA, NA, 4, 1, NA, 4, 3, 4, NA, NA, NA, NA, 3, NA,…\n$ ST313Q08JA   &lt;dbl&gt; NA, 4, NA, NA, NA, NA, NA, 2, NA, NA, 2, 1, NA, NA, NA, 3…\n$ ST313Q09JA   &lt;dbl&gt; 1, NA, 2, 3, 4, NA, 3, NA, NA, 4, 3, NA, 3, NA, NA, 3, 1,…\n$ ST313Q10JA   &lt;dbl&gt; NA, 4, NA, NA, NA, 1, NA, NA, NA, 3, NA, NA, NA, 2, 4, NA…\n$ ST263Q02JA   &lt;dbl&gt; 1, 1, 2, 2, 1, 4, 2, 3, 3, 3, 3, 2, 4, 3, 2, 2, 2, 2, 3, …\n$ ST263Q04JA   &lt;dbl&gt; 2, 1, 2, 3, 1, 1, 1, 3, 2, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, …\n$ ST263Q06JA   &lt;dbl&gt; 2, 1, 3, 2, 1, 1, 1, 3, 2, 3, 1, 1, 3, 3, 2, 2, 1, 2, 3, …\n$ ST263Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST016Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST059Q01TA   &lt;dbl&gt; 4, 10, 8, 10, 5, 7, 20, 18, 6, 3, 9, 10, 5, 12, 5, 7, 8, …\n$ ST059Q02JA   &lt;dbl&gt; 55, 45, 56, 11, 30, 26, 75, 75, 28, 15, 51, 63, 40, 75, 3…\n$ ST296Q01JA   &lt;dbl&gt; 1, 3, 2, 3, 4, 1, 1, 2, 1, 3, 3, 4, 3, 1, 2, 1, 3, 3, 2, …\n$ ST296Q02JA   &lt;dbl&gt; 1, 2, 3, 1, 3, 1, 1, 2, 1, 3, 3, 3, 2, 1, 4, 1, 3, 1, 1, …\n$ ST296Q03JA   &lt;dbl&gt; 2, 3, 3, 2, 4, 1, 1, 2, 1, 2, 3, 4, 3, 1, 3, 1, 3, 3, 2, …\n$ ST296Q04JA   &lt;dbl&gt; 3, 5, 5, 2, 4, 2, 2, 4, 4, 6, 4, 6, 6, 1, 5, 3, 4, 4, 3, …\n$ ST272Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST273Q01JA   &lt;dbl&gt; 3, 4, 3, 2, 4, 3, 2, 3, 2, 3, 3, 3, NA, 2, NA, NA, 4, 4, …\n$ ST273Q02JA   &lt;dbl&gt; NA, 3, 3, NA, NA, 3, 2, 2, 2, 3, 3, NA, 4, 2, 4, 4, NA, 3…\n$ ST273Q03JA   &lt;dbl&gt; NA, 4, NA, NA, 4, NA, NA, 3, 3, 4, NA, 4, 4, NA, 4, 1, 4,…\n$ ST273Q04JA   &lt;dbl&gt; 4, 4, 3, 3, 3, 3, 1, 3, NA, NA, 3, 4, 4, 2, 4, 4, 4, NA, …\n$ ST273Q05JA   &lt;dbl&gt; 4, NA, 4, 2, 3, 3, 2, NA, 3, 3, 3, NA, 4, NA, 2, NA, 4, 4…\n$ ST273Q06JA   &lt;dbl&gt; 3, NA, 3, 4, NA, NA, NA, 3, NA, 3, 3, 4, NA, 2, NA, 1, NA…\n$ ST273Q07JA   &lt;dbl&gt; 3, 4, NA, 4, 2, 3, 4, NA, 3, NA, NA, 4, 4, 2, 2, 1, 4, 4,…\n$ ST270Q01JA   &lt;dbl&gt; 3, 2, 3, 2, 1, 3, 2, 2, 1, 2, 2, 1, 3, 3, 1, 1, 2, 1, 4, …\n$ ST270Q02JA   &lt;dbl&gt; 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 3, 1, 1, 1, 2, 1, 2, …\n$ ST270Q03JA   &lt;dbl&gt; 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 3, 2, 1, 1, 2, 1, 1, …\n$ ST270Q04JA   &lt;dbl&gt; 3, 1, 3, 2, 1, 2, 3, 2, 1, 2, 2, 1, 4, 2, 1, 1, 2, 1, 3, …\n$ ST285Q01JA   &lt;dbl&gt; NA, 1, NA, 3, 3, 1, 1, 1, NA, NA, 2, 1, 1, 1, 4, 5, NA, 2…\n$ ST285Q02JA   &lt;dbl&gt; 3, NA, 2, 3, 4, NA, NA, 5, 5, 4, NA, NA, 3, NA, 4, 5, NA,…\n$ ST285Q03JA   &lt;dbl&gt; NA, NA, 3, 3, 2, 1, NA, NA, 4, 4, 3, NA, NA, 1, 4, 5, NA,…\n$ ST285Q04JA   &lt;dbl&gt; NA, 2, NA, NA, 5, NA, 3, NA, NA, 4, NA, 5, 3, NA, 4, NA, …\n$ ST285Q05JA   &lt;dbl&gt; 2, 2, 1, NA, NA, 5, NA, NA, NA, NA, 2, NA, NA, 2, NA, NA,…\n$ ST285Q06JA   &lt;dbl&gt; 3, 2, NA, 3, 5, NA, 4, 5, 5, NA, 2, 5, NA, NA, NA, NA, 4,…\n$ ST285Q07JA   &lt;dbl&gt; 1, NA, 3, NA, NA, 5, NA, NA, 5, 2, 2, 5, 4, 2, NA, 5, 4, …\n$ ST285Q08JA   &lt;dbl&gt; 4, NA, 2, 3, NA, 5, 4, 5, 5, NA, NA, 5, 5, NA, NA, NA, 4,…\n$ ST285Q09JA   &lt;dbl&gt; NA, 3, NA, NA, NA, NA, 4, 5, NA, 4, NA, NA, NA, 1, 4, 5, …\n$ ST283Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, 4, 1, NA, NA, 3, 2, 5, NA, NA, NA, 4,…\n$ ST283Q02JA   &lt;dbl&gt; NA, 3, NA, 3, NA, NA, 1, NA, 3, NA, 3, 5, NA, NA, NA, NA,…\n$ ST283Q03JA   &lt;dbl&gt; 3, 3, 1, NA, 5, 4, NA, 4, 5, 3, NA, 5, 3, 1, 4, 2, 4, NA,…\n$ ST283Q04JA   &lt;dbl&gt; 3, 3, 3, 3, NA, NA, NA, 5, NA, 3, NA, 5, NA, NA, 4, 5, NA…\n$ ST283Q05JA   &lt;dbl&gt; 3, 3, NA, 3, 5, NA, 3, NA, 5, 3, NA, NA, 4, NA, NA, NA, N…\n$ ST283Q06JA   &lt;dbl&gt; 2, 2, 1, NA, 5, 3, 1, 4, NA, NA, 2, NA, NA, 1, 4, NA, 4, …\n$ ST283Q07JA   &lt;dbl&gt; 4, NA, NA, 3, 5, 4, 3, 5, 5, NA, 3, NA, 3, 1, 4, NA, NA, …\n$ ST283Q08JA   &lt;dbl&gt; NA, NA, 2, 3, NA, NA, NA, 5, NA, NA, NA, 5, 2, 1, NA, 5, …\n$ ST283Q09JA   &lt;dbl&gt; NA, NA, 2, NA, 5, 4, NA, NA, 5, 3, 3, NA, 3, 1, 4, 5, 4, …\n$ ST275Q01WA   &lt;dbl&gt; NA, 3, 2, NA, NA, 2, 3, NA, 3, NA, 3, NA, NA, 4, 2, 1, 3,…\n$ ST275Q02WA   &lt;dbl&gt; 2, 4, NA, 2, 2, 2, NA, NA, NA, 3, NA, 1, NA, NA, NA, 3, 3…\n$ ST275Q03WA   &lt;dbl&gt; 2, 4, NA, 4, 2, 2, NA, NA, 2, 3, 3, NA, 3, 4, 1, 4, NA, N…\n$ ST275Q04WA   &lt;dbl&gt; 2, NA, 2, 2, NA, NA, 1, 1, 2, NA, NA, 1, NA, NA, 3, NA, N…\n$ ST275Q05WA   &lt;dbl&gt; NA, NA, 1, NA, 1, NA, 3, 1, NA, 2, NA, 1, 3, 2, 1, 1, 1, …\n$ ST275Q06WA   &lt;dbl&gt; 1, 4, NA, 3, 2, NA, NA, 1, 1, NA, NA, 1, NA, NA, 2, NA, N…\n$ ST275Q07WA   &lt;dbl&gt; NA, NA, NA, 1, NA, NA, NA, 1, 1, 2, 4, NA, 3, NA, NA, NA,…\n$ ST275Q08WA   &lt;dbl&gt; 4, NA, 2, NA, 2, 2, 2, NA, NA, NA, 2, 1, 4, 2, NA, NA, NA…\n$ ST275Q09WA   &lt;dbl&gt; NA, 2, 1, NA, NA, 1, 4, 1, NA, 2, 4, NA, 3, 2, NA, 1, 1, …\n$ ST276Q01JA   &lt;dbl&gt; NA, NA, NA, 2, NA, NA, NA, NA, NA, 2, 2, 1, 2, 2, 1, NA, …\n$ ST276Q02JA   &lt;dbl&gt; NA, NA, NA, NA, 2, 2, 2, 1, NA, NA, 2, NA, NA, NA, NA, NA…\n$ ST276Q03JA   &lt;dbl&gt; 4, 3, NA, NA, 2, NA, 2, NA, NA, NA, NA, 1, NA, 3, 1, 2, 3…\n$ ST276Q04JA   &lt;dbl&gt; NA, 2, NA, NA, 2, 2, 2, 1, 1, 2, NA, NA, NA, NA, NA, 2, 2…\n$ ST276Q05JA   &lt;dbl&gt; NA, NA, 2, 2, NA, NA, 2, 1, 2, NA, NA, 1, NA, 2, NA, 2, N…\n$ ST276Q06JA   &lt;dbl&gt; NA, 3, 1, NA, 2, NA, NA, NA, 1, 2, 2, NA, 1, NA, 1, NA, 2…\n$ ST276Q07JA   &lt;dbl&gt; 3, 3, NA, 2, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, 2, 2,…\n$ ST276Q08JA   &lt;dbl&gt; 3, NA, 3, 2, 2, 2, 4, 4, 4, NA, 3, 1, 4, NA, NA, NA, 4, 4…\n$ ST276Q09JA   &lt;dbl&gt; 3, NA, 4, 2, NA, 2, NA, 4, 1, 3, 2, 1, 3, 2, 3, 4, NA, NA…\n$ ST276Q10JA   &lt;dbl&gt; 3, 4, 3, NA, NA, NA, NA, NA, NA, 2, NA, NA, 3, 2, NA, NA,…\n$ ST268Q01JA   &lt;dbl&gt; 2, 4, 3, 2, 3, 3, 4, 4, 2, 3, 3, 4, 3, 4, 1, 3, 2, 3, 2, …\n$ ST268Q02JA   &lt;dbl&gt; 3, 3, 2, 3, 4, 3, 3, 2, 2, 2, 2, 3, 2, 2, 4, 3, 2, 3, 2, …\n$ ST268Q03JA   &lt;dbl&gt; 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 4, 2, 4, 2, 2, 2, 3, 3, …\n$ ST268Q04JA   &lt;dbl&gt; 2, 3, 3, 2, 1, 3, 4, 4, 2, 3, 4, 4, 3, 4, 1, 3, 2, 2, 2, …\n$ ST268Q05JA   &lt;dbl&gt; 3, 2, 2, 2, 3, 3, 4, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, …\n$ ST268Q06JA   &lt;dbl&gt; 3, 3, 2, 2, 2, 2, 3, 2, 2, 3, 3, 3, 2, 4, 1, 2, 2, 2, 3, …\n$ ST268Q07JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ ST268Q08JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ ST268Q09JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ ST290Q01WA   &lt;dbl&gt; NA, NA, NA, 3, NA, NA, NA, 3, NA, 3, 4, 4, NA, 4, 2, NA, …\n$ ST290Q02WA   &lt;dbl&gt; 3, 3, 3, 3, 2, NA, NA, NA, 4, NA, 4, 4, 3, 4, NA, NA, NA,…\n$ ST290Q03WA   &lt;dbl&gt; NA, NA, NA, NA, NA, 3, 4, 3, NA, NA, NA, 4, NA, 4, 2, NA,…\n$ ST290Q04WA   &lt;dbl&gt; 3, 3, 2, NA, 2, NA, NA, NA, 3, 3, NA, 4, 2, NA, 2, 3, 2, …\n$ ST290Q05WA   &lt;dbl&gt; 4, 3, 4, NA, 2, 4, NA, NA, NA, 3, 4, NA, 4, NA, NA, NA, N…\n$ ST290Q06WA   &lt;dbl&gt; NA, NA, 3, NA, NA, 2, 4, NA, NA, NA, 4, NA, 4, 4, NA, 4, …\n$ ST290Q07WA   &lt;dbl&gt; NA, 3, 4, 2, NA, 4, 4, 4, 4, 3, NA, NA, 4, NA, 2, 4, 4, N…\n$ ST290Q08WA   &lt;dbl&gt; 2, NA, NA, 2, 2, NA, 3, 4, 4, NA, NA, NA, NA, 4, 2, 4, NA…\n$ ST290Q09WA   &lt;dbl&gt; 4, 3, NA, 2, 4, 4, 4, 4, 4, 3, 4, NA, NA, NA, NA, 4, 4, 4…\n$ ST291Q01JA   &lt;dbl&gt; 3, NA, 2, 2, NA, NA, 3, NA, 4, NA, 4, NA, 3, 4, NA, 4, NA…\n$ ST291Q02JA   &lt;dbl&gt; 3, 3, NA, NA, 2, NA, 3, 4, 4, 3, NA, NA, 4, NA, 2, NA, 2,…\n$ ST291Q03JA   &lt;dbl&gt; 3, 3, 2, 2, 2, 2, NA, 4, 4, NA, 4, NA, NA, NA, NA, NA, 2,…\n$ ST291Q04JA   &lt;dbl&gt; NA, NA, 2, NA, NA, 2, NA, NA, NA, NA, NA, 4, NA, NA, NA, …\n$ ST291Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 4, 4, NA, NA, NA, 4, 2, 3, 2, 3, …\n$ ST291Q06JA   &lt;dbl&gt; NA, 3, 3, NA, 2, 2, 4, 4, 4, 2, 4, NA, NA, 4, 2, 3, NA, N…\n$ ST291Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, 3, NA, 4, 4, 2, NA, NA, 2, 3, 2, NA, …\n$ ST291Q08JA   &lt;dbl&gt; NA, 3, NA, 2, NA, 2, NA, NA, NA, NA, 3, 4, NA, 2, 2, 2, N…\n$ ST291Q09JA   &lt;dbl&gt; 3, 3, 1, 2, 2, NA, 3, NA, NA, 2, 3, 4, NA, NA, NA, NA, NA…\n$ ST291Q10JA   &lt;dbl&gt; 2, NA, NA, 2, 2, NA, NA, NA, NA, 3, NA, 4, 2, NA, NA, NA,…\n$ ST289Q01WA   &lt;dbl&gt; 2, NA, NA, NA, NA, NA, NA, NA, NA, NA, 5, NA, NA, 2, NA, …\n$ ST289Q02JA   &lt;dbl&gt; 5, NA, 5, NA, NA, NA, NA, NA, 5, NA, NA, 5, 3, 5, NA, NA,…\n$ ST289Q03WA   &lt;dbl&gt; NA, NA, NA, NA, 1, NA, NA, NA, NA, 1, NA, 5, 1, NA, 1, NA…\n$ ST289Q04JA   &lt;dbl&gt; NA, 5, 4, 1, NA, 4, 5, 5, 5, NA, NA, 5, NA, NA, NA, NA, N…\n$ ST289Q05WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 5, NA, NA, 4, NA, NA, NA, NA, 4, …\n$ ST289Q06JA   &lt;dbl&gt; NA, NA, NA, 4, NA, 5, NA, 5, 5, NA, 5, NA, NA, 5, NA, 5, …\n$ ST289Q07JA   &lt;dbl&gt; 5, NA, 3, NA, 4, 5, 5, 5, NA, NA, NA, 5, NA, NA, 4, NA, N…\n$ ST289Q08WA   &lt;dbl&gt; NA, 5, NA, NA, 3, NA, NA, 5, NA, 2, NA, NA, 1, NA, 4, 2, …\n$ ST289Q09WA   &lt;dbl&gt; 4, 5, NA, NA, NA, 5, 5, NA, NA, NA, 5, NA, 4, NA, NA, 5, …\n$ ST289Q10WA   &lt;dbl&gt; NA, 5, 5, 5, 4, 5, 5, NA, 5, 5, NA, NA, NA, 5, NA, NA, NA…\n$ ST289Q11WA   &lt;dbl&gt; 1, NA, NA, 4, 2, NA, NA, NA, NA, NA, 1, NA, NA, 2, NA, 2,…\n$ ST289Q14JA   &lt;dbl&gt; NA, 5, 2, 4, NA, NA, NA, 5, 5, 5, 4, 5, 1, NA, 2, NA, 3, …\n$ ST293Q01JA   &lt;dbl&gt; NA, NA, NA, 4, 5, 4, 3, NA, 5, 3, 4, 5, 3, NA, NA, NA, NA…\n$ ST293Q02JA   &lt;dbl&gt; NA, 5, 4, 5, 5, NA, NA, 3, NA, NA, NA, 5, 4, 2, 4, 3, 5, …\n$ ST293Q03JA   &lt;dbl&gt; 4, 5, 3, NA, NA, 5, NA, NA, NA, 3, 5, NA, 4, NA, NA, NA, …\n$ ST293Q04JA   &lt;dbl&gt; NA, NA, 3, NA, NA, NA, NA, 1, 1, NA, NA, NA, 2, NA, 4, NA…\n$ ST293Q05JA   &lt;dbl&gt; 3, 5, NA, NA, 5, 5, 1, 1, 5, 4, 5, 5, NA, 1, NA, NA, 3, 2…\n$ ST293Q06JA   &lt;dbl&gt; 2, NA, 4, NA, NA, NA, NA, NA, NA, NA, 5, 5, NA, 1, 5, 5, …\n$ ST293Q07JA   &lt;dbl&gt; 2, NA, NA, 3, NA, NA, 3, 3, 2, NA, NA, 1, NA, 4, NA, 3, N…\n$ ST293Q08JA   &lt;dbl&gt; NA, 4, NA, 5, 5, 5, 1, NA, 5, 4, NA, NA, 2, NA, 4, 5, 2, …\n$ ST293Q09JA   &lt;dbl&gt; 4, 2, 2, 5, 3, 3, 2, 5, NA, 3, 5, NA, NA, 5, 3, 4, NA, NA…\n$ ST292Q01JA   &lt;dbl&gt; 2, 2, 2, 2, 1, 2, 4, 4, 2, 2, 3, 2, 2, 3, 1, NA, 3, 2, 3,…\n$ ST292Q02JA   &lt;dbl&gt; 2, NA, 3, 2, 1, 4, 4, NA, 3, 2, 3, 2, 3, 3, 1, 3, 3, 3, 3…\n$ ST292Q03JA   &lt;dbl&gt; 3, 3, 3, 2, NA, NA, 4, 4, 3, 2, NA, 3, 2, 3, 1, 4, 3, 3, …\n$ ST292Q04JA   &lt;dbl&gt; 2, 3, 3, NA, 1, 4, 4, 4, 3, 2, 3, 3, 2, 3, 1, 4, 3, 3, 3,…\n$ ST292Q05JA   &lt;dbl&gt; NA, 1, 2, 2, 1, 4, 1, 4, NA, NA, 2, 2, 1, 3, NA, 4, NA, N…\n$ ST292Q06JA   &lt;dbl&gt; 3, 1, NA, 2, 1, 2, NA, 4, 3, 1, 3, NA, NA, NA, 1, 2, 1, 2…\n$ ST297Q01JA   &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, …\n$ ST297Q03JA   &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ ST297Q05JA   &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, …\n$ ST297Q06JA   &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ ST297Q07JA   &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, …\n$ ST297Q09JA   &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, …\n$ ST334Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST334Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST335Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST336Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST337Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST338Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST339Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST339Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST340Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST341Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST342Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST300Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, 5, NA, 5, NA, NA, 3, 5, NA, NA, NA, N…\n$ ST300Q02JA   &lt;dbl&gt; 5, NA, 5, 3, NA, 5, 5, NA, 5, 5, 5, 5, NA, NA, NA, NA, 5,…\n$ ST300Q03JA   &lt;dbl&gt; NA, NA, NA, 3, NA, 5, NA, NA, NA, NA, NA, 5, 1, NA, 4, NA…\n$ ST300Q04JA   &lt;dbl&gt; NA, 2, 2, 3, 5, NA, NA, NA, 1, NA, 1, NA, 1, NA, 3, NA, N…\n$ ST300Q05JA   &lt;dbl&gt; 4, NA, NA, NA, NA, NA, 4, 5, 5, 5, NA, NA, NA, 1, 3, 1, 2…\n$ ST300Q06JA   &lt;dbl&gt; 2, 3, 2, NA, 5, NA, NA, 5, 5, 5, 3, NA, 1, NA, 2, 1, NA, …\n$ ST300Q07JA   &lt;dbl&gt; 4, 3, NA, NA, 5, NA, 3, 4, NA, 5, NA, NA, NA, 4, NA, 5, 4…\n$ ST300Q08JA   &lt;dbl&gt; NA, NA, 3, NA, 5, NA, NA, 5, 5, NA, 3, 5, 1, 4, NA, 1, 2,…\n$ ST300Q09JA   &lt;dbl&gt; NA, 3, NA, 3, 5, 5, 2, NA, NA, NA, NA, NA, 1, 4, NA, 5, N…\n$ ST300Q10JA   &lt;dbl&gt; 4, 5, 1, 3, NA, 5, 1, NA, NA, 5, NA, 5, NA, 4, 2, NA, 1, …\n$ ST327Q01JA   &lt;dbl&gt; 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST327Q02JA   &lt;dbl&gt; 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST327Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST327Q04JA   &lt;dbl&gt; 2, 1, 3, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST327Q05JA   &lt;dbl&gt; 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 2, 1, 1, 1, …\n$ ST327Q06JA   &lt;dbl&gt; 1, 1, 3, 3, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ ST327Q07JA   &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1, 1, 1, 1, 1, NA, 3, 3,…\n$ ST327Q08JA   &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 2, 1, 1, 1, 1, NA, 3, 3,…\n$ ST330Q01WA   &lt;dbl&gt; 3, NA, NA, 3, 3, 3, NA, 3, 3, NA, 3, 3, 3, 1, NA, NA, NA,…\n$ ST330Q02WA   &lt;dbl&gt; 2, NA, 1, NA, 3, NA, 3, NA, 3, 3, NA, NA, 1, 1, 2, NA, NA…\n$ ST330Q03WA   &lt;dbl&gt; NA, NA, 3, 3, NA, 3, 3, NA, NA, NA, NA, 1, 3, NA, 3, 1, 3…\n$ ST330Q04WA   &lt;dbl&gt; NA, 1, NA, NA, 3, 3, NA, 2, NA, 1, NA, NA, NA, NA, NA, 3,…\n$ ST330Q05WA   &lt;dbl&gt; NA, 1, NA, NA, NA, NA, 3, NA, NA, NA, NA, 2, NA, 3, 3, NA…\n$ ST330Q06WA   &lt;dbl&gt; NA, 1, NA, NA, 2, NA, 2, NA, 3, 1, 3, 1, NA, NA, 2, 1, NA…\n$ ST330Q07WA   &lt;dbl&gt; 2, NA, 1, 3, NA, NA, 2, 2, 2, 1, NA, NA, NA, NA, NA, NA, …\n$ ST330Q08WA   &lt;dbl&gt; NA, NA, 3, 3, NA, NA, NA, NA, NA, NA, 3, NA, 3, 1, NA, NA…\n$ ST330Q09WA   &lt;dbl&gt; NA, 1, NA, NA, 2, 3, NA, 2, NA, NA, 2, NA, NA, NA, NA, 1,…\n$ ST330Q11WA   &lt;dbl&gt; 2, 1, NA, 3, NA, 3, NA, NA, NA, 1, 2, NA, NA, 3, NA, NA, …\n$ ST330D10WA   &lt;chr&gt; \"7020003\", \"9999999\", \"7020003\", \"9999999\", \"9999999\", \"9…\n$ ST324Q02JA   &lt;dbl&gt; 3, 3, NA, NA, 4, NA, 3, 2, 3, 3, 2, 3, 4, 3, 3, 4, NA, NA…\n$ ST324Q04JA   &lt;dbl&gt; 3, NA, NA, NA, 4, 3, NA, NA, NA, NA, 3, 4, 3, NA, NA, 3, …\n$ ST324Q05JA   &lt;dbl&gt; NA, NA, 2, 2, NA, 4, NA, 2, 3, 1, 2, 4, NA, 3, 4, NA, 3, …\n$ ST324Q07JA   &lt;dbl&gt; 4, NA, 3, NA, 2, NA, 3, NA, NA, 3, NA, 2, NA, NA, 3, 4, 3…\n$ ST324Q10JA   &lt;dbl&gt; NA, 2, 2, 2, NA, 3, NA, 2, 1, 2, NA, NA, NA, 3, 3, NA, 3,…\n$ ST324Q11JA   &lt;dbl&gt; NA, 1, 2, NA, NA, 2, NA, NA, NA, NA, NA, NA, 2, NA, NA, N…\n$ ST324Q12JA   &lt;dbl&gt; 3, NA, 3, 2, NA, NA, 2, NA, 4, NA, NA, 3, NA, 3, NA, NA, …\n$ ST324Q13JA   &lt;dbl&gt; NA, 1, NA, 2, 4, 3, 1, 2, 4, 3, 3, NA, 3, NA, 3, 1, 2, 3,…\n$ ST324Q14JA   &lt;dbl&gt; 2, 3, NA, 2, 2, NA, 2, 3, NA, NA, 3, NA, 2, 3, NA, 2, 2, …\n$ ST347Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST347Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST348Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST349Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST350Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST351Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST352Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST353Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST354Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST355Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST356Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ST331Q01JA   &lt;dbl&gt; 7, 8, 8, 8, 10, 6, 7, 10, 7, 8, 10, 9, 7, 6, 8, 4, 7, 8, …\n$ ST331Q02JA   &lt;dbl&gt; 8, 10, 9, 10, 10, 10, 8, 10, 10, 10, 10, 10, 9, 7, 10, 10…\n$ ST331Q03JA   &lt;dbl&gt; 8, 8, 7, 10, 10, 7, 8, 10, 9, 8, 9, 10, 10, 6, 10, 10, 8,…\n$ FL150Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL150Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL150Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q09HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q10HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q11HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q12HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q13HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q14HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q15HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL164Q16HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL166Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL174Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL167Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL170Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL159Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL159Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL159Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL159Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL160Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL160Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL160Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL160Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL161Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL161Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL161Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL162Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL163Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL171Q12JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL169Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL172Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL172Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL172Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FL172Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC170Q01JA   &lt;dbl&gt; 4, 4, 1, 2, 5, 3, 5, 3, 4, 4, 5, 2, 1, 5, 4, 4, 4, 1, 3, …\n$ IC170Q02JA   &lt;dbl&gt; 4, 4, 1, 4, 5, 6, 5, 6, 2, 5, 5, 4, 1, 5, 5, 1, 3, 4, 5, …\n$ IC170Q03JA   &lt;dbl&gt; 1, 4, 5, 1, 6, 3, 1, 5, 1, 6, 6, 4, 4, 5, 5, 1, 1, 4, 1, …\n$ IC170Q04JA   &lt;dbl&gt; 4, 4, 5, 1, 5, 3, 5, 5, 1, 4, 5, 4, 5, 5, 5, 4, 1, 4, 5, …\n$ IC170Q05JA   &lt;dbl&gt; 2, 4, 1, 1, 4, 3, 1, 5, 2, 4, 5, 4, 5, 5, 1, 4, 1, 1, 5, …\n$ IC170Q06JA   &lt;dbl&gt; 2, 4, 5, 1, 6, 3, 3, 5, 3, 1, 3, 4, 2, 5, 5, 2, 2, 4, 1, …\n$ IC170Q07JA   &lt;dbl&gt; 4, 4, 5, 1, 5, 3, 3, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 3, 3, …\n$ IC171Q01JA   &lt;dbl&gt; 4, 2, 4, 1, 4, 5, 5, 5, 4, 5, 5, 4, 3, 5, 2, 5, 1, 4, 5, …\n$ IC171Q02JA   &lt;dbl&gt; 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, …\n$ IC171Q03JA   &lt;dbl&gt; 1, 1, 4, 1, 1, 5, 1, 5, 4, 1, 6, 4, 4, 5, 5, 6, 1, 4, 1, …\n$ IC171Q04JA   &lt;dbl&gt; 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 4, 4, 5, 5, 5, 5, 4, 5, …\n$ IC171Q05JA   &lt;dbl&gt; 2, 1, 3, 1, 1, 1, 4, 5, 3, 1, 4, 4, 2, 5, 5, 1, 1, 4, 3, …\n$ IC171Q06JA   &lt;dbl&gt; 4, 1, 5, 1, 1, 5, 5, 5, 4, 5, 4, 4, 1, 5, 2, 5, 1, 4, 3, …\n$ IC172Q01JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, …\n$ IC172Q02JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 2, …\n$ IC172Q03JA   &lt;dbl&gt; 3, 4, 3, 4, 3, 4, 1, 3, 3, 2, 4, 2, 3, 3, 4, 4, 3, 3, 2, …\n$ IC172Q04JA   &lt;dbl&gt; 3, 4, 3, 4, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 4, 4, 3, 3, 2, …\n$ IC172Q05JA   &lt;dbl&gt; 3, 4, 3, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, …\n$ IC172Q06JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 4, 2, 3, 4, 3, 3, 4, 4, 3, 4, 4, 3, 3, 2, …\n$ IC172Q07JA   &lt;dbl&gt; 3, 4, 3, 4, 4, 3, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, …\n$ IC172Q08JA   &lt;dbl&gt; 3, 4, 3, 4, 4, 3, 2, 3, 3, 3, 3, 4, 2, 3, 4, 4, 3, 3, 3, …\n$ IC172Q09JA   &lt;dbl&gt; 3, 4, 4, 4, 4, 3, 4, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 2, …\n$ IC173Q01JA   &lt;dbl&gt; 3, 4, 4, 1, 5, 5, 3, 4, 5, 4, 3, 2, 3, 4, 5, 3, 5, 3, 2, …\n$ IC173Q02JA   &lt;dbl&gt; 1, 2, 5, 1, 1, 3, 5, 4, 3, 1, 1, 3, 1, 2, 2, 2, 2, 4, 1, …\n$ IC173Q03JA   &lt;dbl&gt; 2, 4, 4, 1, 5, 3, 3, 4, 2, 3, 3, 2, 3, 2, 2, 1, 2, 2, 1, …\n$ IC173Q04JA   &lt;dbl&gt; 6, 6, 6, 1, 6, 6, 6, 4, 6, 6, 6, 6, 6, 6, 6, 1, 6, 6, 6, …\n$ IC174Q01JA   &lt;dbl&gt; 2, 3, 1, 3, 3, 2, 1, 1, 2, 3, NA, 4, 1, 1, 3, 2, 2, 1, 1,…\n$ IC174Q02JA   &lt;dbl&gt; 4, 3, 4, 3, 5, 4, 3, 4, 4, 4, NA, 5, 4, 3, 1, 4, 4, 4, 4,…\n$ IC174Q03JA   &lt;dbl&gt; 3, 3, 3, 3, 5, 1, 1, 4, 3, 4, 2, 4, 1, 3, 4, 4, 2, 4, 4, …\n$ IC174Q04JA   &lt;dbl&gt; 2, 3, 4, 3, 3, 3, 3, 4, 3, 4, 2, 5, 3, 3, 2, 1, 2, 3, 1, …\n$ IC174Q05JA   &lt;dbl&gt; 1, 3, 1, 3, 3, 2, 1, 4, 3, 2, 2, 3, 1, 2, 1, 1, 2, 1, 1, …\n$ IC174Q06JA   &lt;dbl&gt; 1, 3, 1, 3, 5, 2, 1, 1, 3, 1, 3, 3, 1, 2, 1, 2, 2, 1, 1, …\n$ IC174Q07JA   &lt;dbl&gt; 2, 3, 1, 3, 5, 2, 5, 4, 3, 4, 3, 5, 4, 2, 1, 3, 2, 1, 1, …\n$ IC174Q08JA   &lt;dbl&gt; 2, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 5, 3, 2, 5, 3, 3, 1, 1, …\n$ IC174Q09JA   &lt;dbl&gt; 2, 3, 3, 3, 3, 3, 3, 1, 3, 4, 3, 4, 1, 3, 1, 4, 3, 1, 1, …\n$ IC174Q10JA   &lt;dbl&gt; 2, 3, 3, 3, 3, 3, 1, 4, 3, 3, 1, 4, 1, 1, 1, 1, 2, 1, 1, …\n$ IC175Q01JA   &lt;dbl&gt; 2, 4, 4, 1, 5, 4, 3, 5, 4, 4, 3, 4, 3, 3, 4, 5, 1, 3, 3, …\n$ IC175Q02JA   &lt;dbl&gt; 2, 3, 2, 2, 5, 5, 3, 5, 4, 4, 3, 4, 1, 3, 1, 5, 1, 1, 3, …\n$ IC175Q03JA   &lt;dbl&gt; 2, 3, 1, 1, 1, 3, 1, 5, 2, 4, 1, 4, 1, 3, 4, 5, 1, 1, 2, …\n$ IC175Q05JA   &lt;dbl&gt; 1, 3, 1, 2, 2, 3, 1, 5, 3, 4, 1, 5, 3, 3, 4, 3, 1, 1, 3, …\n$ IC176Q01JA   &lt;dbl&gt; 3, 2, 5, 1, 5, 3, 3, 5, 3, 5, 3, 4, 1, 3, 4, 4, 3, 4, 3, …\n$ IC176Q02JA   &lt;dbl&gt; 3, 2, 4, 1, 5, 4, 1, 5, 5, 5, 5, 5, 4, 3, 5, 5, 3, 3, 3, …\n$ IC176Q03JA   &lt;dbl&gt; 4, 2, 3, 1, 5, 4, 1, 5, 4, 5, 4, 5, 1, 3, 5, 5, 3, 4, 3, …\n$ IC176Q04JA   &lt;dbl&gt; 4, 3, 5, 2, 5, 4, 3, 5, 5, 5, 5, 5, 5, 3, 5, 5, 3, 5, 4, …\n$ IC176Q05JA   &lt;dbl&gt; 4, 2, 4, 2, 5, 4, 3, 5, 5, 4, 4, 5, 3, 3, 3, 5, 3, 5, 3, …\n$ IC176Q06JA   &lt;dbl&gt; 3, 1, 2, 1, 5, 4, 4, 5, 5, 2, 3, 5, 1, 3, 3, 3, 4, 5, 1, …\n$ IC176Q07JA   &lt;dbl&gt; 4, 2, 4, 1, 5, 4, 5, 5, 5, 4, 5, 5, 4, 3, 5, 3, 4, 5, 5, …\n$ IC176Q08JA   &lt;dbl&gt; 4, 3, 1, 3, 5, 3, 4, 5, 4, 5, 5, 4, 1, 3, 5, 4, 4, 5, 3, …\n$ IC184Q01JA   &lt;dbl&gt; 3, 3, 1, 5, 1, 4, 1, 1, 3, 4, 4, 5, 1, 5, 1, 4, 3, 5, 1, …\n$ IC184Q02JA   &lt;dbl&gt; 4, 3, 3, 4, 1, 4, 1, 1, 3, 4, 4, 5, 1, 5, 4, 4, 3, 4, 3, …\n$ IC184Q03JA   &lt;dbl&gt; 1, 3, 3, 1, 1, 3, 3, 1, 3, 6, 6, 4, 3, 5, 1, 1, 3, 3, 1, …\n$ IC184Q04JA   &lt;dbl&gt; 1, 3, 1, 1, 1, 3, 6, 1, 2, 6, 6, 2, 6, 1, 1, 1, 3, 6, 1, …\n$ IC177Q01JA   &lt;dbl&gt; 3, 2, 4, 2, 4, 4, 4, 3, 1, 5, 3, 2, 1, 4, 4, 3, 4, 3, 1, …\n$ IC177Q02JA   &lt;dbl&gt; 2, 3, 2, 2, 6, 4, 1, 3, 2, 5, 3, 3, 2, 2, 4, 3, 3, 5, 3, …\n$ IC177Q03JA   &lt;dbl&gt; 3, 3, 3, 2, 4, 4, 3, 3, 2, 5, 3, 3, 4, 4, 5, 3, 2, 4, 3, …\n$ IC177Q04JA   &lt;dbl&gt; 2, 3, 2, 2, 4, 4, 3, 1, 2, 3, 2, 2, 1, 3, 5, 2, 1, 3, 2, …\n$ IC177Q05JA   &lt;dbl&gt; 2, 3, 2, 2, 4, 4, 2, 1, 2, 2, 2, 4, 2, 2, 5, 1, 4, 4, 1, …\n$ IC177Q06JA   &lt;dbl&gt; 2, 3, 2, 2, 4, 1, 2, 1, 2, 4, 2, 2, 1, 3, 5, 3, 1, 3, 2, …\n$ IC177Q07JA   &lt;dbl&gt; 1, 3, 1, 2, 5, 1, 1, 3, 1, 3, 1, 2, 1, 1, 5, 2, 1, 1, 1, …\n$ IC178Q01JA   &lt;dbl&gt; 3, 3, 5, 1, 4, 4, 6, 3, 1, 6, 3, 3, 1, 5, 5, 4, 4, 4, 1, …\n$ IC178Q02JA   &lt;dbl&gt; 2, 3, 2, 1, 6, 4, 1, 3, 2, 6, 3, 4, 3, 2, 5, 3, 4, 5, 3, …\n$ IC178Q03JA   &lt;dbl&gt; 3, 3, 2, 1, 4, 4, 2, 3, 2, 6, 3, 4, 5, 5, 5, 3, 1, 4, 2, …\n$ IC178Q04JA   &lt;dbl&gt; 2, 3, 2, 1, 4, 4, 2, 1, 2, 2, 2, 3, 1, 2, 5, 2, 1, 3, 2, …\n$ IC178Q05JA   &lt;dbl&gt; 2, 3, 1, 1, 4, 4, 1, 1, 2, 5, 1, 4, 2, 2, 5, 2, 1, 4, 1, …\n$ IC178Q06JA   &lt;dbl&gt; 3, 3, 2, 1, 4, 1, 1, 1, 2, 3, 3, 4, 1, 2, 5, 2, 1, 3, 2, …\n$ IC178Q07JA   &lt;dbl&gt; 1, 3, 1, 1, 1, 1, 1, 3, 1, 2, 1, 2, 1, 1, 5, 2, 1, 1, 1, …\n$ IC179Q01JA   &lt;dbl&gt; 1, 3, 2, 1, 2, 4, 2, 2, 3, 1, 2, 1, 1, 1, 2, 1, 2, 3, 3, …\n$ IC179Q02JA   &lt;dbl&gt; 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1, …\n$ IC179Q03JA   &lt;dbl&gt; 3, 3, 3, 4, 4, 4, 4, 3, 3, 4, 3, 4, 3, 4, 3, 4, 2, 2, 3, …\n$ IC179Q04JA   &lt;dbl&gt; 2, 3, 2, 1, 2, 4, 2, 2, 4, 1, 2, 4, 1, 3, 3, 3, 2, 2, 2, …\n$ IC179Q05JA   &lt;dbl&gt; 2, 3, 2, 1, 4, 4, 2, 2, 4, 1, 3, 4, 1, 3, 4, 3, 2, 2, 3, …\n$ IC179Q06JA   &lt;dbl&gt; 1, 3, 3, 2, 3, 4, 3, 3, 4, 1, 3, 1, 1, 1, 2, 1, 2, 2, 3, …\n$ IC180Q01JA   &lt;dbl&gt; 3, 2, 2, 2, 3, 4, 2, 2, 2, 3, 2, 2, 2, 3, 2, 1, 3, 2, 2, …\n$ IC180Q02JA   &lt;dbl&gt; 3, 3, 3, 2, 3, 2, 3, 3, 2, 3, 3, 4, 3, 3, 4, 4, 2, 3, 3, …\n$ IC180Q03JA   &lt;dbl&gt; 3, 3, 3, 2, 3, 4, 4, 3, 3, 3, 3, 4, 3, 3, 4, 4, 2, 3, 3, …\n$ IC180Q04JA   &lt;dbl&gt; 2, 2, 2, 2, 3, 1, 2, 3, 3, 3, 3, 4, 1, 1, 1, 2, 2, 3, 2, …\n$ IC180Q05JA   &lt;dbl&gt; 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 4, 1, 1, NA, 2, 2, 3, 3,…\n$ IC180Q06JA   &lt;dbl&gt; 3, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 4, 1, 1, 3, 2, 2, 3, 2, …\n$ IC180Q07JA   &lt;dbl&gt; 3, 2, 2, 2, 2, 4, 1, 3, 3, 3, 2, 2, 3, 2, 1, 4, 2, 3, 2, …\n$ IC180Q08JA   &lt;dbl&gt; 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, …\n$ IC181Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC181Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC181Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC181Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IC182Q01JA   &lt;dbl&gt; 3, 3, 2, 4, 2, 2, 3, 2, 2, 3, 3, 4, 3, 3, 3, 4, 2, 3, 2, …\n$ IC182Q02JA   &lt;dbl&gt; 3, 2, 3, 4, 2, 2, 4, 2, 2, 2, 3, 3, 3, 2, 3, 3, 2, 3, 3, …\n$ IC182Q03JA   &lt;dbl&gt; 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 3, 3, 4, 4, 2, 3, 3, …\n$ IC183Q01JA   &lt;dbl&gt; 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ IC183Q02JA   &lt;dbl&gt; 4, 4, 5, 3, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 4, …\n$ IC183Q03JA   &lt;dbl&gt; 4, 4, 2, 3, 4, 4, 3, 4, 4, 3, 3, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ IC183Q04JA   &lt;dbl&gt; 4, 4, 4, 3, 4, 4, 2, 4, 4, 4, 4, 4, 3, 4, 2, 4, 4, 3, 4, …\n$ IC183Q05JA   &lt;dbl&gt; 4, 4, 3, 3, 4, 3, 3, 1, 4, 4, 3, 4, 3, 4, 3, 4, 3, 3, 4, …\n$ IC183Q07JA   &lt;dbl&gt; 4, 4, 3, 3, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, …\n$ IC183Q08JA   &lt;dbl&gt; 4, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 2, 4, 4, 3, 4, …\n$ IC183Q09JA   &lt;dbl&gt; 4, 4, 1, 3, 4, 4, 2, 4, 4, 4, 3, 4, 2, 4, 3, 4, 4, 3, 4, …\n$ IC183Q10JA   &lt;dbl&gt; 4, 2, 1, 3, 4, 4, 2, 4, 3, 2, 3, 4, 1, 4, 2, 4, 1, 4, 2, …\n$ IC183Q12JA   &lt;dbl&gt; 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, …\n$ IC183Q13JA   &lt;dbl&gt; 4, 4, 2, 3, 4, 4, 4, 1, 4, 3, 3, 4, 2, 1, 4, 4, 1, 3, 4, …\n$ IC183Q14JA   &lt;dbl&gt; 2, 4, 1, 3, 5, 2, 2, 1, 2, 5, 2, 2, 1, 1, 1, 2, 1, 2, 1, …\n$ IC183Q15JA   &lt;dbl&gt; 2, 4, 1, 3, 5, 3, 1, 4, 2, 5, 3, 3, 1, 1, 1, 4, 1, 2, 1, …\n$ IC183Q16JA   &lt;dbl&gt; 3, 4, 3, 3, 5, 3, 2, 4, 3, 5, 3, 3, 1, 1, 1, 4, 1, 2, 1, …\n$ WB150Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB151Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB152Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB153Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB154Q09HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q09HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB155Q10HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB156Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB158Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB160Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB161Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB162Q09HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB163Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB164Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB165Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB166Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB166Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB166Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB166Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB167Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB168Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB168Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB168Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB168Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB171Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB171Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB171Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB171Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB172Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB173Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB173Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB173Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB173Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB176Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB177Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB177Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB177Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB177Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB032Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB032Q02NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB031Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ WB178Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA001Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA001Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA001Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q05IA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q18WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q19WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q20WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q11JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q12JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q13JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q14JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q15JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q16JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA003Q17JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA196Q01WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA196Q02WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA196Q03WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA196Q04WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q01WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q02WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q03WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q04WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA197Q05WA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q04TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q05TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q06NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q07NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q08NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q09NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA008Q10NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q02NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q03NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q04NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q05NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q06NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q07NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q08NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q09NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q10NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA009Q11NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q04TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q05TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q06TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q07TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q09NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q11NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q12NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q13NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q14NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA007Q15NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA005Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q02TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q03TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q04TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q05TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q06TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q07TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q08TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q09TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q10TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q11TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q12HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q13HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA006Q14HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA166Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA167Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA167Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA167Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA167Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA183Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA018Q01NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA018Q02NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA018Q03NA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q03HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q04HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q05HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q06HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q07HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA177Q08HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA180Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA182Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA175Q01HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA175Q02HA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA175Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA175Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA185Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q07JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA186Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA187Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA187Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q08JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA188Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q02JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q03JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q04JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q05JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q06JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q09JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA189Q10JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA194Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA195Q01JA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA041Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PA042Q01TA   &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ EFFORT1      &lt;dbl&gt; 7, 8, 7, NA, 10, NA, 7, 10, 7, 7, 10, 9, 7, 6, 8, NA, 6, …\n$ EFFORT2      &lt;dbl&gt; 9, 8, 8, NA, 10, NA, 8, 10, 10, 10, 10, 10, 10, 7, 10, NA…\n$ OCOD1        &lt;chr&gt; \"9701\", \"31\", \"9701\", \"41\", \"23\", \"9701\", \"11\", \"23\", \"1\"…\n$ OCOD2        &lt;chr&gt; \"83\", \"21\", \"9704\", \"9705\", \"83\", \"34\", \"31\", \"21\", \"14\",…\n$ OCOD3        &lt;chr&gt; \"2634\", \"9705\", \"9704\", \"2411\", \"21\", \"22\", \"2512\", \"2310…\n$ PROGN        &lt;chr&gt; \"07020002\", \"07020002\", \"07020002\", \"07020002\", \"07020002…\n$ AGE          &lt;dbl&gt; 15.50, 15.83, 15.75, 16.17, 15.58, 15.58, 16.08, 16.00, 1…\n$ GRADE        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ ISCEDP       &lt;dbl&gt; 344, 344, 344, 344, 344, 344, 344, 344, 344, 344, 344, 34…\n$ IMMIG        &lt;dbl&gt; 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 3, 1, 2, 3, 3, 1, …\n$ COBN_S       &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200…\n$ COBN_M       &lt;chr&gt; \"070200\", \"070200\", \"970200\", \"070200\", \"070200\", \"970200…\n$ COBN_F       &lt;chr&gt; \"070200\", \"070200\", \"070200\", \"070200\", \"070200\", \"970200…\n$ LANGN        &lt;dbl&gt; 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 998, 99…\n$ REPEAT       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MISSSC       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ SKIPPING     &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ TARDYSD      &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ EXERPRAC     &lt;dbl&gt; 1, 4, 2, 5, 9, 1, 2, 0, 3, 5, 1, 2, 5, 2, 4, 0, 2, 3, 2, …\n$ STUDYHMW     &lt;dbl&gt; 4, 7, 3, 5, 7, 10, 0, 10, 5, 3, 5, 5, 10, 0, 8, 5, 5, 5, …\n$ WORKPAY      &lt;dbl&gt; 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ WORKHOME     &lt;dbl&gt; 10, 2, 0, 10, 5, 5, 7, 0, 0, 4, 2, 2, 10, 0, 10, 0, 5, 5,…\n$ EXPECEDU     &lt;dbl&gt; 7, 7, 6, NA, 6, 7, 9, 9, 7, 6, 7, 8, 9, 9, 9, 9, 7, 7, 7,…\n$ MATHPREF     &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ MATHEASE     &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, …\n$ MATHMOT      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ DURECEC      &lt;dbl&gt; 3, 2, NA, 3, NA, 4, 3, NA, NA, 3, NA, 5, NA, 5, 3, 2, NA,…\n$ BSMJ         &lt;dbl&gt; 85.85, NA, NA, 76.65, 79.49, 76.98, 74.66, 85.41, 24.79, …\n$ SISCO        &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ RELATST      &lt;dbl&gt; -0.2606, 1.2437, 0.7190, -0.2194, 1.0726, 1.1296, -1.0841…\n$ BELONG       &lt;dbl&gt; 0.2442, -0.0437, -0.6137, -1.1147, 2.1143, 0.5159, -0.718…\n$ BULLIED      &lt;dbl&gt; -1.2280, -0.2016, -1.2280, -1.2280, -1.2280, -1.2280, 1.0…\n$ FEELSAFE     &lt;dbl&gt; -0.7560, 1.1246, 0.4417, -0.7560, 1.1246, 0.1413, 0.0936,…\n$ SCHRISK      &lt;dbl&gt; -0.6386, 0.1810, -0.6386, -0.6386, -0.6386, 0.1810, 0.181…\n$ PERSEVAGR    &lt;dbl&gt; 0.4369, 0.4540, -0.4017, 0.5617, -0.5954, -0.1554, 0.4375…\n$ CURIOAGR     &lt;dbl&gt; 2.7951, 0.3058, -0.6563, 0.1778, 0.3406, 0.3247, 0.0134, …\n$ COOPAGR      &lt;dbl&gt; -0.0319, 0.1187, -0.6986, 0.0849, 0.8806, 4.8203, -0.7118…\n$ EMPATAGR     &lt;dbl&gt; 1.3979, 0.1290, -0.2087, -0.1344, -0.5172, -0.5881, -1.26…\n$ ASSERAGR     &lt;dbl&gt; -0.2970, -0.1734, -0.4816, -0.1538, 0.2470, -0.8113, -0.1…\n$ STRESAGR     &lt;dbl&gt; 0.9777, -0.7402, -0.2053, 0.2201, -0.9940, 0.2183, 0.4922…\n$ EMOCOAGR     &lt;dbl&gt; 1.2321, -0.5609, 0.5777, -0.0563, -0.8597, NA, 0.8646, 1.…\n$ GROSAGR      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ INFOSEEK     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FAMSUP       &lt;dbl&gt; -0.3780, -0.5969, -1.0537, -0.8521, 1.7459, 1.7327, -0.88…\n$ DISCLIM      &lt;dbl&gt; 0.3884, 1.1687, 0.2002, -0.1219, 0.1166, -0.1006, -0.8709…\n$ TEACHSUP     &lt;dbl&gt; -0.5635, 0.1475, -0.5635, -0.1002, 1.5558, 0.1475, -0.332…\n$ COGACRCO     &lt;dbl&gt; -0.4092, -0.7102, -0.6541, -0.0820, 0.5051, 0.2635, 0.142…\n$ COGACMCO     &lt;dbl&gt; 0.1666, 0.0024, -0.5604, 0.1868, 2.3426, 0.7819, -0.5560,…\n$ EXPOFA       &lt;dbl&gt; 0.3356, -1.2147, 0.6908, -0.2194, 0.5478, 0.5515, -0.1287…\n$ EXPO21ST     &lt;dbl&gt; -0.7746, -0.5240, 0.0949, 0.4959, 0.4121, 0.4936, 0.2869,…\n$ MATHEFF      &lt;dbl&gt; 0.1429, -0.2874, 0.2226, -0.9344, -0.9322, 0.2637, 1.2000…\n$ MATHEF21     &lt;dbl&gt; 0.4317, 0.7644, -0.4779, -0.5515, -0.5730, -0.2489, 1.401…\n$ FAMCON       &lt;dbl&gt; 1.3180, 4.7588, 0.4736, 0.0594, 0.8973, 2.2322, 3.6192, 4…\n$ ANXMAT       &lt;dbl&gt; 0.3729, 0.6647, 0.0510, 0.6387, 2.5026, -0.7358, -0.9755,…\n$ MATHPERS     &lt;dbl&gt; -0.1305, 0.6178, -0.3993, 1.8158, 1.1353, 0.9918, -1.5183…\n$ CREATEFF     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATSCH     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATFAM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATAS      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATOOS     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATOP      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ OPENART      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IMAGINE      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SCHSUST      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ LEARRES      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PROBSELF     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FAMSUPSL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FEELLAH      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SDLEFF       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MISCED       &lt;dbl&gt; 8, 7, 4, 6, 7, 7, 6, 9, 8, 8, 4, 9, 10, 6, 4, 9, 8, 8, 6,…\n$ FISCED       &lt;dbl&gt; 7, 7, 4, 6, 7, 9, 2, 8, 8, 7, 4, 9, 10, 9, 6, 4, 9, 8, 6,…\n$ HISCED       &lt;dbl&gt; 8, 7, 4, 6, 7, 9, 6, 9, 8, 8, 4, 9, 10, 9, 6, 9, 9, 8, 6,…\n$ PAREDINT     &lt;dbl&gt; 16.0, 14.5, 12.0, 12.0, 14.5, 16.0, 12.0, 16.0, 16.0, 16.…\n$ BMMJ1        &lt;dbl&gt; 17.00, 37.83, 17.00, 43.33, 75.54, 17.00, 70.34, 75.54, 6…\n$ BFMJ2        &lt;dbl&gt; 30.34, 77.10, NA, NA, 30.34, 57.64, 40.54, 80.78, 43.85, …\n$ HISEI        &lt;dbl&gt; 30.34, 77.10, 17.00, 43.33, 75.54, 57.64, 70.34, 80.78, 6…\n$ ICTRES       &lt;dbl&gt; 0.1940, 0.6249, -0.3987, -0.9028, 0.2514, -0.4733, 0.9904…\n$ HOMEPOS      &lt;dbl&gt; 0.7524, 0.7842, 0.0666, -0.9300, -0.8949, -0.5988, 0.0975…\n$ ESCS         &lt;dbl&gt; 0.1836, 0.8261, -1.0357, -0.9606, 0.0856, 0.1268, -0.0154…\n$ FCFMLRTY     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLSCHOOL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLMULTSB     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLFAMILY     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ACCESSFP     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLCONFIN     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FLCONICT     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ACCESSFA     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ATTCONFM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FRINFLFM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ICTSCH       &lt;dbl&gt; 0.4062, 0.4062, 0.4062, 0.4062, -1.6647, -0.8411, 0.4062,…\n$ ICTAVSCH     &lt;dbl&gt; 7, 7, 7, 7, 5, 6, 7, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, …\n$ ICTHOME      &lt;dbl&gt; 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0.3346, 0…\n$ ICTAVHOM     &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, …\n$ ICTQUAL      &lt;dbl&gt; 0.3623, 2.8889, 1.2313, 2.8889, 1.8870, 1.6615, -0.2086, …\n$ ICTSUBJ      &lt;dbl&gt; -0.1315, 0.5048, 0.9454, -2.0101, 0.6775, 0.6012, 0.5918,…\n$ ICTENQ       &lt;dbl&gt; -0.3767, 0.3109, -0.1658, 0.3109, 1.0316, -0.0477, -0.021…\n$ ICTFEED      &lt;dbl&gt; -0.4038, 0.4297, -0.4292, -0.6744, 0.2776, 0.9257, -0.354…\n$ ICTOUT       &lt;dbl&gt; 0.2260, -0.8080, 0.1088, -1.2894, 2.9804, 0.3464, -0.4834…\n$ ICTWKDY      &lt;dbl&gt; -0.4469, 0.4182, -0.3710, -0.5032, 1.3145, 0.4565, -0.250…\n$ ICTWKEND     &lt;dbl&gt; -0.3452, 0.3311, -0.7926, -3.5000, 0.8948, 0.4976, -1.085…\n$ ICTREG       &lt;dbl&gt; -0.1855, 0.9444, 0.2941, -0.6148, 0.8255, 1.9301, 0.3353,…\n$ ICTINFO      &lt;dbl&gt; 0.2929, -0.4797, 0.0811, -0.8360, 0.4191, 0.2147, 0.1939,…\n$ ICTDISTR     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ICTEFFIC     &lt;dbl&gt; 0.5764, 0.7781, -0.8446, -0.5172, 1.0613, 0.3941, -0.5344…\n$ STUBMI       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ BODYIMA      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SOCONPA      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ LIFESAT      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PSYCHSYM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SOCCON       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ EXPWB        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CURSUPP      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PQMIMP       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PQMCAR       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PARINVOL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PQSCHOOL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PASCHPOL     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ATTIMMP      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ PAREXPT      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATHME     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATACT     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATOPN     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREATOR      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ W_FSTUWT     &lt;dbl&gt; 5.35943, 6.74318, 6.77500, 5.43200, 5.82353, 5.69931, 4.9…\n$ W_FSTURWT1   &lt;dbl&gt; 8.20073, 9.88533, 10.16250, 7.98824, 9.00000, 2.68850, 2.…\n$ W_FSTURWT2   &lt;dbl&gt; 2.63139, 10.36607, 10.16250, 2.66275, 8.48571, 8.76685, 2…\n$ W_FSTURWT3   &lt;dbl&gt; 8.03388, 10.33466, 10.16250, 8.31429, 2.82857, 8.34850, 2…\n$ W_FSTURWT4   &lt;dbl&gt; 8.18690, 3.44489, 3.38750, 7.98824, 9.00000, 8.40156, 7.3…\n$ W_FSTURWT5   &lt;dbl&gt; 7.89416, 3.29511, 3.38750, 2.56226, 8.48571, 2.89878, 7.5…\n$ W_FSTURWT6   &lt;dbl&gt; 8.04452, 3.44489, 3.38750, 2.56226, 8.48571, 9.60179, 2.4…\n$ W_FSTURWT7   &lt;dbl&gt; 2.67796, 3.29830, 3.38750, 2.66275, 9.00000, 2.92228, 7.5…\n$ W_FSTURWT8   &lt;dbl&gt; 2.72897, 10.36607, 10.16250, 2.66275, 3.00000, 8.76685, 7…\n$ W_FSTURWT9   &lt;dbl&gt; 7.74437, 3.29511, 3.38750, 7.98824, 2.82857, 3.05511, 2.4…\n$ W_FSTURWT10  &lt;dbl&gt; 2.67796, 9.89489, 10.16250, 2.66275, 8.48571, 2.89878, 2.…\n$ W_FSTURWT11  &lt;dbl&gt; 2.73358, 3.29830, 3.38750, 8.31429, 2.82857, 3.02482, 7.3…\n$ W_FSTURWT12  &lt;dbl&gt; 8.18690, 9.88533, 10.16250, 2.66275, 2.82857, 2.80052, 7.…\n$ W_FSTURWT13  &lt;dbl&gt; 8.20073, 10.33466, 10.16250, 7.68679, 9.00000, 8.02740, 7…\n$ W_FSTURWT14  &lt;dbl&gt; 2.58146, 10.36607, 10.16250, 7.68679, 8.48571, 8.34850, 7…\n$ W_FSTURWT15  &lt;dbl&gt; 2.68151, 9.89489, 10.16250, 7.98824, 3.00000, 2.58510, 2.…\n$ W_FSTURWT16  &lt;dbl&gt; 2.63139, 3.45536, 3.38750, 8.31429, 2.82857, 8.40156, 7.5…\n$ W_FSTURWT17  &lt;dbl&gt; 2.73358, 3.45536, 3.38750, 2.66275, 3.00000, 8.69635, 2.4…\n$ W_FSTURWT18  &lt;dbl&gt; 7.89416, 9.88533, 10.16250, 2.77143, 3.00000, 2.89878, 7.…\n$ W_FSTURWT19  &lt;dbl&gt; 2.72897, 3.29830, 3.38750, 7.68679, 9.00000, 2.78283, 2.5…\n$ W_FSTURWT20  &lt;dbl&gt; 8.03388, 3.44489, 3.38750, 2.77143, 3.00000, 8.34850, 2.5…\n$ W_FSTURWT21  &lt;dbl&gt; 7.89416, 9.88533, 3.38750, 2.66275, 9.00000, 2.92228, 2.4…\n$ W_FSTURWT22  &lt;dbl&gt; 2.63139, 10.36607, 3.38750, 7.98824, 8.48571, 8.06550, 2.…\n$ W_FSTURWT23  &lt;dbl&gt; 8.03388, 10.33466, 3.38750, 2.77143, 2.82857, 7.73009, 2.…\n$ W_FSTURWT24  &lt;dbl&gt; 7.88647, 3.44489, 10.16250, 2.66275, 9.00000, 9.16534, 7.…\n$ W_FSTURWT25  &lt;dbl&gt; 7.89416, 3.29511, 10.16250, 7.68679, 8.48571, 2.89878, 7.…\n$ W_FSTURWT26  &lt;dbl&gt; 8.36312, 3.44489, 10.16250, 7.68679, 8.48571, 8.76685, 2.…\n$ W_FSTURWT27  &lt;dbl&gt; 2.67796, 3.29830, 10.16250, 7.98824, 9.00000, 2.92228, 7.…\n$ W_FSTURWT28  &lt;dbl&gt; 2.62882, 10.36607, 3.38750, 7.98824, 3.00000, 8.76685, 7.…\n$ W_FSTURWT29  &lt;dbl&gt; 8.03388, 3.29511, 10.16250, 2.66275, 2.82857, 2.80052, 2.…\n$ W_FSTURWT30  &lt;dbl&gt; 2.78196, 9.89489, 3.38750, 7.98824, 8.48571, 2.89878, 2.4…\n$ W_FSTURWT31  &lt;dbl&gt; 2.73358, 3.29830, 10.16250, 2.77143, 2.82857, 2.78283, 7.…\n$ W_FSTURWT32  &lt;dbl&gt; 8.18690, 9.88533, 3.38750, 7.98824, 2.82857, 2.80052, 7.5…\n$ W_FSTURWT33  &lt;dbl&gt; 8.20073, 10.33466, 3.38750, 2.56226, 9.00000, 8.02740, 7.…\n$ W_FSTURWT34  &lt;dbl&gt; 2.67796, 10.36607, 3.38750, 2.56226, 8.48571, 8.34850, 7.…\n$ W_FSTURWT35  &lt;dbl&gt; 2.68151, 9.89489, 3.38750, 2.66275, 3.00000, 2.80052, 2.5…\n$ W_FSTURWT36  &lt;dbl&gt; 2.73358, 3.45536, 10.16250, 2.77143, 2.82857, 8.40156, 7.…\n$ W_FSTURWT37  &lt;dbl&gt; 2.63139, 3.45536, 10.16250, 7.98824, 3.00000, 9.48693, 2.…\n$ W_FSTURWT38  &lt;dbl&gt; 7.89416, 9.88533, 3.38750, 8.31429, 3.00000, 2.89878, 7.3…\n$ W_FSTURWT39  &lt;dbl&gt; 2.62882, 3.29830, 10.16250, 2.56226, 9.00000, 2.78283, 2.…\n$ W_FSTURWT40  &lt;dbl&gt; 8.03388, 3.44489, 10.16250, 8.31429, 3.00000, 9.07446, 2.…\n$ W_FSTURWT41  &lt;dbl&gt; 8.20073, 9.88533, 3.38750, 2.66275, 2.82857, 9.07446, 7.5…\n$ W_FSTURWT42  &lt;dbl&gt; 2.63139, 10.36607, 3.38750, 7.98824, 3.00000, 2.78283, 7.…\n$ W_FSTURWT43  &lt;dbl&gt; 8.03388, 10.33466, 3.38750, 2.77143, 9.00000, 2.92228, 7.…\n$ W_FSTURWT44  &lt;dbl&gt; 8.18690, 3.44489, 10.16250, 2.66275, 2.82857, 2.89878, 2.…\n$ W_FSTURWT45  &lt;dbl&gt; 7.89416, 3.29511, 10.16250, 7.68679, 3.00000, 8.40156, 2.…\n$ W_FSTURWT46  &lt;dbl&gt; 8.04452, 3.44489, 10.16250, 7.68679, 3.00000, 2.57670, 7.…\n$ W_FSTURWT47  &lt;dbl&gt; 2.67796, 3.29830, 10.16250, 7.98824, 2.82857, 8.34850, 2.…\n$ W_FSTURWT48  &lt;dbl&gt; 2.72897, 10.36607, 3.38750, 7.98824, 8.48571, 2.78283, 2.…\n$ W_FSTURWT49  &lt;dbl&gt; 7.74437, 3.29511, 10.16250, 2.66275, 9.00000, 8.02740, 7.…\n$ W_FSTURWT50  &lt;dbl&gt; 2.67796, 9.89489, 3.38750, 7.98824, 3.00000, 8.40156, 7.5…\n$ W_FSTURWT51  &lt;dbl&gt; 2.73358, 3.29830, 10.16250, 2.77143, 9.00000, 8.06550, 2.…\n$ W_FSTURWT52  &lt;dbl&gt; 8.18690, 9.88533, 3.38750, 7.98824, 9.00000, 8.69635, 2.4…\n$ W_FSTURWT53  &lt;dbl&gt; 8.20073, 10.33466, 3.38750, 2.56226, 2.82857, 3.05511, 2.…\n$ W_FSTURWT54  &lt;dbl&gt; 2.58146, 10.36607, 3.38750, 2.56226, 3.00000, 2.92228, 2.…\n$ W_FSTURWT55  &lt;dbl&gt; 2.68151, 9.89489, 3.38750, 2.66275, 8.48571, 9.48693, 7.3…\n$ W_FSTURWT56  &lt;dbl&gt; 2.63139, 3.45536, 10.16250, 2.77143, 9.00000, 2.89878, 2.…\n$ W_FSTURWT57  &lt;dbl&gt; 2.73358, 3.45536, 10.16250, 7.98824, 8.48571, 2.80052, 7.…\n$ W_FSTURWT58  &lt;dbl&gt; 7.89416, 9.88533, 3.38750, 8.31429, 8.48571, 8.40156, 2.5…\n$ W_FSTURWT59  &lt;dbl&gt; 2.72897, 3.29830, 10.16250, 2.56226, 2.82857, 8.76685, 7.…\n$ W_FSTURWT60  &lt;dbl&gt; 8.03388, 3.44489, 10.16250, 8.31429, 8.48571, 2.92228, 7.…\n$ W_FSTURWT61  &lt;dbl&gt; 7.89416, 9.88533, 10.16250, 7.98824, 2.82857, 8.34850, 7.…\n$ W_FSTURWT62  &lt;dbl&gt; 2.63139, 10.36607, 10.16250, 2.66275, 3.00000, 3.02482, 7…\n$ W_FSTURWT63  &lt;dbl&gt; 8.03388, 10.33466, 10.16250, 8.31429, 9.00000, 3.20060, 7…\n$ W_FSTURWT64  &lt;dbl&gt; 7.88647, 3.44489, 3.38750, 7.98824, 2.82857, 2.67580, 2.5…\n$ W_FSTURWT65  &lt;dbl&gt; 7.89416, 3.29511, 3.38750, 2.56226, 3.00000, 8.40156, 2.4…\n$ W_FSTURWT66  &lt;dbl&gt; 8.36312, 3.44489, 3.38750, 2.56226, 3.00000, 2.78283, 7.5…\n$ W_FSTURWT67  &lt;dbl&gt; 2.67796, 3.29830, 3.38750, 2.66275, 2.82857, 8.34850, 2.4…\n$ W_FSTURWT68  &lt;dbl&gt; 2.62882, 10.36607, 10.16250, 2.66275, 8.48571, 2.78283, 2…\n$ W_FSTURWT69  &lt;dbl&gt; 8.03388, 3.29511, 3.38750, 7.98824, 9.00000, 8.69635, 7.5…\n$ W_FSTURWT70  &lt;dbl&gt; 2.78196, 9.89489, 10.16250, 2.66275, 3.00000, 8.40156, 7.…\n$ W_FSTURWT71  &lt;dbl&gt; 2.73358, 3.29830, 3.38750, 8.31429, 9.00000, 8.76685, 2.5…\n$ W_FSTURWT72  &lt;dbl&gt; 8.18690, 9.88533, 10.16250, 2.66275, 9.00000, 8.69635, 2.…\n$ W_FSTURWT73  &lt;dbl&gt; 8.20073, 10.33466, 10.16250, 7.68679, 2.82857, 3.05511, 2…\n$ W_FSTURWT74  &lt;dbl&gt; 2.67796, 10.36607, 10.16250, 7.68679, 3.00000, 2.92228, 2…\n$ W_FSTURWT75  &lt;dbl&gt; 2.68151, 9.89489, 10.16250, 7.98824, 8.48571, 8.69635, 7.…\n$ W_FSTURWT76  &lt;dbl&gt; 2.73358, 3.45536, 3.38750, 8.31429, 9.00000, 2.89878, 2.4…\n$ W_FSTURWT77  &lt;dbl&gt; 2.63139, 3.45536, 3.38750, 2.66275, 8.48571, 2.58510, 7.5…\n$ W_FSTURWT78  &lt;dbl&gt; 7.89416, 9.88533, 10.16250, 2.77143, 8.48571, 8.40156, 2.…\n$ W_FSTURWT79  &lt;dbl&gt; 2.62882, 3.29830, 3.38750, 7.68679, 2.82857, 8.76685, 7.3…\n$ W_FSTURWT80  &lt;dbl&gt; 8.03388, 3.44489, 3.38750, 2.77143, 8.48571, 2.68850, 7.3…\n$ UNIT         &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, …\n$ WVARSTRR     &lt;dbl&gt; 70, 53, 2, 27, 35, 50, 65, 1, 63, 4, 13, 65, 42, 68, 15, …\n$ PV1MATH      &lt;dbl&gt; 639.004, 697.191, 693.710, 427.317, 436.462, 569.982, 771…\n$ PV2MATH      &lt;dbl&gt; 601.251, 754.277, 654.450, 410.376, 453.450, 539.609, 672…\n$ PV3MATH      &lt;dbl&gt; 621.480, 671.940, 696.938, 423.586, 392.315, 531.648, 653…\n$ PV4MATH      &lt;dbl&gt; 631.596, 657.300, 646.187, 388.935, 439.986, 534.368, 734…\n$ PV5MATH      &lt;dbl&gt; 579.276, 621.126, 678.119, 330.962, 443.125, 465.815, 727…\n$ PV6MATH      &lt;dbl&gt; 591.791, 655.729, 644.019, 379.988, 452.648, 528.509, 729…\n$ PV7MATH      &lt;dbl&gt; 600.709, 747.934, 720.531, 398.535, 396.970, 514.326, 597…\n$ PV8MATH      &lt;dbl&gt; 587.322, 694.365, 671.425, 422.127, 459.945, 521.029, 772…\n$ PV9MATH      &lt;dbl&gt; 618.131, 742.732, 694.085, 375.354, 438.166, 472.382, 694…\n$ PV10MATH     &lt;dbl&gt; 581.973, 656.934, 668.304, 453.348, 448.084, 503.387, 725…\n$ PV1READ      &lt;dbl&gt; 676.298, 625.585, 620.116, 381.495, 448.199, 469.441, 744…\n$ PV2READ      &lt;dbl&gt; 692.247, 686.716, 559.078, 400.815, 560.636, 500.350, 679…\n$ PV3READ      &lt;dbl&gt; 690.981, 663.147, 554.767, 374.911, 365.478, 375.703, 635…\n$ PV4READ      &lt;dbl&gt; 643.067, 567.435, 587.026, 367.484, 469.970, 377.452, 725…\n$ PV5READ      &lt;dbl&gt; 627.908, 614.500, 591.806, 336.009, 503.664, 470.781, 731…\n$ PV6READ      &lt;dbl&gt; 684.676, 604.745, 570.547, 324.630, 481.215, 415.448, 684…\n$ PV7READ      &lt;dbl&gt; 661.380, 669.375, 599.078, 396.242, 436.800, 448.547, 646…\n$ PV8READ      &lt;dbl&gt; 674.070, 623.735, 545.610, 374.723, 531.226, 434.381, 756…\n$ PV9READ      &lt;dbl&gt; 666.282, 649.579, 610.466, 314.704, 480.997, 411.703, 653…\n$ PV10READ     &lt;dbl&gt; 657.387, 571.261, 590.758, 342.956, 478.578, 410.846, 784…\n$ PV1SCIE      &lt;dbl&gt; 710.634, 670.646, 666.095, 340.308, 456.333, 475.158, 693…\n$ PV2SCIE      &lt;dbl&gt; 618.739, 748.839, 604.771, 329.889, 453.400, 470.030, 626…\n$ PV3SCIE      &lt;dbl&gt; 591.623, 635.443, 704.217, 411.353, 498.937, 461.218, 627…\n$ PV4SCIE      &lt;dbl&gt; 659.770, 639.735, 687.659, 327.974, 532.324, 504.199, 676…\n$ PV5SCIE      &lt;dbl&gt; 635.892, 608.385, 690.974, 292.183, 508.231, 486.930, 661…\n$ PV6SCIE      &lt;dbl&gt; 646.901, 670.662, 617.175, 355.423, 504.461, 493.011, 618…\n$ PV7SCIE      &lt;dbl&gt; 603.569, 734.807, 692.886, 400.182, 404.572, 469.950, 602…\n$ PV8SCIE      &lt;dbl&gt; 621.352, 639.748, 630.900, 317.518, 549.457, 464.012, 653…\n$ PV9SCIE      &lt;dbl&gt; 659.674, 716.768, 656.620, 298.893, 411.062, 440.113, 645…\n$ PV10SCIE     &lt;dbl&gt; 649.719, 655.670, 649.087, 362.702, 473.613, 495.410, 662…\n$ PV1MCCR      &lt;dbl&gt; 649.392, 636.431, 645.218, 437.613, 474.516, 471.365, 718…\n$ PV2MCCR      &lt;dbl&gt; 575.372, 674.370, 680.260, 498.958, 521.987, 546.743, 708…\n$ PV3MCCR      &lt;dbl&gt; 603.792, 716.787, 711.792, 402.737, 434.209, 581.279, 707…\n$ PV4MCCR      &lt;dbl&gt; 656.162, 616.569, 633.124, 407.151, 432.725, 567.093, 720…\n$ PV5MCCR      &lt;dbl&gt; 625.136, 701.626, 711.941, 450.975, 392.901, 498.145, 736…\n$ PV6MCCR      &lt;dbl&gt; 582.174, 686.793, 749.576, 487.698, 444.589, 519.416, 670…\n$ PV7MCCR      &lt;dbl&gt; 665.104, 665.775, 692.915, 391.175, 382.733, 521.639, 749…\n$ PV8MCCR      &lt;dbl&gt; 599.694, 729.194, 648.755, 426.641, 497.778, 513.726, 729…\n$ PV9MCCR      &lt;dbl&gt; 616.165, 715.697, 721.097, 366.883, 439.754, 519.125, 696…\n$ PV10MCCR     &lt;dbl&gt; 628.181, 688.830, 657.165, 435.455, 486.126, 544.408, 779…\n$ PV1MCQN      &lt;dbl&gt; 615.137, 661.706, 569.292, 403.429, 432.794, 526.659, 703…\n$ PV2MCQN      &lt;dbl&gt; 538.617, 762.401, 647.507, 419.847, 494.698, 580.938, 707…\n$ PV3MCQN      &lt;dbl&gt; 587.580, 712.182, 660.431, 418.996, 429.147, 577.745, 703…\n$ PV4MCQN      &lt;dbl&gt; 686.280, 685.954, 569.983, 375.158, 464.070, 622.873, 631…\n$ PV5MCQN      &lt;dbl&gt; 589.078, 722.402, 637.599, 433.093, 446.069, 557.871, 673…\n$ PV6MCQN      &lt;dbl&gt; 554.608, 677.709, 652.780, 453.028, 441.193, 488.815, 645…\n$ PV7MCQN      &lt;dbl&gt; 682.886, 691.586, 636.887, 435.661, 417.452, 602.295, 740…\n$ PV8MCQN      &lt;dbl&gt; 574.889, 688.120, 611.227, 441.660, 474.220, 578.260, 757…\n$ PV9MCQN      &lt;dbl&gt; 601.470, 704.436, 685.701, 430.417, 443.916, 541.486, 721…\n$ PV10MCQN     &lt;dbl&gt; 627.403, 692.040, 604.016, 382.344, 476.528, 572.958, 715…\n$ PV1MCSS      &lt;dbl&gt; 686.193, 664.374, 655.070, 397.737, 429.063, 466.035, 637…\n$ PV2MCSS      &lt;dbl&gt; 656.043, 660.937, 612.146, 336.048, 437.136, 498.416, 712…\n$ PV3MCSS      &lt;dbl&gt; 650.782, 692.654, 646.904, 438.551, 451.101, 502.442, 702…\n$ PV4MCSS      &lt;dbl&gt; 643.126, 654.111, 590.767, 347.416, 464.708, 517.999, 702…\n$ PV5MCSS      &lt;dbl&gt; 675.389, 644.941, 727.077, 420.278, 338.774, 452.092, 663…\n$ PV6MCSS      &lt;dbl&gt; 591.001, 699.918, 648.958, 401.609, 453.538, 494.653, 632…\n$ PV7MCSS      &lt;dbl&gt; 684.203, 653.602, 623.166, 413.761, 366.873, 463.116, 727…\n$ PV8MCSS      &lt;dbl&gt; 617.908, 629.891, 677.449, 299.027, 469.776, 498.289, 719…\n$ PV9MCSS      &lt;dbl&gt; 601.076, 696.030, 654.643, 373.784, 438.499, 441.962, 635…\n$ PV10MCSS     &lt;dbl&gt; 645.941, 663.634, 649.460, 363.817, 484.175, 411.098, 774…\n$ PV1MCUD      &lt;dbl&gt; 597.328, 655.345, 658.932, 393.019, 429.756, 491.327, 650…\n$ PV2MCUD      &lt;dbl&gt; 564.341, 732.821, 648.412, 393.366, 491.067, 522.747, 721…\n$ PV3MCUD      &lt;dbl&gt; 655.238, 737.658, 672.524, 384.864, 445.708, 486.597, 663…\n$ PV4MCUD      &lt;dbl&gt; 638.884, 651.654, 614.591, 379.824, 510.568, 524.480, 680…\n$ PV5MCUD      &lt;dbl&gt; 604.706, 690.450, 664.442, 501.245, 403.908, 483.108, 753…\n$ PV6MCUD      &lt;dbl&gt; 576.996, 666.422, 687.078, 453.339, 412.346, 505.925, 667…\n$ PV7MCUD      &lt;dbl&gt; 672.527, 673.451, 708.829, 425.168, 419.173, 501.566, 746…\n$ PV8MCUD      &lt;dbl&gt; 599.424, 728.294, 640.191, 407.016, 576.384, 506.734, 801…\n$ PV9MCUD      &lt;dbl&gt; 604.423, 701.038, 732.526, 421.738, 392.309, 499.849, 638…\n$ PV10MCUD     &lt;dbl&gt; 664.795, 650.797, 619.894, 421.415, 461.595, 442.838, 734…\n$ PV1MPEM      &lt;dbl&gt; 604.382, 705.040, 676.642, 401.548, 437.563, 528.852, 692…\n$ PV2MPEM      &lt;dbl&gt; 575.460, 710.217, 705.385, 389.686, 474.378, 604.877, 678…\n$ PV3MPEM      &lt;dbl&gt; 534.443, 713.023, 585.184, 390.502, 433.958, 498.130, 711…\n$ PV4MPEM      &lt;dbl&gt; 571.301, 679.747, 670.486, 434.343, 442.276, 511.479, 711…\n$ PV5MPEM      &lt;dbl&gt; 675.638, 661.754, 645.880, 366.385, 522.369, 555.593, 706…\n$ PV6MPEM      &lt;dbl&gt; 566.880, 718.268, 760.958, 401.969, 417.482, 555.839, 749…\n$ PV7MPEM      &lt;dbl&gt; 582.805, 613.478, 731.917, 496.875, 444.665, 553.932, 690…\n$ PV8MPEM      &lt;dbl&gt; 558.696, 643.541, 676.092, 353.233, 464.823, 571.562, 708…\n$ PV9MPEM      &lt;dbl&gt; 662.795, 640.522, 684.182, 476.154, 434.014, 520.155, 719…\n$ PV10MPEM     &lt;dbl&gt; 640.998, 655.268, 702.866, 342.948, 458.216, 609.277, 671…\n$ PV1MPFS      &lt;dbl&gt; 518.732, 763.661, 690.547, 421.798, 454.383, 493.759, 702…\n$ PV2MPFS      &lt;dbl&gt; 557.279, 729.497, 728.787, 467.856, 401.375, 538.097, 662…\n$ PV3MPFS      &lt;dbl&gt; 497.254, 714.971, 633.737, 414.444, 453.331, 485.088, 670…\n$ PV4MPFS      &lt;dbl&gt; 615.386, 753.899, 703.501, 445.029, 414.743, 465.568, 658…\n$ PV5MPFS      &lt;dbl&gt; 615.007, 719.492, 654.486, 389.460, 443.411, 493.251, 653…\n$ PV6MPFS      &lt;dbl&gt; 591.702, 715.191, 734.709, 414.555, 391.559, 545.850, 699…\n$ PV7MPFS      &lt;dbl&gt; 595.836, 702.035, 737.481, 489.032, 386.903, 504.961, 697…\n$ PV8MPFS      &lt;dbl&gt; 540.481, 704.257, 685.489, 363.830, 452.824, 532.093, 692…\n$ PV9MPFS      &lt;dbl&gt; 600.664, 664.705, 665.867, 419.104, 478.373, 568.115, 656…\n$ PV10MPFS     &lt;dbl&gt; 613.118, 705.987, 727.280, 394.856, 407.059, 485.863, 698…\n$ PV1MPIN      &lt;dbl&gt; 602.757, 733.566, 682.130, 407.066, 414.746, 459.876, 691…\n$ PV2MPIN      &lt;dbl&gt; 571.184, 744.273, 692.729, 381.339, 399.365, 490.634, 671…\n$ PV3MPIN      &lt;dbl&gt; 646.605, 758.913, 647.770, 364.773, 447.814, 386.584, 737…\n$ PV4MPIN      &lt;dbl&gt; 679.914, 695.003, 629.600, 406.470, 454.758, 499.082, 731…\n$ PV5MPIN      &lt;dbl&gt; 685.582, 714.181, 693.276, 433.901, 442.892, 441.024, 737…\n$ PV6MPIN      &lt;dbl&gt; 637.760, 716.221, 660.979, 406.423, 463.269, 544.965, 763…\n$ PV7MPIN      &lt;dbl&gt; 645.213, 663.813, 684.474, 489.700, 423.967, 475.647, 782…\n$ PV8MPIN      &lt;dbl&gt; 577.579, 662.428, 656.617, 432.277, 464.304, 504.819, 769…\n$ PV9MPIN      &lt;dbl&gt; 661.673, 640.743, 687.070, 512.069, 435.375, 459.819, 686…\n$ PV10MPIN     &lt;dbl&gt; 670.254, 768.695, 648.410, 374.502, 495.469, 481.600, 708…\n$ PV1MPRE      &lt;dbl&gt; 537.068, 706.337, 630.753, 378.730, 364.784, 523.219, 756…\n$ PV2MPRE      &lt;dbl&gt; 614.320, 672.767, 694.543, 400.807, 399.972, 536.264, 700…\n$ PV3MPRE      &lt;dbl&gt; 583.272, 651.949, 604.546, 407.607, 452.831, 454.319, 687…\n$ PV4MPRE      &lt;dbl&gt; 620.093, 620.759, 614.087, 336.451, 394.357, 501.514, 701…\n$ PV5MPRE      &lt;dbl&gt; 634.054, 645.072, 603.798, 317.742, 409.755, 498.824, 724…\n$ PV6MPRE      &lt;dbl&gt; 602.552, 677.174, 644.046, 349.040, 428.787, 523.705, 680…\n$ PV7MPRE      &lt;dbl&gt; 595.217, 634.813, 710.851, 450.198, 375.015, 497.360, 697…\n$ PV8MPRE      &lt;dbl&gt; 603.353, 648.907, 656.938, 392.060, 414.975, 508.547, 736…\n$ PV9MPRE      &lt;dbl&gt; 611.942, 641.203, 690.323, 447.422, 441.178, 499.691, 734…\n$ PV10MPRE     &lt;dbl&gt; 663.352, 644.001, 664.134, 382.088, 421.531, 580.387, 727…\n$ SENWT        &lt;dbl&gt; 0.63867, 0.80357, 0.80736, 0.64732, 0.69397, 0.67917, 0.5…\n$ VER_DAT      &lt;chr&gt; \"01MAY23:14:19:45\", \"01MAY23:14:19:44\", \"01MAY23:14:19:45…\n$ i            &lt;dbl&gt; 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 21…\n\n\n\n\n\nCheck for duplicates\nUsing the duplicated function, we see that there are no duplicate entries in the data.\n\nstu_qqq_SG[duplicated(stu_qqq_SG),]\n\n# A tibble: 0 × 1,279\n# ℹ 1,279 variables: CNT &lt;chr&gt;, CNTRYID &lt;dbl&gt;, CNTSCHID &lt;dbl&gt;, CNTSTUID &lt;dbl&gt;,\n#   CYC &lt;chr&gt;, NatCen &lt;chr&gt;, STRATUM &lt;chr&gt;, SUBNATIO &lt;chr&gt;, REGION &lt;dbl&gt;,\n#   OECD &lt;dbl&gt;, ADMINMODE &lt;dbl&gt;, LANGTEST_QQQ &lt;dbl&gt;, LANGTEST_COG &lt;dbl&gt;,\n#   LANGTEST_PAQ &lt;dbl&gt;, Option_CT &lt;dbl&gt;, Option_FL &lt;dbl&gt;, Option_ICTQ &lt;dbl&gt;,\n#   Option_WBQ &lt;dbl&gt;, Option_PQ &lt;dbl&gt;, Option_TQ &lt;dbl&gt;, Option_UH &lt;dbl&gt;,\n#   BOOKID &lt;dbl&gt;, ST001D01T &lt;dbl&gt;, ST003D02T &lt;dbl&gt;, ST003D03T &lt;dbl&gt;,\n#   ST004D01T &lt;dbl&gt;, ST250Q01JA &lt;dbl&gt;, ST250Q02JA &lt;dbl&gt;, ST250Q03JA &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nInsights\n\n\n\nThe data has 6,606 rows, corresponding to this report that 6,606 students in Singapore participated in PISA 2022 .\nThe data 1279 columns, which are variables of the dataset for each student. These variables include characteristics of the student (such as age, gender), background (socioeconomic status, parental involvement, language spoken at home), proficiency (plausible values for each subject), individual responses to the questionnaire and so on.\n\n\n\n\nFiltering data for selected variables\nNext, we want to reduce the size of the dataset to focus on the variables that would be suitable for this exercise. I primarily referred to the PISA Data Analysis Manual: SPSS and SAS, Second Edition and the Code Book to understand which variables might be helpful for this visualization exercise.\nThe primary columns/variables that will be used in this exercise are:\n\n\n\nVariable Name\nDescription\nType of Variable\n\n\n\n\nCNTSTUID\nIntl. Student ID. It also uniquely identifies each row.\nNominal\n\n\nST004D01T\nStudent (standardized) gender.\n\n1: Female\n2: Male\n\nCategorical unordered\n\n\nSTRATUM\nClassification of the students’ school. For Singapore:\n\nSGP01: Public secondary school\nSCP03: Private secondary school\n\nCategorical unordered\n\n\nIMMIG\nIndex on immigrant background.\n\n1: Native students\n2: Second-generation students\n3: First-generation students\n\nCategorical unordered\n\n\nESCS\nThe PISA index of economic, social and cultural status which is a composite score derived from variables related to family background.\nNumerical continuous\n\n\n\nPV1MATH to PV10MATH\nPV1READ to PV10READ\nPV1SCIE to PV10SCIE\n\nPlausible values representing the students’ performance in mathematics, reading and science. For each subject, there are 10 plausible values.\nThe higher the value, the better the indicative performance of the student in that subject.\nNumerical discrete\n\n\n\nWe filter the stu_qqq_SG for the aforementioned variables into stu_qqq_SG_filtered in the code chunk below.\n\nstu_qqq_SG_filtered &lt;- stu_qqq_SG %&gt;%\n  select(CNTSTUID, ST004D01T, STRATUM, IMMIG, ESCS,\n         PV1MATH:PV10MATH, PV1READ:PV10READ, PV1SCIE:PV10SCIE)\n\nWe then perform some checks on the data:\n\n\nCheck data structure\nUsing the str() function, we observe that some variables may need to be re-casted. For example, ST004D01T, would be better classifed as categorical type chr than numeric type num.\n\n\nstr(stu_qqq_SG_filtered)\n\ntibble [6,606 × 35] (S3: tbl_df/tbl/data.frame)\n $ CNTSTUID : num [1:6606] 70200001 70200002 70200003 70200004 70200005 ...\n  ..- attr(*, \"label\")= chr \"Intl. Student ID\"\n $ ST004D01T: num [1:6606] 1 2 2 2 1 1 2 2 1 2 ...\n  ..- attr(*, \"label\")= chr \"Student (Standardized) Gender\"\n $ STRATUM  : chr [1:6606] \"SGP01\" \"SGP01\" \"SGP01\" \"SGP01\" ...\n  ..- attr(*, \"label\")= chr \"Stratum ID 5-character (cnt + original stratum ID)\"\n $ IMMIG    : num [1:6606] 1 1 1 1 1 3 1 3 1 1 ...\n  ..- attr(*, \"label\")= chr \"Index on immigrant background (OECD definition)\"\n $ ESCS     : num [1:6606] 0.1836 0.8261 -1.0357 -0.9606 0.0856 ...\n  ..- attr(*, \"label\")= chr \"Index of economic, social and cultural status\"\n $ PV1MATH  : num [1:6606] 639 697 694 427 436 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 1 in Mathematics\"\n $ PV2MATH  : num [1:6606] 601 754 654 410 453 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 2 in Mathematics\"\n $ PV3MATH  : num [1:6606] 621 672 697 424 392 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 3 in Mathematics\"\n $ PV4MATH  : num [1:6606] 632 657 646 389 440 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 4 in Mathematics\"\n $ PV5MATH  : num [1:6606] 579 621 678 331 443 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 5 in Mathematics\"\n $ PV6MATH  : num [1:6606] 592 656 644 380 453 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 6 in Mathematics\"\n $ PV7MATH  : num [1:6606] 601 748 721 399 397 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 7 in Mathematics\"\n $ PV8MATH  : num [1:6606] 587 694 671 422 460 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 8 in Mathematics\"\n $ PV9MATH  : num [1:6606] 618 743 694 375 438 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 9 in Mathematics\"\n $ PV10MATH : num [1:6606] 582 657 668 453 448 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 10 in Mathematics\"\n $ PV1READ  : num [1:6606] 676 626 620 381 448 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 1 in Reading\"\n $ PV2READ  : num [1:6606] 692 687 559 401 561 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 2 in Reading\"\n $ PV3READ  : num [1:6606] 691 663 555 375 365 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 3 in Reading\"\n $ PV4READ  : num [1:6606] 643 567 587 367 470 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 4 in Reading\"\n $ PV5READ  : num [1:6606] 628 614 592 336 504 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 5 in Reading\"\n $ PV6READ  : num [1:6606] 685 605 571 325 481 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 6 in Reading\"\n $ PV7READ  : num [1:6606] 661 669 599 396 437 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 7 in Reading\"\n $ PV8READ  : num [1:6606] 674 624 546 375 531 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 8 in Reading\"\n $ PV9READ  : num [1:6606] 666 650 610 315 481 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 9 in Reading\"\n $ PV10READ : num [1:6606] 657 571 591 343 479 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 10 in Reading\"\n $ PV1SCIE  : num [1:6606] 711 671 666 340 456 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 1 in Science\"\n $ PV2SCIE  : num [1:6606] 619 749 605 330 453 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 2 in Science\"\n $ PV3SCIE  : num [1:6606] 592 635 704 411 499 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 3 in Science\"\n $ PV4SCIE  : num [1:6606] 660 640 688 328 532 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 4 in Science\"\n $ PV5SCIE  : num [1:6606] 636 608 691 292 508 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 5 in Science\"\n $ PV6SCIE  : num [1:6606] 647 671 617 355 504 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 6 in Science\"\n $ PV7SCIE  : num [1:6606] 604 735 693 400 405 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 7 in Science\"\n $ PV8SCIE  : num [1:6606] 621 640 631 318 549 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 8 in Science\"\n $ PV9SCIE  : num [1:6606] 660 717 657 299 411 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 9 in Science\"\n $ PV10SCIE : num [1:6606] 650 656 649 363 474 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 10 in Science\"\n\n\n\n\nCheck and handle missing values\n\nfor(column_name in names(stu_qqq_SG_filtered)) {\n  na_count &lt;- sum(is.na(stu_qqq_SG_filtered[[column_name]]))\n\n  if (na_count &gt; 0) {\n    message(\"Column '\", column_name, \"' has \", na_count, \" NA values.\")\n  }\n}\n\nColumn ESCS has 47 out of 6606 values missing. Since the percentage of missing values is less than 80%, we will impute the missing values with the mean value, and check that there are no more missing values in the ESCS column.\n\nstu_qqq_SG_filtered$ESCS[is.na(stu_qqq_SG_filtered$ESCS)] &lt;- mean(stu_qqq_SG_filtered$ESCS, na.rm = TRUE)\nsum(is.na(stu_qqq_SG_filtered$ESCS))\n\n[1] 0\n\n\nColumn IMMIG has 236 out of 6606 values missing. We will not remove the rows with these missing values, but will handle the missing values later when exploring relationships of subject performance with IMMIG.\n\n\n\nRecast, rename and recode variables\nAfter inspection, the variables are modified as follows:\n\nCNTSTUID is of type num, numeric data. As it should be nominal, it will be cast as chr class.\nST004D01T is of type num, numeric data. As it should be categorical, it will be cast as chr class. We will also rename this variable to GENDER, and recode all values of 1 to FEMALE and all values of 2 to MALE.\nSTRATUM is of type chr, character. We will also recode all values of SGP01 to PRIVATE and all values of SGP03 to PUBLIC.\nIMMIG is of type num, numeric data. As it should be categorical it will be cast as chr class. We will also recode all values of 1 to NATIVE, 2 to 2NDGEN, 3 to 1STGEN.\n\n\n# Update 'CNTSTUID'\nstu_qqq_SG_filtered &lt;- stu_qqq_SG_filtered %&gt;%\n  mutate(CNTSTUID = as.character(CNTSTUID))\n\n# Update 'ST004D01T'\nnames(stu_qqq_SG_filtered)[names(stu_qqq_SG_filtered) == 'ST004D01T'] &lt;- 'GENDER'\nstu_qqq_SG_filtered &lt;- stu_qqq_SG_filtered %&gt;%\n  mutate(GENDER = recode(as.character(GENDER), '1' = 'FEMALE', '2' = 'MALE'))\n\n# Update 'STRATUM'\nstu_qqq_SG_filtered &lt;- stu_qqq_SG_filtered %&gt;%\n  mutate(STRATUM = recode(STRATUM, 'SGP01' = 'PUBLIC', 'SGP03' = 'PRIVATE'))\n\n# Update 'STRATUM'\nstu_qqq_SG_filtered &lt;- stu_qqq_SG_filtered %&gt;%\n  mutate(IMMIG = recode(IMMIG, '1' = 'NATIVE', '2' = '2ND GEN', '3' = '1STGEN'))\n\n\n\n\nCreate new variables\nWe will create the following new columns:\n\nSOCIOECONOMICSTATUS: From the numerical continuous variables ESCS, we will order students from the most disadvantaged to the most advantaged to create four groups of students of equal size (each comprising 25% of the population). The creation of this four groups is as per recommended in the PISA 2022 Results: Factsheets, Singapore.\n30 new columns mapping plausible values to proficiency levels as described in the PISA 2022 Results: The State of Learning and Equity in Education publication. This means that for each subject, we will create ten columns of proficiency levels, ranging from PV1MATH_LEVEL to PV10MATH_LEVEL, PV1READ_LEVEL to PV10READ_LEVEL, and PV1SCIE_LEVEL to PV10SCIE_LEVEL.\n\n\n# Create a new column, SOCIOECONOMICSTATUS\nstu_qqq_SG_filtered &lt;- stu_qqq_SG_filtered %&gt;%\n  mutate(SOCIOECONOMICSTATUS = ntile(ESCS, 4), .after = ESCS) %&gt;%\n  mutate(SOCIOECONOMICSTATUS = recode(SOCIOECONOMICSTATUS,\n                                      `1` = \"Most Disadvantaged\",\n                                      `2` = \"Disadvantaged\",\n                                      `3` = \"Advantaged\",\n                                      `4` = \"Most Advantaged\")) %&gt;%\n  mutate(SOCIOECONOMICSTATUS = factor(SOCIOECONOMICSTATUS, \n                                      levels = c(\"Most Disadvantaged\", \"Disadvantaged\", \n                                                 \"Advantaged\", \"Most Advantaged\"),\n                                      ordered = TRUE))\n\n# Create new columns for MATH\nbreaks &lt;- c(0, 233, 295, 358, 420, 482, 545, 607, 669, Inf)\nlevel_labels &lt;- c(\"Level 0\", \"Level 1c\", \"Level 1b\", \"Level 1a\", \"Level 2\", \"Level 3\", \"Level 4\", \"Level 5\", \"Level 6\")\nfor(i in 1:10) {\n  pv_col_name &lt;- paste0(\"PV\", i, \"MATH\")\n  level_col_name &lt;- paste0(pv_col_name, \"_LEVEL\")\n\n  stu_qqq_SG_filtered[[level_col_name]] &lt;- cut(stu_qqq_SG_filtered[[pv_col_name]], breaks = breaks, labels = level_labels, right = FALSE)\n}\n\n# Create new columns for READ\nbreaks &lt;- c(0, 189, 262, 335, 407, 480, 553, 626, 698, Inf)\nlevel_labels &lt;- c(\"Level 0\", \"Level 1c\", \"Level 1b\", \"Level 1a\", \"Level 2\", \"Level 3\", \"Level 4\", \"Level 5\", \"Level 6\")\nfor(i in 1:10) {\n  pv_col_name &lt;- paste0(\"PV\", i, \"READ\")\n  level_col_name &lt;- paste0(pv_col_name, \"_LEVEL\")\n\n  stu_qqq_SG_filtered[[level_col_name]] &lt;- cut(stu_qqq_SG_filtered[[pv_col_name]], breaks = breaks, labels = level_labels, right = FALSE)\n}\n\n# Create new columns for SCIE\nbreaks &lt;- c(0, 261, 335, 410, 484, 559, 633, 708, Inf)\nlevel_labels &lt;- c(\"Level 0\", \"Level 1b\", \"Level 1a\", \"Level 2\", \"Level 3\", \"Level 4\", \"Level 5\", \"Level 6\")\nfor(i in 1:10) {\n  pv_col_name &lt;- paste0(\"PV\", i, \"SCIE\")\n  level_col_name &lt;- paste0(pv_col_name, \"_LEVEL\")\n\n  stu_qqq_SG_filtered[[level_col_name]] &lt;- cut(stu_qqq_SG_filtered[[pv_col_name]], breaks = breaks, labels = level_labels, right = FALSE)\n}\n\n\n\nPreview pre-processed dataframe\nWe use the function head() to preview the first few rows of the pre-processed dataframe:\n\nhead(stu_qqq_SG_filtered, 200)\n\n# A tibble: 200 × 66\n   CNTSTUID GENDER STRATUM IMMIG     ESCS SOCIOECONOMICSTATUS PV1MATH PV2MATH\n   &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;ord&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n 1 70200001 FEMALE PUBLIC  NATIVE  0.184  Disadvantaged          639.    601.\n 2 70200002 MALE   PUBLIC  NATIVE  0.826  Advantaged             697.    754.\n 3 70200003 MALE   PUBLIC  NATIVE -1.04   Most Disadvantaged     694.    654.\n 4 70200004 MALE   PUBLIC  NATIVE -0.961  Most Disadvantaged     427.    410.\n 5 70200005 FEMALE PUBLIC  NATIVE  0.0856 Disadvantaged          436.    453.\n 6 70200006 FEMALE PUBLIC  1STGEN  0.127  Disadvantaged          570.    540.\n 7 70200007 MALE   PUBLIC  NATIVE -0.0154 Disadvantaged          772.    673.\n 8 70200008 MALE   PUBLIC  1STGEN  1.16   Most Advantaged        568.    631.\n 9 70200009 FEMALE PUBLIC  NATIVE  1.47   Most Advantaged        740.    768.\n10 70200010 MALE   PUBLIC  NATIVE  0.520  Advantaged             586.    537.\n# ℹ 190 more rows\n# ℹ 58 more variables: PV3MATH &lt;dbl&gt;, PV4MATH &lt;dbl&gt;, PV5MATH &lt;dbl&gt;,\n#   PV6MATH &lt;dbl&gt;, PV7MATH &lt;dbl&gt;, PV8MATH &lt;dbl&gt;, PV9MATH &lt;dbl&gt;, PV10MATH &lt;dbl&gt;,\n#   PV1READ &lt;dbl&gt;, PV2READ &lt;dbl&gt;, PV3READ &lt;dbl&gt;, PV4READ &lt;dbl&gt;, PV5READ &lt;dbl&gt;,\n#   PV6READ &lt;dbl&gt;, PV7READ &lt;dbl&gt;, PV8READ &lt;dbl&gt;, PV9READ &lt;dbl&gt;, PV10READ &lt;dbl&gt;,\n#   PV1SCIE &lt;dbl&gt;, PV2SCIE &lt;dbl&gt;, PV3SCIE &lt;dbl&gt;, PV4SCIE &lt;dbl&gt;, PV5SCIE &lt;dbl&gt;,\n#   PV6SCIE &lt;dbl&gt;, PV7SCIE &lt;dbl&gt;, PV8SCIE &lt;dbl&gt;, PV9SCIE &lt;dbl&gt;, …\n\n\nUsing str(), we observed that our preprocessed dataset now consists of 66 columns.\n\n\nstr(stu_qqq_SG_filtered)\n\ntibble [6,606 × 66] (S3: tbl_df/tbl/data.frame)\n $ CNTSTUID           : chr [1:6606] \"70200001\" \"70200002\" \"70200003\" \"70200004\" ...\n $ GENDER             : chr [1:6606] \"FEMALE\" \"MALE\" \"MALE\" \"MALE\" ...\n $ STRATUM            : chr [1:6606] \"PUBLIC\" \"PUBLIC\" \"PUBLIC\" \"PUBLIC\" ...\n $ IMMIG              : chr [1:6606] \"NATIVE\" \"NATIVE\" \"NATIVE\" \"NATIVE\" ...\n $ ESCS               : num [1:6606] 0.1836 0.8261 -1.0357 -0.9606 0.0856 ...\n  ..- attr(*, \"label\")= chr \"Index of economic, social and cultural status\"\n $ SOCIOECONOMICSTATUS: Ord.factor w/ 4 levels \"Most Disadvantaged\"&lt;..: 2 3 1 1 2 2 2 4 4 3 ...\n $ PV1MATH            : num [1:6606] 639 697 694 427 436 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 1 in Mathematics\"\n $ PV2MATH            : num [1:6606] 601 754 654 410 453 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 2 in Mathematics\"\n $ PV3MATH            : num [1:6606] 621 672 697 424 392 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 3 in Mathematics\"\n $ PV4MATH            : num [1:6606] 632 657 646 389 440 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 4 in Mathematics\"\n $ PV5MATH            : num [1:6606] 579 621 678 331 443 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 5 in Mathematics\"\n $ PV6MATH            : num [1:6606] 592 656 644 380 453 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 6 in Mathematics\"\n $ PV7MATH            : num [1:6606] 601 748 721 399 397 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 7 in Mathematics\"\n $ PV8MATH            : num [1:6606] 587 694 671 422 460 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 8 in Mathematics\"\n $ PV9MATH            : num [1:6606] 618 743 694 375 438 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 9 in Mathematics\"\n $ PV10MATH           : num [1:6606] 582 657 668 453 448 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 10 in Mathematics\"\n $ PV1READ            : num [1:6606] 676 626 620 381 448 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 1 in Reading\"\n $ PV2READ            : num [1:6606] 692 687 559 401 561 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 2 in Reading\"\n $ PV3READ            : num [1:6606] 691 663 555 375 365 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 3 in Reading\"\n $ PV4READ            : num [1:6606] 643 567 587 367 470 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 4 in Reading\"\n $ PV5READ            : num [1:6606] 628 614 592 336 504 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 5 in Reading\"\n $ PV6READ            : num [1:6606] 685 605 571 325 481 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 6 in Reading\"\n $ PV7READ            : num [1:6606] 661 669 599 396 437 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 7 in Reading\"\n $ PV8READ            : num [1:6606] 674 624 546 375 531 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 8 in Reading\"\n $ PV9READ            : num [1:6606] 666 650 610 315 481 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 9 in Reading\"\n $ PV10READ           : num [1:6606] 657 571 591 343 479 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 10 in Reading\"\n $ PV1SCIE            : num [1:6606] 711 671 666 340 456 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 1 in Science\"\n $ PV2SCIE            : num [1:6606] 619 749 605 330 453 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 2 in Science\"\n $ PV3SCIE            : num [1:6606] 592 635 704 411 499 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 3 in Science\"\n $ PV4SCIE            : num [1:6606] 660 640 688 328 532 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 4 in Science\"\n $ PV5SCIE            : num [1:6606] 636 608 691 292 508 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 5 in Science\"\n $ PV6SCIE            : num [1:6606] 647 671 617 355 504 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 6 in Science\"\n $ PV7SCIE            : num [1:6606] 604 735 693 400 405 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 7 in Science\"\n $ PV8SCIE            : num [1:6606] 621 640 631 318 549 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 8 in Science\"\n $ PV9SCIE            : num [1:6606] 660 717 657 299 411 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 9 in Science\"\n $ PV10SCIE           : num [1:6606] 650 656 649 363 474 ...\n  ..- attr(*, \"label\")= chr \"Plausible Value 10 in Science\"\n $ PV1MATH_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 9 9 5 5 7 9 7 9 7 ...\n $ PV2MATH_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 7 9 8 4 5 6 9 8 9 6 ...\n $ PV3MATH_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 9 9 5 4 6 8 7 9 6 ...\n $ PV4MATH_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 8 8 4 5 6 9 8 9 7 ...\n $ PV5MATH_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 7 8 9 3 5 5 9 8 9 6 ...\n $ PV6MATH_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 7 8 8 4 5 6 9 8 9 7 ...\n $ PV7MATH_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 7 9 9 4 4 6 7 7 9 7 ...\n $ PV8MATH_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 7 9 9 5 5 6 9 8 9 7 ...\n $ PV9MATH_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 9 9 4 5 5 9 8 9 7 ...\n $ PV10MATH_LEVEL     : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 7 8 8 5 5 6 9 8 9 7 ...\n $ PV1READ_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 7 7 4 5 5 9 6 9 6 ...\n $ PV2READ_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 8 7 4 7 6 8 7 9 6 ...\n $ PV3READ_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 8 7 4 4 4 8 6 8 6 ...\n $ PV4READ_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 7 7 4 5 4 9 6 9 6 ...\n $ PV5READ_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 7 7 4 6 5 9 7 8 6 ...\n $ PV6READ_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 7 7 3 6 5 8 6 8 6 ...\n $ PV7READ_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 8 7 4 5 5 8 6 9 6 ...\n $ PV8READ_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 7 6 4 6 5 9 6 8 6 ...\n $ PV9READ_LEVEL      : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 8 7 3 6 5 8 6 9 5 ...\n $ PV10READ_LEVEL     : Factor w/ 9 levels \"Level 0\",\"Level 1c\",..: 8 7 7 4 5 5 9 6 9 6 ...\n $ PV1SCIE_LEVEL      : Factor w/ 8 levels \"Level 0\",\"Level 1b\",..: 8 7 7 3 4 4 7 6 8 6 ...\n $ PV2SCIE_LEVEL      : Factor w/ 8 levels \"Level 0\",\"Level 1b\",..: 6 8 6 2 4 4 6 7 7 5 ...\n $ PV3SCIE_LEVEL      : Factor w/ 8 levels \"Level 0\",\"Level 1b\",..: 6 7 7 4 5 4 6 5 7 5 ...\n $ PV4SCIE_LEVEL      : Factor w/ 8 levels \"Level 0\",\"Level 1b\",..: 7 7 7 2 5 5 7 6 8 5 ...\n $ PV5SCIE_LEVEL      : Factor w/ 8 levels \"Level 0\",\"Level 1b\",..: 7 6 7 2 5 5 7 6 8 5 ...\n $ PV6SCIE_LEVEL      : Factor w/ 8 levels \"Level 0\",\"Level 1b\",..: 7 7 6 3 5 5 6 6 8 6 ...\n $ PV7SCIE_LEVEL      : Factor w/ 8 levels \"Level 0\",\"Level 1b\",..: 6 8 7 3 3 4 6 5 8 5 ...\n $ PV8SCIE_LEVEL      : Factor w/ 8 levels \"Level 0\",\"Level 1b\",..: 6 7 6 2 5 4 7 5 7 5 ...\n $ PV9SCIE_LEVEL      : Factor w/ 8 levels \"Level 0\",\"Level 1b\",..: 7 8 7 2 4 4 7 6 7 5 ...\n $ PV10SCIE_LEVEL     : Factor w/ 8 levels \"Level 0\",\"Level 1b\",..: 7 7 7 3 4 5 7 5 7 6 ...\n\n\n\nNow that data pre-processing is complete, we proceed to share 5 key visualizations performed from the exploratory data analysis of this dataset."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#eda-1-distribution-of-students-performance",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#eda-1-distribution-of-students-performance",
    "title": "Take-home Exercise 1",
    "section": "EDA 1: Distribution of students performance",
    "text": "EDA 1: Distribution of students performance\nTo visualize the distribution Singapore students’ performance in mathematics, reading, and science, we use the plausible value (PV) for performance. While there are 10 PVs, using 1 PV can be sufficient for exploratory data analysis (PISA Data Analysis Manual: SPSS and SAS, Second Edition).\nWe first plot a ridge plot that shows the density distribution of scores for each subject, and boxplot showing summary of the distribution. The line within each density plot and each box indicates the median score in red, providing a measure of the central tendency for each subject. We also ensure that both plots have the same scale to provide a more accurate visual representation of the distributions.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Reshape the data to long format\nstu_qqq_SG_filtered_long &lt;- stu_qqq_SG_filtered %&gt;% \n  mutate(PV1MATH_copied = PV1MATH,\n         PV1READ_copied = PV1READ,\n         PV1SCIE_copied = PV1SCIE) %&gt;%\n  pivot_longer(\n    cols = c(\"PV1MATH_copied\", \"PV1READ_copied\", \"PV1SCIE_copied\"),\n    names_to = \"Subject\",\n    values_to = \"PV1\"\n  ) %&gt;%\n  mutate(Subject = recode(Subject, \n                          \"PV1MATH_copied\" = \"MATH\", \n                          \"PV1READ_copied\" = \"READING\", \n                          \"PV1SCIE_copied\" = \"SCIENCE\"))\n\n# Create the ridge plot\nPV_ridgeplot &lt;- ggplot(stu_qqq_SG_filtered_long, aes(x = PV1, y = Subject, fill = Subject)) +\n  geom_density_ridges(alpha = 0.5, quantile_lines=TRUE, quantile_fun = function(y, ...) quantile(y, probs = 0.5, na.rm = TRUE)) +\n  ylab(\"\") +\n  xlab(\"PV1\") +\n  scale_x_continuous(limits = c(0,1000)) + \n  labs(title = \"Distributions of performance of students in Singapore for Science, Reading and Math\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\",  # Hides the legend\n        axis.title.x = element_blank(),  # Hides the x-axis title\n        axis.text.x = element_blank(),  # Hides the x-axis text/numbers\n        axis.ticks.x = element_blank(), # Hides the x-axis ticks\n        plot.title = element_text(face = \"bold\", size = 12), plot.subtitle = element_text(size = 8))  \n\n# Calculate mean, median, and whisker positions for each subject. Retaining code although not all are used.\nstats &lt;- stu_qqq_SG_filtered %&gt;%\n  summarise(\n    Mean_MATH = mean(PV1MATH, na.rm = TRUE),\n    Median_MATH = median(PV1MATH, na.rm = TRUE),\n    WhiskerLow_MATH = max(min(PV1MATH, na.rm = TRUE), quantile(PV1MATH, 0.25, na.rm = TRUE) - 1.5 * IQR(PV1MATH, na.rm = TRUE)),\n    WhiskerHigh_MATH = min(max(PV1MATH, na.rm = TRUE), quantile(PV1MATH, 0.75, na.rm = TRUE) + 1.5 * IQR(PV1MATH, na.rm = TRUE)),\n\n    Mean_READ = mean(PV1READ, na.rm = TRUE),\n    Median_READ = median(PV1READ, na.rm = TRUE),\n    WhiskerLow_READ = max(min(PV1READ, na.rm = TRUE), quantile(PV1READ, 0.25, na.rm = TRUE) - 1.5 * IQR(PV1READ, na.rm = TRUE)),\n    WhiskerHigh_READ = min(max(PV1READ, na.rm = TRUE), quantile(PV1READ, 0.75, na.rm = TRUE) + 1.5 * IQR(PV1READ, na.rm = TRUE)),\n\n    Mean_SCIE = mean(PV1SCIE, na.rm = TRUE),\n    Median_SCIE = median(PV1SCIE, na.rm = TRUE),\n    WhiskerLow_SCIE = max(min(PV1SCIE, na.rm = TRUE), quantile(PV1SCIE, 0.25, na.rm = TRUE) - 1.5 * IQR(PV1SCIE, na.rm = TRUE)),\n    WhiskerHigh_SCIE = min(max(PV1SCIE, na.rm = TRUE), quantile(PV1SCIE, 0.75, na.rm = TRUE) + 1.5 * IQR(PV1SCIE, na.rm = TRUE))\n  )\n\n# Create boxplot\nPV_boxplot &lt;- ggplot(data = stu_qqq_SG_filtered) +\n  geom_boxplot(aes(y = PV1READ, x = \"READING\")) +\n  geom_boxplot(aes(y = PV1MATH, x = \"MATH\")) +\n  geom_boxplot(aes(y = PV1SCIE, x = \"SCIENCE\")) +\n  geom_text(data = stats, aes(y = Mean_READ, x = \"READING\", label = round(Median_READ, 1)), color = \"red\", hjust = 0.9) +\n  geom_text(data = stats, aes(y = Mean_MATH, x = \"MATH\", label = round(Median_MATH, 1)), color = \"red\", hjust = 0.9) +\n  geom_text(data = stats, aes(y = Mean_SCIE, x = \"SCIENCE\", label = round(Median_SCIE, 1)), color = \"red\", hjust = 0.9) +\n  ylab(\"Plausible Value 1\") +\n  xlab(\"\") +\n  scale_y_continuous(limits = c(0,1000))+ \n  coord_flip() +\n  theme(axis.title.y = element_text(size = 8)) \n\nPV_ridgeplot / PV_boxplot\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nInsights\n\nThe bell-shaped density curves of the ridge plot show at a glance that the distributions of PV1 values are approximately normal for each subject.\nThe median of performance in Math is the highest, followed by Science, then Reading. Statistical testing should be done to analyse if students did indeed perform the best in Math\nFrom the box plots, we also observe a higher number of outliers that had lower scores for all three subjects."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#insights",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#insights",
    "title": "Take-home Exercise 1",
    "section": "Insights",
    "text": "Insights\n\nThe bell-shaped density curves of the ridge plot show at a glance that the distributions of PV1 values are approximately normal for each subject.\nThe median of performance in Math is the highest, followed by Science, then Reading. Statistical testing should be done to analyse if students did indeed perform the best in Math\nFrom the box plots, we also observe a higher number of outliers that had lower scores for all three subjects."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#eda-2-exploring-relationship-between-performance-and-gender.",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#eda-2-exploring-relationship-between-performance-and-gender.",
    "title": "Take-home Exercise 1",
    "section": "EDA 2: Exploring relationship between performance and gender.",
    "text": "EDA 2: Exploring relationship between performance and gender.\nWe next explore if there are significant differences in performance across the subjects based on the gender of the student. Given that GENDER is a non-ordered categorical variable while plausible values are numerical values, I decided to look at distribution of performance for each gender. Similar to the first EDA, I used density curves and boxplots to give different details of the distribution.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Create the faceted density plot\ngenderPV_densityplot &lt;- ggplot(data = stu_qqq_SG_filtered_long, aes(x = PV1, fill = GENDER)) +    \n  geom_density(alpha = 0.3) + \n  facet_wrap(~ Subject, scales = \"free_y\", nrow = 3) +  # Added '+' here\n  labs(x = \"Plausible Value\", y = \"Density\", title = \"Male and Female students perfom similarly across all subjects\", subtitle = \"Density plot of PV1, coloured by gender\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\", plot.title = element_text(face = \"bold\", size = 12), plot.subtitle = element_text(size = 8), axis.title.y = element_text(size = 8), axis.title.x = element_text(size = 8))\n\n# Create the boxplot\ngenderPV_boxplot &lt;- ggplot(stu_qqq_SG_filtered_long, aes(x = Subject, y = PV1, fill = GENDER)) +\n  geom_boxplot(position = position_dodge(0.8)) +\n  coord_cartesian(ylim = c(0, 1000)) +\n  theme_minimal() +\n  labs(y = \"Plausible Value 1\",  subtitle = \"Box plots of PV1, coloured by gender\") +\n  theme(axis.title.x = element_blank(), legend.title = element_text(face = \"bold\", size = 8), plot.subtitle = element_text(size = 8), axis.title.y = element_text(size = 8)) +\n  scale_fill_manual(values = c(\"FEMALE\" = \"#F7BBBB\", \"MALE\" = \"#C3D9EA\"))\n\n# Combine the plots\ngenderPV_densityplot + genderPV_boxplot\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nInsights\n\nBoth genders appear to perform similarly across all subjects, given that the density distributions are very similar for both genders\nFrom the boxplot:\n\nMales appear to perform better than females in Math, with a higher median of PV\nFemales appear to perform better than males in Reading, with a higher median of PV\nFor Science, there is larger variation in the performance of males than females\n\nThe relationship between gender and performance for each subject does not seem to be very strong, as there are no evident trends that suggest one gender consistently outperforms the other across the subjects"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#insights-1",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#insights-1",
    "title": "Take-home Exercise 1",
    "section": "Insights",
    "text": "Insights\n\nBoth genders appear to perform similarly across all subjects, given that the density distributions are very similar for both genders\nFrom the boxplot:\n\nMales appear to perform better than females in Math, with a higher median of PV\nFemales appear to perform better than males in Reading, with a higher median of PV\nFor Science, there is larger variation in the performance of males than females\n\nThe relationship between gender and performance for each subject does not seem to be very strong, as there are no evident trends that suggest one gender consistently outperforms the other across the subjects"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#eda-3-exploring-relationship-of-performance-with-school-type",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#eda-3-exploring-relationship-of-performance-with-school-type",
    "title": "Take-home Exercise 1",
    "section": "EDA 3: Exploring relationship of performance with school type",
    "text": "EDA 3: Exploring relationship of performance with school type\nThis dataset is based on the performance of students from 149 secondary schools (public schools) and 15 private schools in Singapore, including international schools and madrasahs. Using the STRATUM column, schools were classified as private or public schools. The majority of students in dataset are from public schools.\nTo explore if there were differences in performance between the two types of school I opted to observe the distributions as a boxplot first.\nI wanted to see if we could find a relationship between proficiency levels and school type, and also experiment with other types of visualizations. I hence tried using the geom_tile() function to plot a heat map. First, I reclassified students’ proficiency levels in mathematics, reading, and science into broader categories (Low Proficiency, Moderate Proficiency, High Proficiency) based on their original detailed levels. I then calculated the proportion of students in each category for public and private schools, and used geom_tile() to create a heatmap to visualize these proportions across different proficiency levels.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Boxplot to show distribution comparison between schools\nSTRATUM_boxplot &lt;- ggplot(stu_qqq_SG_filtered_long, aes(x = Subject, y = PV1, fill = STRATUM)) +\n  geom_boxplot(position = position_dodge(0.8)) +\n  coord_cartesian(ylim = c(0, 1000)) +\n  theme_minimal() +\n  labs(y = \"Plausible Value 1\", x = \"Subject\", title = \"Public schools have greater variation in performance\", subtitle = \"Boxplot comparing distribution of school type\") +\n  scale_fill_manual(values = c(\"PUBLIC\" = \"steelblue\", \"PRIVATE\" = \"#ff7f0e\")) +\n  theme(legend.position=\"bottom\", \n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8),\n        plot.title = element_text(size = 12, face = \"bold\"),\n        axis.text.x = element_text(size = 8),\n        plot.subtitle = element_text(size = 8),\n        legend.text = element_text(size = 8),\n        legend.title = element_text(size = 8)\n        )\n\n\n# Heat map with geom_tile()\n# Reclassify proficiency levels\nstu_qqq_SG_filtered &lt;- stu_qqq_SG_filtered %&gt;%\n  mutate(\n    PV1MATH_PROFICIENCY = factor(case_when(\n      PV1MATH_LEVEL %in% c(\"Level 5\", \"Level 6\") ~ \"High\",\n      PV1MATH_LEVEL %in% c(\"Level 3\", \"Level 4\") ~ \"Moderate\",\n      TRUE ~ \"Low\"\n    ), levels = c(\"Low\", \"Moderate\", \"High\")),\n    PV1READ_PROFICIENCY = factor(case_when(\n      PV1READ_LEVEL %in% c(\"Level 5\", \"Level 6\") ~ \"High\",\n      PV1READ_LEVEL %in% c(\"Level 3\", \"Level 4\") ~ \"Moderate\",\n      TRUE ~ \"Low\"\n    ), levels = c(\"Low\", \"Moderate\", \"High\")),\n    PV1SCIE_PROFICIENCY = factor(case_when(\n      PV1SCIE_LEVEL %in% c(\"Level 5\", \"Level 6\") ~ \"High\",\n      PV1SCIE_LEVEL %in% c(\"Level 3\", \"Level 4\") ~ \"Moderate\",\n      TRUE ~ \"Low\"\n    ), levels = c(\"Low\", \"Moderate\", \"High\"))\n  )\n\n# Calculate proportions across proficiency levels by STRATUM\nproportions &lt;- stu_qqq_SG_filtered %&gt;%\n  gather(key = \"Proficiency_Level\", value = \"Value\", PV1MATH_PROFICIENCY, PV1READ_PROFICIENCY, PV1SCIE_PROFICIENCY) %&gt;%\n  count(STRATUM, Proficiency_Level, Value) %&gt;%\n  group_by(STRATUM, Proficiency_Level) %&gt;%\n  mutate(Proportion = n / sum(n),\n         Value = factor(Value, levels = c(\"Low\", \"Moderate\", \"High\")),\n         Proficiency_Level = recode(Proficiency_Level, PV1MATH_PROFICIENCY = \"Math\", PV1READ_PROFICIENCY = \"Reading\", PV1SCIE_PROFICIENCY = \"Science\")) %&gt;%\n  ungroup()\n\n\n# Create the heatmap\nSTRATUM_heatmap &lt;- ggplot(proportions, aes(x = STRATUM, y = factor(Value), fill = Proportion)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"steelblue\", labels = percent_format()) +\n  facet_wrap(~ Proficiency_Level, scales = \"free_x\") +\n  labs(x = \"School Type\",\n       y = \"Proficiency Level\",\n       fill = \"Proportion\",\n       subtitle = \"Heatmap of variation in proficiency between schools\") +\n  theme_minimal() +\n  theme(panel.background = element_rect(fill = \"white\", colour = NA),\n    axis.text.x = element_text(size = 8), \n    axis.text.y = element_text(angle = 45, size = 8),\n    axis.title.x = element_text(size = 8),\n    axis.title.y = element_text(size = 8), \n    legend.text = element_text(size = 8),\n    legend.title = element_text(size = 8),\n    plot.subtitle = element_text(size = 8)\n  )\n\nSTRATUM_boxplot + STRATUM_heatmap\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nInsights\n\nFrom the boxplot it is observed that public schools exhibit a wider range of performance scores in all three subjects compared to private schools, indicated by the larger spread in interquartile range.\nThis finding somewhat corroborates with the heatmap. The darker a tile on the heat map, the high the proportion of students with that proficiency level. From the heatmap, private schools have darker shades in the ‘Moderate proficiency’, compared to public schools where the proportion of students is more evenly spread out across the 3 defined proficiency levels.\nAdmittedly, a heatmap would be better if data was in matrix form or across two continuous variables. It was an interesting exercise to explore geom_tiles() for this purpose, to represent proportion with intensity of colour."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#insights-2",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#insights-2",
    "title": "Take-home Exercise 1",
    "section": "Insights",
    "text": "Insights\n\nFrom the boxplot it is observed that public schools exhibit a wider range of performance scores in all three subjects compared to private schools, indicated by the larger spread in interquartile range.\nThis finding somewhat corroborates with the heatmap. The darker a tile on the heat map, the high the proportion of students with that proficiency level. From the heatmap, private schools have darker shades in the ‘Moderate proficiency’, compared to public schools where the proportion of students is more evenly spread out across the 3 defined proficiency levels.\nAdmittedly, a heatmap would be better if data was in matrix form or across two continuous variables. It was an interesting exercise to explore geom_tiles() for this purpose, to represent proportion with intensity of colour."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#eda-4-exploring-relationship-of-performance-with-socioeconomic-status",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#eda-4-exploring-relationship-of-performance-with-socioeconomic-status",
    "title": "Take-home Exercise 1",
    "section": "EDA 4: Exploring relationship of performance with socioeconomic status",
    "text": "EDA 4: Exploring relationship of performance with socioeconomic status\nThe variables representing socioeconomic status are the continuous variable ESCS and the derived categorical variable SOCIOECONOMICSTATUS.\nInitial attempts to plot scatterplots to explore correlation between ESCS and plausible value 1 for each of the subjects is not shown due to the scatter plots not exhibiting any discernible trend. This could mean that the relationship could be weak or variation could be high.\nHence, we explore the relationship of socioeconomic status with the derived variable SOCIOECONOMICSTATUS instead. To recap , SOCIOECONOMICSTATUS was obtained by ordering students from the most disadvantaged to the most advantaged and creating four groups of students of equal size (each comprising 25% of the population).\nWe will use a ridgeplot to visualize this relationship.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Create the ridge plot\nSES_ridgeplot &lt;- ggplot(stu_qqq_SG_filtered_long, aes(x = PV1, y = SOCIOECONOMICSTATUS, fill = 0.5 - abs(0.5 - stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\", direction = -1) +\n  facet_wrap(~ Subject, scales = \"free_x\") + # Faceting by subject\n  labs(title = \"Students with more advantageous socioeconomic status tend to perform better\",\n       subtitle = \"Density plots of performance stratified by socioeconomic status\",\n       x = \"Score\") +\n  scale_x_continuous(limits = c(0, 1000)) + \n  theme_minimal() +\n  theme(legend.position = \"none\", # Hides the legend\n        axis.title.x = element_blank(), # Hides the x-axis title\n        axis.title.y = element_blank(),\n        axis.text.x = element_blank(), # Hides the x-axis text/numbers\n        axis.ticks.x = element_blank(), # Hides the x-axis ticks\n        plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(size = 8))\n\n\nSES_ridgeplot\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nInsights\n\nAcross all three subjects, the density curves show that students from more advantaged socioeconomic backgrounds tend to have higher performance scores.\nGiven this consistent trend, there could be a strong relationship between socioeconomic status and performance."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#insights-3",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#insights-3",
    "title": "Take-home Exercise 1",
    "section": "Insights",
    "text": "Insights\n\nAcross all three subjects, the density curves show that students from more advantaged socioeconomic backgrounds tend to have higher performance scores.\nGiven this consistent trend, there could be a strong relationship between socioeconomic status and performance."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#eda-5-exploring-relationship-of-performance-with-immigrant-status",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#eda-5-exploring-relationship-of-performance-with-immigrant-status",
    "title": "Take-home Exercise 1",
    "section": "EDA 5: Exploring relationship of performance with immigrant status",
    "text": "EDA 5: Exploring relationship of performance with immigrant status\nEarlier, it was found that IMMIG has 236 out of 6606 values missing. We will remove the rows with these missing values, before performing EDA.\n\nstu_qqq_SG_filtered_long_IMMIG &lt;- stu_qqq_SG_filtered_long %&gt;%\n  filter(!is.na(IMMIG))\n\nTo compare the perfomances across the immigrant statuses, we will use distributions in the form of density plots and boxplots.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(stu_qqq_SG_filtered_long_IMMIG,\n       aes(x = IMMIG, \n           y = PV1,\n           fill = IMMIG)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .10,\n               outlier.shape = NA,\n               alpha = 0.2) + \n  facet_wrap(~ Subject, scales = \"free_x\") + # Faceting by subject\n  theme_minimal() +\n  labs(title = \"2nd Generation Immigrant Students tend to perform better\",\n       subtitle = \"Density and boxplots stratified by immigrant status showing plausible value for all subjects\") +\n  coord_flip() +\n  theme(legend.position = \"none\", # Hides the legend\n        plot.title = element_text(face = \"bold\", size = 12),\n        plot.subtitle = element_text(size = 8),\n        axis.title.x =element_text(size = 8),\n        axis.title.y = element_text(size = 8),\n        axis.text.x = element_text(size = 8),\n        plot.margin = unit(c(0.25, 0.5, 0.25, 0.5), \n                                \"inches\"))\n\n\n\n\n\n\n\n\n\n\nInsights\n\n\n\nInsights\n\n2nd generation students consistently performed better across all subjects, followed by 1st generation and then native students.\nObserving the shape of the density plots, while most resemble normal distribution, there are some variations. For example, for 1st generation students, there is some fluctuation in the subject of science."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#insights-4",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#insights-4",
    "title": "Take-home Exercise 1",
    "section": "Insights",
    "text": "Insights\n\n2nd generation students consistently performed better across all subjects, followed by 1st generation and then native students.\nObserving the shape of the density plots, while most resemble normal distribution, there are some variations. For example, for 1st generation students, there is some fluctuation in the subject of science."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#summary-and-conclusion",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#summary-and-conclusion",
    "title": "Take-home Exercise 1",
    "section": "Summary and conclusion",
    "text": "Summary and conclusion\nIn this exercise, we performed data cleaning and visualization of the data to examine the relationship of performance with factors such as gender, school, socioeconomic status and immigrant status. The key findings are:\n\nStudents in Singapore performed slightly better in Mathematics over all oversubjects.\nMale students performed slightly better in Math while female students performed slightly better in Reading.\nPublic schools exhibit a wider range of performance scores in all three subjects compared to private schools.\nStudents who are more socioeconomically advantaged tend to perform better than their peers, across all subjects.\n2nd-generation students generally perform better than other students across all subjects.\n\nTo conclude with recommendations, Since this exercise focused solely on visuals, it’s important to use statistical tests to support all the findings. Furthermore, it’s worth noting that the original dataset contained numerous unexplored variables, and some of these variables are quite complex, as they are derived from the values of others. It may be worthwhile to explore the relationships between these variables to uncover more insights of the dataset. Lastly, this exercise has been a good practice of ggplot fundamentals; but the plot quality can be improved by making more extensive use of other features.\nOverall, this exercise was a helpful way to practice presenting data effectively in various ways to convey a clear message."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#references",
    "href": "Take-home_Ex/Take-home_Ex1/Take-home_Ex1.html#references",
    "title": "Take-home Exercise 1",
    "section": "References",
    "text": "References\n\nCode Book\nHow to prepare and analyse the PISA database\nPISA Data Analysis Manual: SPSS and SAS, Second Edition"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html",
    "title": "In-class Exercise 6",
    "section": "",
    "text": "pacman::p_load(ggHoriPlot, ggthemes, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#load-libraries",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#load-libraries",
    "title": "In-class Exercise 6",
    "section": "",
    "text": "pacman::p_load(ggHoriPlot, ggthemes, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#import-data",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#import-data",
    "title": "In-class Exercise 6",
    "section": "Import Data",
    "text": "Import Data\n\naverp &lt;- read_csv(\"data/AVERP.csv\") %&gt;%\n  mutate(`Date` = dmy(`Date`))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#plot-horizon-plot",
    "href": "In-class_Ex/In-class_Ex6/In-class_Ex6.html#plot-horizon-plot",
    "title": "In-class Exercise 6",
    "section": "Plot Horizon Plot",
    "text": "Plot Horizon Plot\n\naverp %&gt;% \n  filter(Date &gt;= \"2018-01-01\") %&gt;%\n  ggplot() +\n  geom_horizon(aes(x = Date, y=Values), \n               origin = \"midpoint\", \n               horizonscale = 6)+\n  facet_grid(`Consumer Items`~.) +\n    theme_few() +\n  scale_fill_hcl(palette = 'RdBu') +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Average Retail Prices of Selected Consumer Items (Jan 2018 to Dec 2022)')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "In this exercise I will create the following visualizations using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart\n\n\n\n\n\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)\n\n\n\n\nWe will plot a calendar heatmap that shows the number of cyber attacks across all countries (colour) by weekday (row) and hour of the day (column).\n\n\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\n\nWe will use kable() to examine the structure of the datarame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns:\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n\nTo plot the calendar heatmap, we will define two new fields:\n\nwkday\nhour\n\nWe will write a function to perform this.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) { # the three columns will be the inputs to this function\n  # use ymd_hms() from lubridate to transform dates stored as ch / num to POSIXct date-time. assumes ch contains year, month, day, hour, min, sec components. \n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1],\n                        quiet = TRUE)\n  \n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times), # extract just the weekday\n                   hour = hour(real_times) # extract just the hour\n                   )\n  \n  return(dt)\n}\n\nNext, derive the attacks dataframe with the columns for wkday and hour:\n\nwkday_levels &lt;-  c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\n# group_by ensures that the make_hr_wkday function is applied based on the unique values of the tz column. This means that each resulting group will have its own tz value.\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;% \n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\nCreate a new data table with the count of attacks for each “combination” of wkday and hour.\n\n# Create new data table, grouped, where we count the number of attacks per wkday per hour of each wkday. \ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% # to count number of attacks for each combi of wkday and hour\n  ungroup() %&gt;%\n  na.omit() # used to exclude missing value.\n\nkable(head(grouped))\n\n\n\n\nwkday\nhour\nn\n\n\n\n\nSaturday\n0\n1081\n\n\nSaturday\n1\n1053\n\n\nSaturday\n2\n1088\n\n\nSaturday\n3\n1130\n\n\nSaturday\n4\n1183\n\n\nSaturday\n5\n1226\n\n\n\n\n\nPlot the heatmap.\n\nWith theme_tufte()Without theme_tufte()\n\n\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte() +\n  coord_equal() + # to ensure the plot will have an aspect ratio of 1:1\nscale_fill_gradient(name = \"# of attacks\", # used to creates a two colour gradient (low-high)\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will attempt to build multiple heatmaps for the top four countries with highest number of attacks without referring to the code in the exercise.\n\nStudent’s attemptAnswer\n\n\n\n# Identify the top_4_countries\ntop_4_countries &lt;- attacks %&gt;%\n  count(source_country) %&gt;%                  # Count number of attacks for each country\n  na.omit() %&gt;%                              \n  arrange(desc(n)) %&gt;%                       # Arrange by number of attacks\n  top_n(4, n)    \n\n# Check that are only 4 countries\nunique(top_4_countries$source_country)\n\n[1] \"CN\" \"US\" \"KR\" \"NL\"\n\n# Filter the original dataset for these 4 countries\nattacks_top_4 &lt;- attacks %&gt;%\n  filter(source_country %in% unique(top_4_countries$source_country)) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  na.omit()%&gt;%\n  ungroup()\n\n# Re-order source_country factor levels based on total number of attacks\nattacks_top_4$source_country &lt;- factor(attacks_top_4$source_country, levels = top_4_countries$source_country)\n\nDT::datatable(attacks_top_4, \n          class= \"compact\",\n          rownames = FALSE,\n          width=\"100%\", \n          options = list(pageLength = 10,scrollX=T))\n\n\n\n\n\n\n# Plot heatmap for these 4 countries with facet\nggplot(attacks_top_4,\n       aes(hour, wkday,\n           fill = n)) +\n  geom_tile(color = \"white\", \n          size = 0.1) + \n  coord_equal() +\n  facet_wrap(~ source_country) +\n  theme_tufte() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL,\n     y = NULL,\n     title = \"Attacks on top 4 countries, by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte() + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere I will plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\nY-axis: Number of visitors\nX-axis: Months ### Import data\n\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\nkable(head(air))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth-Year\nRepublic of South Africa\nCanada\nUSA\nBangladesh\nBrunei\nChina\nHong Kong SAR (China)\nIndia\nIndonesia\nJapan\nSouth Korea\nKuwait\nMalaysia\nMyanmar\nPakistan\nPhilippines\nSaudi Arabia\nSri Lanka\nTaiwan\nThailand\nUnited Arab Emirates\nVietnam\nBelgium & Luxembourg\nCIS\nFinland\nFrance\nGermany\nIreland\nItaly\nNetherlands\nSpain\nSwitzerland\nUnited Kingdom\nAustralia\nNew Zealand\n\n\n\n\n2000-01-01\n3291\n5545\n25906\n2883\n3749\n33895\n13692\n19235\n65151\n59288\n21457\n507\n27472\n1177\n2150\n8404\n1312\n3922\n15766\n12048\n1318\n1527\n1434\n2703\n1634\n4752\n12739\n1292\n3544\n4962\n925\n3731\n28986\n34616\n5034\n\n\n2000-02-01\n2357\n6120\n28262\n2469\n3236\n34344\n19870\n18975\n37105\n58188\n19634\n199\n29084\n1161\n2496\n9128\n623\n3988\n24861\n12745\n899\n2269\n1596\n1182\n1297\n6391\n13093\n1200\n2897\n5054\n747\n3980\n35148\n26030\n3938\n\n\n2000-03-01\n4036\n6255\n30439\n2904\n3342\n27053\n17086\n21049\n44205\n74426\n20719\n386\n30504\n1355\n2429\n11691\n1578\n4259\n18767\n16971\n1474\n2034\n1548\n1088\n1220\n5528\n13645\n1368\n2717\n4950\n935\n3576\n36117\n31119\n4668\n\n\n2000-04-01\n4241\n4521\n25378\n2843\n5117\n30464\n22346\n26160\n45480\n49985\n17489\n221\n34478\n1593\n2711\n14141\n705\n6579\n22735\n20397\n1284\n2420\n1592\n1012\n1208\n5544\n13366\n1345\n2512\n4149\n941\n3850\n33792\n34824\n6890\n\n\n2000-05-01\n2841\n3914\n26163\n2793\n4152\n30775\n16357\n35869\n38350\n48937\n19398\n164\n34795\n1397\n2594\n13305\n679\n4625\n18399\n15769\n1042\n1833\n1167\n660\n743\n4225\n10878\n1067\n2205\n3643\n764\n3025\n23377\n33139\n7006\n\n\n2000-06-01\n2776\n3487\n28179\n3146\n5018\n26720\n18133\n31314\n47982\n53798\n17522\n440\n34660\n1715\n2924\n10555\n2749\n4740\n21042\n17217\n1545\n2480\n1170\n712\n982\n4047\n9054\n1363\n2196\n3544\n855\n2580\n21769\n35731\n7634\n\n\n\n\n\n\n\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\n\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nBefore getting started, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\npacman::p_load(CGPfunctions)\n\n\n\n\nrice &lt;- read_csv(\"data/rice.csv\")\nkable(head(rice))\n\n\n\n\nCountry\nYear\nYield\nProduction\n\n\n\n\nChina\n1961\n20787\n56217601\n\n\nChina\n1962\n23700\n65675288\n\n\nChina\n1963\n26833\n76439280\n\n\nChina\n1964\n28289\n85853780\n\n\nChina\n1965\n29667\n90705630\n\n\nChina\n1966\n31445\n98403990\n\n\n\n\n\n\n\n\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Countries\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#overview",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "In this exercise I will create the following visualizations using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#loading-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#loading-libraries",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "pacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-calendar-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-calendar-heatmap",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "We will plot a calendar heatmap that shows the number of cyber attacks across all countries (colour) by weekday (row) and hour of the day (column).\n\n\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\n\nWe will use kable() to examine the structure of the datarame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns:\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n\nTo plot the calendar heatmap, we will define two new fields:\n\nwkday\nhour\n\nWe will write a function to perform this.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) { # the three columns will be the inputs to this function\n  # use ymd_hms() from lubridate to transform dates stored as ch / num to POSIXct date-time. assumes ch contains year, month, day, hour, min, sec components. \n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1],\n                        quiet = TRUE)\n  \n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times), # extract just the weekday\n                   hour = hour(real_times) # extract just the hour\n                   )\n  \n  return(dt)\n}\n\nNext, derive the attacks dataframe with the columns for wkday and hour:\n\nwkday_levels &lt;-  c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\n# group_by ensures that the make_hr_wkday function is applied based on the unique values of the tz column. This means that each resulting group will have its own tz value.\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;% \n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\nCreate a new data table with the count of attacks for each “combination” of wkday and hour.\n\n# Create new data table, grouped, where we count the number of attacks per wkday per hour of each wkday. \ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% # to count number of attacks for each combi of wkday and hour\n  ungroup() %&gt;%\n  na.omit() # used to exclude missing value.\n\nkable(head(grouped))\n\n\n\n\nwkday\nhour\nn\n\n\n\n\nSaturday\n0\n1081\n\n\nSaturday\n1\n1053\n\n\nSaturday\n2\n1088\n\n\nSaturday\n3\n1130\n\n\nSaturday\n4\n1183\n\n\nSaturday\n5\n1226\n\n\n\n\n\nPlot the heatmap.\n\nWith theme_tufte()Without theme_tufte()\n\n\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte() +\n  coord_equal() + # to ensure the plot will have an aspect ratio of 1:1\nscale_fill_gradient(name = \"# of attacks\", # used to creates a two colour gradient (low-high)\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will attempt to build multiple heatmaps for the top four countries with highest number of attacks without referring to the code in the exercise.\n\nStudent’s attemptAnswer\n\n\n\n# Identify the top_4_countries\ntop_4_countries &lt;- attacks %&gt;%\n  count(source_country) %&gt;%                  # Count number of attacks for each country\n  na.omit() %&gt;%                              \n  arrange(desc(n)) %&gt;%                       # Arrange by number of attacks\n  top_n(4, n)    \n\n# Check that are only 4 countries\nunique(top_4_countries$source_country)\n\n[1] \"CN\" \"US\" \"KR\" \"NL\"\n\n# Filter the original dataset for these 4 countries\nattacks_top_4 &lt;- attacks %&gt;%\n  filter(source_country %in% unique(top_4_countries$source_country)) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  na.omit()%&gt;%\n  ungroup()\n\n# Re-order source_country factor levels based on total number of attacks\nattacks_top_4$source_country &lt;- factor(attacks_top_4$source_country, levels = top_4_countries$source_country)\n\nDT::datatable(attacks_top_4, \n          class= \"compact\",\n          rownames = FALSE,\n          width=\"100%\", \n          options = list(pageLength = 10,scrollX=T))\n\n\n\n\n\n\n# Plot heatmap for these 4 countries with facet\nggplot(attacks_top_4,\n       aes(hour, wkday,\n           fill = n)) +\n  geom_tile(color = \"white\", \n          size = 0.1) + \n  coord_equal() +\n  facet_wrap(~ source_country) +\n  theme_tufte() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL,\n     y = NULL,\n     title = \"Attacks on top 4 countries, by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package.\n\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte() + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-cycle-plot",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-cycle-plot",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "Here I will plot a cycle plot showing the time-series patterns and trend of visitor arrivals from Vietnam programmatically by using ggplot2 functions.\n\nY-axis: Number of visitors\nX-axis: Months ### Import data\n\n\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\nkable(head(air))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonth-Year\nRepublic of South Africa\nCanada\nUSA\nBangladesh\nBrunei\nChina\nHong Kong SAR (China)\nIndia\nIndonesia\nJapan\nSouth Korea\nKuwait\nMalaysia\nMyanmar\nPakistan\nPhilippines\nSaudi Arabia\nSri Lanka\nTaiwan\nThailand\nUnited Arab Emirates\nVietnam\nBelgium & Luxembourg\nCIS\nFinland\nFrance\nGermany\nIreland\nItaly\nNetherlands\nSpain\nSwitzerland\nUnited Kingdom\nAustralia\nNew Zealand\n\n\n\n\n2000-01-01\n3291\n5545\n25906\n2883\n3749\n33895\n13692\n19235\n65151\n59288\n21457\n507\n27472\n1177\n2150\n8404\n1312\n3922\n15766\n12048\n1318\n1527\n1434\n2703\n1634\n4752\n12739\n1292\n3544\n4962\n925\n3731\n28986\n34616\n5034\n\n\n2000-02-01\n2357\n6120\n28262\n2469\n3236\n34344\n19870\n18975\n37105\n58188\n19634\n199\n29084\n1161\n2496\n9128\n623\n3988\n24861\n12745\n899\n2269\n1596\n1182\n1297\n6391\n13093\n1200\n2897\n5054\n747\n3980\n35148\n26030\n3938\n\n\n2000-03-01\n4036\n6255\n30439\n2904\n3342\n27053\n17086\n21049\n44205\n74426\n20719\n386\n30504\n1355\n2429\n11691\n1578\n4259\n18767\n16971\n1474\n2034\n1548\n1088\n1220\n5528\n13645\n1368\n2717\n4950\n935\n3576\n36117\n31119\n4668\n\n\n2000-04-01\n4241\n4521\n25378\n2843\n5117\n30464\n22346\n26160\n45480\n49985\n17489\n221\n34478\n1593\n2711\n14141\n705\n6579\n22735\n20397\n1284\n2420\n1592\n1012\n1208\n5544\n13366\n1345\n2512\n4149\n941\n3850\n33792\n34824\n6890\n\n\n2000-05-01\n2841\n3914\n26163\n2793\n4152\n30775\n16357\n35869\n38350\n48937\n19398\n164\n34795\n1397\n2594\n13305\n679\n4625\n18399\n15769\n1042\n1833\n1167\n660\n743\n4225\n10878\n1067\n2205\n3643\n764\n3025\n23377\n33139\n7006\n\n\n2000-06-01\n2776\n3487\n28179\n3146\n5018\n26720\n18133\n31314\n47982\n53798\n17522\n440\n34660\n1715\n2924\n10555\n2749\n4740\n21042\n17217\n1545\n2480\n1170\n712\n982\n4047\n9054\n1363\n2196\n3544\n855\n2580\n21769\n35731\n7634\n\n\n\n\n\n\n\n\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\n\n\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n\n\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\n\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"black\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"red\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_minimal()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-slopegraph",
    "href": "Hands-on_Ex/Hands-on_Ex6/Hands-on_Ex6.html#plotting-slopegraph",
    "title": "Hands-on Exercise 6",
    "section": "",
    "text": "Before getting started, make sure that CGPfunctions has been installed and loaded onto R environment. Then, refer to Using newggslopegraph to learn more about the function. Lastly, read more about newggslopegraph() and its arguments by referring to this link.\n\npacman::p_load(CGPfunctions)\n\n\n\n\nrice &lt;- read_csv(\"data/rice.csv\")\nkable(head(rice))\n\n\n\n\nCountry\nYear\nYield\nProduction\n\n\n\n\nChina\n1961\n20787\n56217601\n\n\nChina\n1962\n23700\n65675288\n\n\nChina\n1963\n26833\n76439280\n\n\nChina\n1964\n28289\n85853780\n\n\nChina\n1965\n29667\n90705630\n\n\nChina\n1966\n31445\n98403990\n\n\n\n\n\n\n\n\n\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Countries\",\n                SubTitle = \"1961-1980\",\n                Caption = \"Prepared by: Dr. Kam Tin Seong\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data.\nIt is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nHere I practice:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package.\n\n\n\n\n\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\n\n\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\n\n\n\n\n\n\nggparcoord(data = wh,  # used to map the data object (i.e. wh)\n           columns = c(7:12)) # used to select the columns for preparing the parallel coordinates plot\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happiness Variables\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\n\n\n\nWe can rotate axis text labels using theme() function in ggplot2.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\n\n\n\nparallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js.\n\n\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\n\n\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#overview",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "Parallel coordinates plot is a data visualisation specially designed for visualising and analysing multivariate, numerical data.\nIt is ideal for comparing multiple variables together and seeing the relationships between them. For example, the variables contribute to Happiness Index. Parallel coordinates was invented by Alfred Inselberg in the 1970s as a way to visualize high-dimensional data. This data visualisation technique is more often found in academic and scientific communities than in business and consumer data visualizations. As pointed out by Stephen Few(2006), “This certainly isn’t a chart that you would present to the board of directors or place on your Web site for the general public. In fact, the strength of parallel coordinates isn’t in their ability to communicate some truth in the data to others, but rather in their ability to bring meaningful multivariate patterns and comparisons to light when used interactively for analysis.” For example, parallel coordinates plot can be used to characterise clusters detected during customer segmentation.\nHere I practice:\n\nplotting statistic parallel coordinates plots by using ggparcoord() of GGally package,\nplotting interactive parallel coordinates plots by using parcoords package, and\nplotting interactive parallel coordinates plots by using parallelPlot package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#loading-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#loading-libraries",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "pacman::p_load(GGally, parallelPlot, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#import-and-prepare-data",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#import-and-prepare-data",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "wh &lt;- read_csv(\"data/WHData-2018.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#plotting-static-parallel-coordinates-plot-using-ggparcoord",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#plotting-static-parallel-coordinates-plot-using-ggparcoord",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "ggparcoord(data = wh,  # used to map the data object (i.e. wh)\n           columns = c(7:12)) # used to select the columns for preparing the parallel coordinates plot\n\n\n\n\n\n\n\n\n\n\n\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happiness Variables\")\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ngroupColumn argument is used to group the observations (i.e. parallel lines) by using a single variable (i.e. Region) and colour the parallel coordinates lines by region name.\nscale argument is used to scale the variables in the parallel coordinate plot by using uniminmax method. The method univariately scale each variable so the minimum of the variable is zero and the maximum is one.\nalphaLines argument is used to reduce the intensity of the line colour to 0.2. The permissible value range is between 0 to 1.\nboxplot argument is used to turn on the boxplot by using logical TRUE. The default is FALSE.\ntitle argument is used to provide the parallel coordinates plot a title.\n\n\n\n\nIn the code chunk below, facet_wrap() of ggplot2 is used to plot 10 small multiple parallel coordinates plots. Each plot represent one geographical region such as East Asia.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\n\n\n\nWe can rotate axis text labels using theme() function in ggplot2.\n\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5d.html#plotting-interactive-parallel-coordinates-plot-parallelplot-methods",
    "title": "Hands-on Exercise 5d",
    "section": "",
    "text": "parallelPlot is an R package specially designed to plot a parallel coordinates plot by using ‘htmlwidgets’ package and d3.js.\n\n\n\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\n\n\n\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\nOne of the useful interactive feature of parallelPlot is we can click on a variable of interest, for example Happiness score, the monotonous blue colour (default) will change a blues with different intensity colour scheme will be used.\n\n\n\n\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\n\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "Here I practice the following:\n\nCreating correlation matrix using pairs() of R Graphics\nPlotting corrgram using corrplot package\n\n\n\n\n\npacman::p_load(tidyverse, corrplot, ggstatsplot)\n\n\n\n\n\n\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\n\n\n\n\n\n\n\npairs(wine[,1:11]) # Means to select all rows, and columns 1 to 11. \n\n\n\n\n\n\n\n\n\npairs(wine[,2:12]) # TO select all rows from col 2 to 12\n\n\n\n\n\n\n\n\n\n\n\nIt is common practice to show either upper half or lower half of the correlation matrix instead of both, because a correlation matrix is symmetric.\nWe can customize this using upper.panel or lower.panel.\n\npairs(wine[,2:5], upper.panel = NULL)\n\n\n\n\n\n\n\n\n\npairs(wine[,2:5], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor) {\n    usr &lt;- par(\"usr\")\n    par(usr = c(0, 1, 0, 1))\n    r &lt;- abs(cor(x, y, use = \"complete.obs\"))\n    txt &lt;- format(c(r, 0.123456789), digits = digits)[1]\n    txt &lt;- paste(prefix, txt, sep = \"\")\n    if (missing(cex.cor)) cex.cor &lt;- 0.8 / strwidth(txt)\n    text(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\n\npairs(wine[,2:5], \n      upper.panel = panel.cor)\n\n\n\n\n\n\n\n\n\n\n\n\nThere are at least three R packages that provide functions to plot corrgram:\n\ncorrgram\nellipse\ncorrplot\n\nIn this section, we will practice visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\n\n\n\nggcorrmat(\n  data = wine, \n  cor.vars = 1:11 # used to compute the correlation matrix needed to build the corrgram\n  )\n\n\n\n\n\n\n\n\n\nggcorrmat(\n  data = wine, \n  cor.vars = 1:11, # used to compute the correlation matrix needed to build the corrgram\n  ggcorrplot.args =  #  provide additional aesthetic arguments\n    list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\",\n  ggplot.component = list(\n    theme(text=element_text(size=10),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor ggcorrplot.arg, the list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits.\n\n\n\n\n\n\nTo plot facet, we need to use grouped_ggcorrmat()\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\nFirst, read up on this link.\n\nabout 50 parameters including method, type, order, diag\ncolor intensity of glyph is proportional to correlation coefficient by default\nThe colors of the correlation plots can be customized by col in corrplot()\n\n\n\nFirst we compute the correlation matrix of the data frame:\n\nwine.cor &lt;- cor(wine[,1:11])\n\nNex, we plot the corrgram:\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\nThings that can be changed:\n\nVisualization method (method)\nLayout (type)\nColor scheme (diverging or sequential)\ncolor choice for positive or negative correlation coefficient\nsaturation to represent strength of correlation coeffcient\n\n\n\n\nSeven visualization methods ‘circle’, ‘square’, ‘ellipse’, ‘number’, ‘shade’, ‘color’, ‘pie’ Vary the visualization method with method argument.\n\ncorrplot(wine.cor, \n         method = \"color\") \n\n\n\n\n\n\n\n\n\n\n\n\nuse type argument to vary layout: “full”, “upper” or “lower”\n\n\ncorrplot(wine.cor, \n         method = \"color\", \n         type=\"upper\")\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(): Set visual methods of lower and upper triangular separately\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"color\", \n               upper = \"pie\", #  circles are filled clockwise for positive values, anti-clockwise for negative values\n               tl.pos = \"lt\", # “d”, “lt”, “n” text label in diagonal, lt is left and top, n is none\n               diag = \"u\", # “n”, “l”, “u” none, lower or upper\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\np.mat is the p-value matrix.\nWe can get p-value matrix and confidence intervals matrix by cor.mtest() which returns a list containing: p is the p-values matrix, lowCI is the lower bound of confidence interval matrix. uppCI is the lower bound of confidence interval matrix.\n\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nThen use p.mat argument:\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n\nWe can re-order the matrix according to correlation coefficients using order argument.\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\nMore algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"color\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\n'hclust.method' should be one of 'ward', 'ward.D', 'ward.D2', 'single', 'complete', 'average', 'mcquitty', 'median' or 'centroid'.\n\n\ncorrplot(wine.cor, \n         method = \"color\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#overview",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "Here I practice the following:\n\nCreating correlation matrix using pairs() of R Graphics\nPlotting corrgram using corrplot package"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#loading-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#loading-libraries",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "pacman::p_load(tidyverse, corrplot, ggstatsplot)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#import-and-prepare-data",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#import-and-prepare-data",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "wine &lt;- read_csv(\"data/wine_quality.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#building-correlation-matrix-with-pairs-method",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#building-correlation-matrix-with-pairs-method",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "pairs(wine[,1:11]) # Means to select all rows, and columns 1 to 11. \n\n\n\n\n\n\n\n\n\npairs(wine[,2:12]) # TO select all rows from col 2 to 12\n\n\n\n\n\n\n\n\n\n\n\nIt is common practice to show either upper half or lower half of the correlation matrix instead of both, because a correlation matrix is symmetric.\nWe can customize this using upper.panel or lower.panel.\n\npairs(wine[,2:5], upper.panel = NULL)\n\n\n\n\n\n\n\n\n\npairs(wine[,2:5], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor) {\n    usr &lt;- par(\"usr\")\n    par(usr = c(0, 1, 0, 1))\n    r &lt;- abs(cor(x, y, use = \"complete.obs\"))\n    txt &lt;- format(c(r, 0.123456789), digits = digits)[1]\n    txt &lt;- paste(prefix, txt, sep = \"\")\n    if (missing(cex.cor)) cex.cor &lt;- 0.8 / strwidth(txt)\n    text(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\n\npairs(wine[,2:5], \n      upper.panel = panel.cor)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#visualising-correlation-matrix-ggcormat",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#visualising-correlation-matrix-ggcormat",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "There are at least three R packages that provide functions to plot corrgram:\n\ncorrgram\nellipse\ncorrplot\n\nIn this section, we will practice visualising correlation matrix by using ggcorrmat() of ggstatsplot package.\n\n\n\nggcorrmat(\n  data = wine, \n  cor.vars = 1:11 # used to compute the correlation matrix needed to build the corrgram\n  )\n\n\n\n\n\n\n\n\n\nggcorrmat(\n  data = wine, \n  cor.vars = 1:11, # used to compute the correlation matrix needed to build the corrgram\n  ggcorrplot.args =  #  provide additional aesthetic arguments\n    list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\",\n  ggplot.component = list(\n    theme(text=element_text(size=10),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor ggcorrplot.arg, the list should avoid any of the following arguments since they are already internally being used: corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, digits."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#building-multiple-plots",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#building-multiple-plots",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "To plot facet, we need to use grouped_ggcorrmat()\n\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for wine dataset\",\n    subtitle = \"The measures are: alcohol, sulphates, fixed acidity, citric acid, chlorides, residual sugar, density, free sulfur dioxide and volatile acidity\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#visualising-correlation-matrix-using-corrplot",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5b.html#visualising-correlation-matrix-using-corrplot",
    "title": "Hands-on Exercise 5b",
    "section": "",
    "text": "First, read up on this link.\n\nabout 50 parameters including method, type, order, diag\ncolor intensity of glyph is proportional to correlation coefficient by default\nThe colors of the correlation plots can be customized by col in corrplot()\n\n\n\nFirst we compute the correlation matrix of the data frame:\n\nwine.cor &lt;- cor(wine[,1:11])\n\nNex, we plot the corrgram:\n\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\nThings that can be changed:\n\nVisualization method (method)\nLayout (type)\nColor scheme (diverging or sequential)\ncolor choice for positive or negative correlation coefficient\nsaturation to represent strength of correlation coeffcient\n\n\n\n\nSeven visualization methods ‘circle’, ‘square’, ‘ellipse’, ‘number’, ‘shade’, ‘color’, ‘pie’ Vary the visualization method with method argument.\n\ncorrplot(wine.cor, \n         method = \"color\") \n\n\n\n\n\n\n\n\n\n\n\n\nuse type argument to vary layout: “full”, “upper” or “lower”\n\n\ncorrplot(wine.cor, \n         method = \"color\", \n         type=\"upper\")\n\n\n\n\n\n\n\n\n\n\n\n\ncorrplot.mixed(): Set visual methods of lower and upper triangular separately\n\n\ncorrplot.mixed(wine.cor, \n               lower = \"color\", \n               upper = \"pie\", #  circles are filled clockwise for positive values, anti-clockwise for negative values\n               tl.pos = \"lt\", # “d”, “lt”, “n” text label in diagonal, lt is left and top, n is none\n               diag = \"u\", # “n”, “l”, “u” none, lower or upper\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\np.mat is the p-value matrix.\nWe can get p-value matrix and confidence intervals matrix by cor.mtest() which returns a list containing: p is the p-values matrix, lowCI is the lower bound of confidence interval matrix. uppCI is the lower bound of confidence interval matrix.\n\n\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\nThen use p.mat argument:\n\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n\nWe can re-order the matrix according to correlation coefficients using order argument.\n\n“AOE” is for the angular order of the eigenvectors. See Michael Friendly (2002) for details.\n“FPC” for the first principal component order.\n“hclust” for hierarchical clustering order, and “hclust.method” for the agglomeration method to be used.\n\n“hclust.method” should be one of “ward”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.\n\n“alphabet” for alphabetical order.\n\nMore algorithms can be found in seriation package.\n\ncorrplot.mixed(wine.cor, \n               lower = \"color\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nIf using hclust, corrplot() can draw rectangles around the corrgram based on the results of hierarchical clustering.\n\n'hclust.method' should be one of 'ward', 'ward.D', 'ward.D2', 'single', 'complete', 'average', 'mcquitty', 'median' or 'centroid'.\n\n\ncorrplot(wine.cor, \n         method = \"color\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html",
    "title": "Hands-on Exercise 4",
    "section": "",
    "text": "In this exercise, I will practice visualising distributions using ridgeline plots and raincloud plots.\n\n\n\n\n\n\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n\nThe data used is the year end examination grades of a cohort of primary 3 students from a local school. From Hands-On Exercise 1, we know that the data has:\n\nFour categorical attributes: ID, CLASS, GENDER, RACE\nThree continuous attributes: MATHS, ENGLISH, SCIENCE\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\n\nRidgeline plots make sense when the number of group to represent is medium to high, and thus a classic window separation would take to much space. Indeed, the fact that groups overlap each other allows to use space more efficiently. If you have less than 5 groups, dealing with other distribution plots is probably better.\nIt works well when there is a clear pattern in the result, like if there is an obvious ranking in groups. Otherwise group will tend to overlap each other, leading to a messy plot not providing any insight.sthetic\n\nLet’s first demonstrate how we can make some graphs interactive with a tooltip.\n\n\nggridges package provides two main geom to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nWe demonstrate a plot using geom_density_ridges() :\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, \n       aes(x = SCIENCE, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3, # set amount of overlap between the ridges\n    rel_min_height = 0.01, # Lines with heights below this cutoff will be removed.\n    bandwidth = 3.4,\n    fill = \"#93c7c2\", # change the fill colour\n    color = \"white\", # color of outline\n    alpha = 0.8, # transparency of fill\n    linewidth = 0.4\n  ) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\n\n\n\nBesides geom_density_ridges_gradient(), geom_ridgeline_gradient() can also be used to achieve this effect. Note that we can only change either fill colors or transparency, but not both.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = SCIENCE, y = CLASS, fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3, # set amount of overlap between the ridges\n    rel_min_height = 0.01, # Lines with heights below this cutoff will be removed.\n\n  ) + \n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"cividis\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\n\n\n\nWe map the probabilities calculated by using stat(ecdf), which represent the empirical cumulative density function for the distribution of Science scores.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = SCIENCE, y = CLASS, fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) + \n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1,\n                       option = \"cividis\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\n\n\n\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown below.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = SCIENCE, y = CLASS, fill = factor(stat(quantile)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE,\n                      quantiles = 4, # Add quantiles\n                      quantile_lines = TRUE) + # Add quantile lines\n  scale_fill_viridis_d(name = \"Quartiles\") +  # change fill to scale_fill_viridis_d and name to Quartiles\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = SCIENCE, y = CLASS, fill = factor(stat(quantile)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE,\n                      quantiles = c(0.025, 0.975)) + # Specify quantile values\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360)) \n\n\n\n\n\n\n\n\nThe raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). It also shows where densities are clustered.\n\n\nNote that we remove the slab interval by setting .width = 0 and point_colour = NA.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = CLASS, y = SCIENCE)) +\n    stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\n\n\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = CLASS, y = SCIENCE)) +\n    stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\n\n\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = CLASS, y = SCIENCE)) +\n    stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2) +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\n\n\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = CLASS, y = SCIENCE)) +\n    stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA,\n               fill = \"#93c7c2\") +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2,\n               fill = \"#93c7c2\",\n            colour = \"#93c7c2\") +\n  coord_flip() +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#overview",
    "title": "Hands-on Exercise 4",
    "section": "",
    "text": "In this exercise, I will practice visualising distributions using ridgeline plots and raincloud plots."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#getting-started",
    "title": "Hands-on Exercise 4",
    "section": "",
    "text": "pacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n\nThe data used is the year end examination grades of a cohort of primary 3 students from a local school. From Hands-On Exercise 1, we know that the data has:\n\nFour categorical attributes: ID, CLASS, GENDER, RACE\nThree continuous attributes: MATHS, ENGLISH, SCIENCE\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#visualising-distribution-with-ridgeline-plot",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#visualising-distribution-with-ridgeline-plot",
    "title": "Hands-on Exercise 4",
    "section": "",
    "text": "Ridgeline plots make sense when the number of group to represent is medium to high, and thus a classic window separation would take to much space. Indeed, the fact that groups overlap each other allows to use space more efficiently. If you have less than 5 groups, dealing with other distribution plots is probably better.\nIt works well when there is a clear pattern in the result, like if there is an obvious ranking in groups. Otherwise group will tend to overlap each other, leading to a messy plot not providing any insight.sthetic\n\nLet’s first demonstrate how we can make some graphs interactive with a tooltip.\n\n\nggridges package provides two main geom to plot gridgeline plots, they are: geom_ridgeline() and geom_density_ridges(). The former takes height values directly to draw the ridgelines, and the latter first estimates data densities and then draws those using ridgelines.\nWe demonstrate a plot using geom_density_ridges() :\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, \n       aes(x = SCIENCE, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3, # set amount of overlap between the ridges\n    rel_min_height = 0.01, # Lines with heights below this cutoff will be removed.\n    bandwidth = 3.4,\n    fill = \"#93c7c2\", # change the fill colour\n    color = \"white\", # color of outline\n    alpha = 0.8, # transparency of fill\n    linewidth = 0.4\n  ) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\n\n\n\nBesides geom_density_ridges_gradient(), geom_ridgeline_gradient() can also be used to achieve this effect. Note that we can only change either fill colors or transparency, but not both.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = SCIENCE, y = CLASS, fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3, # set amount of overlap between the ridges\n    rel_min_height = 0.01, # Lines with heights below this cutoff will be removed.\n\n  ) + \n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"cividis\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\n\n\n\nWe map the probabilities calculated by using stat(ecdf), which represent the empirical cumulative density function for the distribution of Science scores.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = SCIENCE, y = CLASS, fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) + \n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1,\n                       option = \"cividis\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\n\n\n\nBy using geom_density_ridges_gradient(), we can colour the ridgeline plot by quantile, via the calculated stat(quantile) aesthetic as shown below.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = SCIENCE, y = CLASS, fill = factor(stat(quantile)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE,\n                      quantiles = 4, # Add quantiles\n                      quantile_lines = TRUE) + # Add quantile lines\n  scale_fill_viridis_d(name = \"Quartiles\") +  # change fill to scale_fill_viridis_d and name to Quartiles\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\nInstead of using number to define the quantiles, we can also specify quantiles by cut points such as 2.5% and 97.5% tails to colour the ridgeline plot as shown in the figure below.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = SCIENCE, y = CLASS, fill = factor(stat(quantile)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE,\n                      quantiles = c(0.025, 0.975)) + # Specify quantile values\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#visualising-distribution-with-raincloud-plot",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#visualising-distribution-with-raincloud-plot",
    "title": "Hands-on Exercise 4",
    "section": "",
    "text": "The raincloud (half-density) plot enhances the traditional box-plot by highlighting multiple modalities (an indicator that groups may exist). It also shows where densities are clustered.\n\n\nNote that we remove the slab interval by setting .width = 0 and point_colour = NA.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = CLASS, y = SCIENCE)) +\n    stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\n\n\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = CLASS, y = SCIENCE)) +\n    stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\n\n\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = CLASS, y = SCIENCE)) +\n    stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2) +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))\n\n\n\n\n\n\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x = CLASS, y = SCIENCE)) +\n    stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA,\n               fill = \"#93c7c2\") +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2,\n               fill = \"#93c7c2\",\n            colour = \"#93c7c2\") +\n  coord_flip() +\n  labs(title = \"Science scores generally improve across classes\",\n       subtitle = \"Distributions of Science scores across the various classes\",\n       x = \"Science scores\",\n       y = \"Classes\") +\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8),\n        axis.title.y = element_text(size = 8, angle = 360))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#overview-1",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#overview-1",
    "title": "Hands-on Exercise 4",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nIn this exercise, I will practice using:\n\nggstatsplot package to create visual graphics with rich statistical information,\nperformance package to visualise model diagnostics, and\nparameters package to visualise model parameters\n\nggstatsplot is an extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves. To provide alternative statistical inference methods by default. To follow best practices for statistical reporting. For all statistical tests reported in the plots, the default template abides by the APA gold standard for statistical reporting."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#getting-started-1",
    "title": "Hands-on Exercise 4",
    "section": "2.2 Getting started",
    "text": "2.2 Getting started\n\n2.2.1 Loading libraries\n\npacman::p_load(ggstatsplot, tidyverse,\n               rstantools)\n\n\n\n2.2.2 Importing data\nThe data used is the year end examination grades of a cohort of primary 3 students from a local school. From Hands-On Exercise 1, we know that the data has:\n\nFour categorical attributes: ID, CLASS, GENDER, RACE\nThree continuous attributes: MATHS, ENGLISH, SCIENCE\n\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n2.2.3 One-sample test: gghistostats() method\nIn the code chunk below, gghistostats() is used to to build an visual of one-sample test on Science scores.\n\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = SCIENCE,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"Science scores\"\n)\n\n\n\n\n\n\n\n\nA Bayes factor is the ratio of the likelihood of one particular hypothesis to the likelihood of another. It can be interpreted as a measure of the strength of evidence in favor of one theory among two competing theories.\nWhen we are comparing two hypotheses, H1 (the alternate hypothesis) and H0 (the null hypothesis), the Bayes Factor is often written as B10. In this case,\n\nNull Hypothesis (H0): The true mean of the science scores is equal to the test value of 60. This hypothesis posits that there is no significant difference between the mean score and the test value.\nAlternative Hypothesis (H1): The true mean of the science scores is not equal to 60. This hypothesis suggests that the mean score is significantly different from the test value.\n\nStatistical Annotations:\n\nlog_e(BF_01) = 2.12: This is the natural logarithm of the Bayes Factor (BF) comparing the null hypothesis (science scores = 60) to the alternative hypothesis. A Bayes Factor greater than 1 indicates evidence against the null, and the value here suggests that the data provide evidence against the null hypothesis H0 (since log_e(2.12) &gt; 0).\nΔ_posterior mean = 1.12: This indicates the difference between the sample mean and the test value (60), suggesting the average score is higher than the test value.\n95% CI: This confidence interval shows the range of values within which the true mean score lies with 95% probability, according to the posterior distribution.\nJZS = 0.71: This likely refers to a standardized effect size measure in a Bayesian context, possibly related to a Bayesian equivalent of Cohen’s d, used to indicate the magnitude of the difference between groups or conditions.\n\nTherefore, the Bayesian analysis suggests rejecting the null hypothesis in favor of the alternative hypothesis, meaning that the data provide substantial evidence that the true mean science score is different from the test value of 60, specifically that it’s higher since the posterior mean is indicated to be 70.50.\nInterpreting Bayes Factor:\n\n\n\n\n2.2.4 Two-sample mean test: ggbetweenstats()\nIn the code chunk below, ggbetweenstats() is used to build a visual for two-sample mean test of English scores by gender.\n\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = ENGLISH,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nThe ggbetweenstats function is used to create a plot that compares the distribution of Engish scores (numeric variable) across different levels of Gender (categorical variable). It also computes and displays statistical tests to assess whether there are statistically significant differences between these groups.\nThe type = \"np\" argument specifies that a non-parametric statistical test should be used for the comparison. Non-parametric tests do not assume that the data follows a normal distribution and are often used when the sample size is small, or the data is skewed or ordinal.\nThe hypothesis would be:\n\nNull Hypothesis (H0): There is no difference in the median English scores between genders. This means that any observed difference in the sample is due to random chance.\nAlternative Hypothesis (H1): There is a difference in the median English scores between genders. This means that the observed difference is not due to chance, and there is a true difference in the population from which the sample was drawn.\n\nStatistical annotations:\n\nMedian Scores: The plot indicates that the median English score for females is 73.00, while for males it is 67.00, suggesting a higher median score for females.\nSample Size: The sample sizes are given as n = 170 for females and n = 152 for males.\nMann-Whitney U Test: The test used is the Mann-Whitney U test, appropriate for comparing two independent samples. The test statistic W is 15627.50.\nP-Value: The p-value is 1.17e−03 (0.00117), which is less than the conventional threshold of 0.05 for statistical significance.\nEffect Size: The rank biserial is 0.21, which is an effect size measure based on the ranks of the data. This value suggests a small to medium effect size according to Cohen’s guidelines (0.1 is small, 0.3 is medium, 0.5 is large).\nThe confidence interval for this effect size is [0.09, 0.33], which does not include 0, further indicating that the effect is statistically significant.\nTotal Observations: The total number of observations (combined for both genders) is 322.\n\nGiven the small p-value (0.00117 &lt; 0.05) is what actually tells us there is a statistically significant difference between the English scores of males and females, with females having a higher median score. The effect size is small to medium, indicating that while the difference is statistically significant, the magnitude of the difference is not large.\n\n\n2.2.5 Oneway ANOVA Test: ggbetweenstats() method\nIn the code chunk below, ggbetweenstats() is used to build a visual for One-way ANOVA test on Maths score by race.\n\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = MATHS,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\nFor pairwise.display:\n\n“ns” → only non-significant\n“s” → only significant\n“all” → everything\n\n\n\n2.2.6 ggbetweenstats - Summary of tests\n\n\n\n\n\n2.2.7 Significant Test of Correlation: ggscatterstats()\nIn the code chunk below, ggscatterstats() is used to build a visual for Significant Test of Correlation between Maths scores and Science scores.\n\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = SCIENCE,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\n2.2.8 Significant Test of Association (Depedence) : ggbarstats() methods\n\n# First, bin the MATHS scores into a 4 class variable\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100)) # Specify values to break at\n)\n# Use ggbarstats() to build a visual for Significant Test of Association\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#visualising-models",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#visualising-models",
    "title": "Hands-on Exercise 4",
    "section": "2.3 Visualising Models",
    "text": "2.3 Visualising Models\nHere we visualize model diagnostic and model parameters by using parameters package.\n\n2.3.1 Loading libraries\n\npacman::p_load(readxl, performance, parameters, see, qqplotr)\n\n\n\n2.3.2 Importing data\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n\n\n2.3.3 Multiple Regression Model using lm()\nThe code chunk below is used to calibrate a multiple linear regression model by using lm() of Base Stats of R.\n\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n2.3.4 Model Diagnostic: Checking for multicolinearity using check_collinearity()\n\ncheck_collinearity(model)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n\n2.3.5 Model Diagnostic: checking normality assumption with check_normality()\n\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\ncheck_n &lt;- check_normality(model1)\n\nplot(check_n)\n\n\n\n\n\n\n\n\n\n\n2.3.6 Model Diagnostic: Check model for homogeneity of variances using check_heteroscedasticity()\n\ncheck_h &lt;- check_heteroscedasticity(model1)\n\nplot(check_h)\n\n\n\n\n\n\n\n\n\n\n2.3.7 Model Diagnostic: Complete check using check_model()\n\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\n\n2.3.8 Visualising Regression Parameters: see methods\nIn the code below, plot() of see package and parameters() of parameters package are used to visualise the parameters of a regression model.\n\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\n\n2.3.9 Visualising Regression Parameters: ggcoefstats() methods\n\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#overview-2",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#overview-2",
    "title": "Hands-on Exercise 4",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nIn this exercise, I will learn how to:\n\nto plot statistics error bars by using ggplot2,\nto plot interactive error bars by combining ggplot2, plotly and DT,\nto create advanced by using ggdist, and\nto create hypothetical outcome plots (HOPs) by using ungeviz package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#getting-started-2",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#getting-started-2",
    "title": "Hands-on Exercise 4",
    "section": "3.2 Getting Started",
    "text": "3.2 Getting Started\n\n3.2.1 Loading libraries\n\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)\n\n\n\n3.2.2 Import data\n\nexam &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#visualizing-uncertainty-of-point-estimates-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#visualizing-uncertainty-of-point-estimates-ggplot2-methods",
    "title": "Hands-on Exercise 4",
    "section": "3.3 Visualizing uncertainty of point estimates: ggplot2 methods",
    "text": "3.3 Visualizing uncertainty of point estimates: ggplot2 methods\nA point estimate is a single number, such as a mean. Uncertainty, on the other hand, is expressed as standard error, confidence interval, or credible interval.\n\n3.3.1 Derive summary statistics\n\n# First, derive summary statistics\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise( # used to compute the count of observations, mean, standard deviation\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1)) # used to derive standard error of Maths by RACE\n\nThe code chunk displays the my_sum tibble data frame in an html table format.\n\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n3.3.2 Plotting standard error bars of point estimates\nNow, we are ready to plot the standard error bars of mean maths score by race.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, # The error bars are computed by using the formula mean+/-se.\n        ymax=mean+se),  \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", # important to indicate stat=“identity”.\n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\n3.3.3 Plotting confidence interval of point estimates\nWe can also plot the confidence intervals of mean maths score by race. Note how the error bars are sorted by using the average maths scores.\n\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, # The confidence intervals are computed by using the formula mean+/-1.96*se.\n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"black\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\n3.3.4 Plotting interactive error bars of point estimates\n\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"black\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#visualizing-uncertainty-ggdist",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#visualizing-uncertainty-ggdist",
    "title": "Hands-on Exercise 4",
    "section": "3.4 Visualizing Uncertainty: ggdist",
    "text": "3.4 Visualizing Uncertainty: ggdist\nggdist is designed for both frequentist and Bayesian uncertainty visualization, taking the view that uncertainty visualization can be unified through the perspective of distribution visualization:\n\nfor frequentist models, one visualises confidence distributions or bootstrap distributions (see vignette(“freq-uncertainty-vis”));\nfor Bayesian models, one visualises probability distributions (see the tidybayes package, which builds on top of ggdist).\n\n\n3.4.1 Visualizing the uncertainty of point estimates\nstat_pointinterval() is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nWe can change the arguments in stat_pointinterval() to visualize intervals that cover 95% of the distribution.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95, #  the interval should cover 95% of the distribution, which is equivalent to a 95% confidence interval in frequentist terms\n  point_interval = median_qi) + #  point estimate to be plotted should be the median of the distribution, and qi stands for quantile interval\n  labs(\n    title = \"Visualising confidence intervals of median math score - 95%\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\nWe can also visualize both the 95% and 99% of the distributions:\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = c(0.95, 0.99),  # 95% confidence interval\n                     point_interval = median_qi,\n                     show.legend = c(size = FALSE),\n                     geom = \"pointinterval\",\n                     position = \"dodge\") +\n\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point with 95% and 99% Confidence Intervals\")\n\n\n\n\n\n\n\n\n\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,  # 95% confidence interval\n                     point_interval = median_qi,\n                     color = \"blue\") +  # Differentiate the intervals by color\n  stat_pointinterval(.width = 0.99,  # 99% confidence interval\n                     point_interval = median_qi,\n                     color = \"red\") +   #  Differentiate the intervals by color\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point with 95% and 99% Confidence Intervals\",\n    caption = \"Blue: 95% CI, Red: 99% CI\") \n\n\n\n\n\n\n\n\nIn the code chunk below, stat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"tomato\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#visualising-uncertainty-with-hypothetical-outcome-plots-hops",
    "title": "Hands-on Exercise 4",
    "section": "3.5 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)",
    "text": "3.5 Visualising Uncertainty with Hypothetical Outcome Plots (HOPs)\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    linewidth = 0.4, color = \"darkolivegreen\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), color = \"lightpink\") +\n  theme_bw() + \n  transition_states(.draw, 1, 3)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#overview-3",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#overview-3",
    "title": "Hands-on Exercise 4",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nFunnel plot is a specially designed data visualisation for conducting unbiased comparison between outlets, stores or business entities. In this exercise, I will gain experience on:\n\nplotting funnel plots by using funnelPlotR package,\nplotting static funnel plot by using ggplot2 package, and\nplotting interactive funnel plot by using both plotly R and ggplot2 packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#getting-started-3",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#getting-started-3",
    "title": "Hands-on Exercise 4",
    "section": "4.2 Getting started",
    "text": "4.2 Getting started\n\n4.2.1 Loading libraries\n\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n4.2.2 Importing Data\nIn this section, COVID-19_DKI_Jakarta will be used. The data was downloaded from Open Data Covid-19 Provinsi DKI Jakarta portal. For this hands-on exercise, we are going to compare the cumulative COVID-19 cases and death by sub-district (i.e. kelurahan) as at 31st July 2021, DKI Jakarta.\nThe code chunk below imports the data into R and save it into a tibble data frame object called covid19.\n\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\n\n\nkable(head(covid19), format = 'html')\n\n\n\n\nSub-district ID\nCity\nDistrict\nSub-district\nPositive\nRecovered\nDeath\n\n\n\n\n3172051003\nJAKARTA UTARA\nPADEMANGAN\nANCOL\n1776\n1691\n26\n\n\n3173041007\nJAKARTA BARAT\nTAMBORA\nANGKE\n1783\n1720\n29\n\n\n3175041005\nJAKARTA TIMUR\nKRAMAT JATI\nBALE KAMBANG\n2049\n1964\n31\n\n\n3175031003\nJAKARTA TIMUR\nJATINEGARA\nBALI MESTER\n827\n797\n13\n\n\n3175101006\nJAKARTA TIMUR\nCIPAYUNG\nBAMBU APUS\n2866\n2792\n27\n\n\n3174031002\nJAKARTA SELATAN\nMAMPANG PRAPATAN\nBANGKA\n1828\n1757\n26"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#funnelplotr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#funnelplotr-methods",
    "title": "Hands-on Exercise 4",
    "section": "4.3 FunnelPlotR methods",
    "text": "4.3 FunnelPlotR methods\nFunnelPlotR requires a numerator (events of interest), denominator (population to be considered) and group. The key arguments selected for customisation are:\n\nlimit: plot limits (95 or 99).\nlabel_outliers: to label outliers (true or false).\nPoisson_limits: to add Poisson limits to the plot.\nOD_adjust: to add overdispersed limits to the plot.\nxrange and yrange: to specify the range to display for axes, acts like a zoom function.\nOther aesthetic components such as graph title, axis labels etc.\n\n\n4.3.1 Version 1\n\nfunnel_plot(\n  numerator = covid19$Death, # events of interest\n  denominator = covid19$Positive, # population to be considered\n  group = covid19$`Sub-district`,\n  title = \"Cumulative COVID-19 cases and death by sub-district\"\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 1 are outliers. \nPlot is adjusted for overdispersion. \n\n\nThe code chunk above plots an object that does not resemble a funnel. We will try tweaking the input parameters.\n\ndata_type: the default is set to “SR”, indirectly standardised ratios. We will try to use “PR” for proportions.\n\n\n\n4.3.2 Version 2\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",     # Use PR for proportions\n  x_range = c(0, 6500),  # Specify x axis range\n  y_range = c(0, 0.05),   # Specify x axis range\n  title = \"Cumulative COVID-19 cases and death by sub-district\"\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n4.3.3 Version 3\n\nfunnel_plot(\n  numerator = covid19$Death,\n  denominator = covid19$Positive,\n  group = covid19$`Sub-district`,\n  data_type = \"PR\",   \n  x_range = c(0, 6500),  \n  y_range = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;          \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\nlabel = NA argument is to removed the default label outliers feature.\ntitle argument is used to add plot title.\nx_label and y_label arguments are used to add/edit x-axis and y-axis titles."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex4/Hands-on_Ex4.html#funnel-plot-for-fair-visual-comparison-ggplot2-methods",
    "title": "Hands-on Exercise 4",
    "section": "4.4 Funnel Plot for Fair Visual Comparison: ggplot2 methods",
    "text": "4.4 Funnel Plot for Fair Visual Comparison: ggplot2 methods\nHere we build funnel plots step-by-step with ggplot2. We first compute the baseic statistical details.\n\n\n4.4.1 Computing the basic derived fields\nWe first compute the cumulative death rate, standard error, and weighted average of the death rate across groups in the covid19 dataframe.\n\n\n\n\n\n\nNote\n\n\n\nweighted.mean() function is used to calculate a mean death rate, weighted by the inverse of the square of the standard error (1/df$rate.se^2). This weighting gives more influence to rates with smaller standard errors, which are considered more reliable.\n\n\n\n#  Derive cumulative death rate and standard error of cumulative death rate\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\n# Compute weighted average of the death rate \nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n4.4.2 Calculate lower and upper limits for 95% and 99.9% CI\n\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, \n                   number.ll999, number.ul999, \n                   number.seq, fit.mean)\n\n\n\n4.4.3 Plot static funnel plot\n\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(alpha=0.4) +\n  geom_text(aes(label=`Sub-district`), nudge_y = 0.001, check_overlap = TRUE) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n\n\n\n4.4.4 Interactive Funnel Plot: plotly + ggplot2\nThe funnel plot created using ggplot2 functions can be made interactive with ggplotly() of plotly r package.\n\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "In this exercises I will practice using ggplot2 to create elegant and effective statistical graphics, achieving the following:\n\ncontrol the placement of annotation on a graph by using functions provided in ggrepel package,\ncreate professional publication quality figure by using functions provided in ggthemes and hrbrthemes packages,\nplot composite figure by combining ggplot2 graphs by using patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#overview",
    "title": "Hands-on Exercise 2",
    "section": "",
    "text": "In this exercises I will practice using ggplot2 to create elegant and effective statistical graphics, achieving the following:\n\ncontrol the placement of annotation on a graph by using functions provided in ggrepel package,\ncreate professional publication quality figure by using functions provided in ggthemes and hrbrthemes packages,\nplot composite figure by combining ggplot2 graphs by using patchwork package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#getting-started",
    "title": "Hands-on Exercise 2",
    "section": "2 Getting started",
    "text": "2 Getting started\n\n2.1 Loading libraries\nThe following libraries will be used\n\nggrepel: an R package provides geoms for ggplot2 to repel overlapping text labels.\nggthemes: an R package provides some extra themes, geoms, and scales for ‘ggplot2’.\nhrbrthemes: an R package provides typography-centric themes and theme components for ggplot2.\npatchwork: an R package for preparing composite figure created using ggplot2.\n\n\npacman::p_load(ggrepel, patchwork, \n               ggthemes, hrbrthemes,\n               tidyverse)\n\n\n\n2.2 Importing data\nThe data used is the year end examination grades of a cohort of primary 3 students from a local school. From Hands-On Exercise 1, we know that the data has:\n\nFour categorical attributes: ID, CLASS, GENDER, RACE\nThree continuous attributes: MATHS, ENGLISH, SCIENCE\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#annotation-of-plots",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#annotation-of-plots",
    "title": "Hands-on Exercise 2",
    "section": "3 Annotation of plots",
    "text": "3 Annotation of plots\n\n3.1 Using Text geoms in ggplot2\nAnnotation of statistical graphs can be done by using text geoms like geom_text() or geom_label() (has rectangle behind the text) as shown below.\n\nPlot with geom_text()Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= ENGLISH, \n           y=SCIENCE)) +\n  geom_point() +\n  geom_text(mapping = aes(label = ID)) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"Science scores versus English scores\")\n\n\n\n\n\nPlot with geom_label()Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= ENGLISH, \n           y=SCIENCE)) +\n  geom_point() +\n  geom_label(mapping = aes(label = ID)) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"Science scores versus English scores\")\n\n\n\n\n\n\n3.2 Using ggrepel\nggrepel helps to repel overlapping text. Simply replace geom_text() by geom_text_repel() and geom_label() by geom_label_repel().\n\nPlot with geom_text_repel()Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= ENGLISH, \n           y=SCIENCE)) +\n  geom_point() +\n  geom_text_repel(mapping = aes(label = ID)) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"Science scores versus English scores\")\n\n\n\n\n\nPlot with geom_label_repel()Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data, \n       aes(x= ENGLISH, \n           y=SCIENCE)) +\n  geom_point() +\n  geom_label_repel(mapping = aes(label = ID),\n                   max.overlaps = getOption(\"ggrepel.max.overlaps\", default = 20)) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"Science scores versus English scores\")\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote how when there are too many data points (320 in this case), the labels that overlap cannot be labeled. Consider varying the number of overlaps allowed in the option for max.overlaps. The default value is 10."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#themes",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#themes",
    "title": "Hands-on Exercise 2",
    "section": "4 Themes",
    "text": "4 Themes\n\n4.1 ggplot2 themes\nAs explored in Hands-on Exercise 1, there are several themes in ggplot2. Below is an example.\n\nPlotThe code\n\n\nUsing theme_bw() theme.\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data) +\n  geom_point(aes(x = ENGLISH, y = SCIENCE, colour = RACE)) +\n  theme_bw() +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"Science scores versus English scores, with race perspective\")\n\n\n\n\n\n\n4.2 Using ggthemes\nThe list of themes available from the ggthemes() package can be found at this link.\n\nPlotThe code\n\n\nUsing theme_fivethirtyeight().\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data) +\n  geom_point(aes(x = ENGLISH, y = SCIENCE, colour = RACE)) +\n  theme_economist() +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"Science scores versus English scores, with race perspective\")\n\n\n\n\n\n\n4.3 Using hrbthemes\nWith this package we can edit typographic elements such as placement of labels or fonts used.\n\nUsing hrbthemesThe code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data) +\n  geom_point(aes(x = ENGLISH, y = SCIENCE, colour = RACE)) +\n  theme_ipsum_rc(axis_title_size = 12,\n                 plot_title_size = 14,\n              base_size = 10,\n              grid = \"XY\") +  # gridlines for X and Y axis\n  ggtitle(\"Science scores versus English scores, with race perspective\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#beyond-single-graph",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#beyond-single-graph",
    "title": "Hands-on Exercise 2",
    "section": "5 Beyond Single Graph",
    "text": "5 Beyond Single Graph\n\n5.1 Create individual plots\nWe will create the following plots:\n\nDistribution of Science scores\nDistribution of English scores\nScatterplot for Science score versus English score\n\n\nPlot 1Plot 2Plot 3\n\n\n\np1 &lt;- ggplot(data=exam_data, \n             aes(x = SCIENCE)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Science scores\")\n\np1\n\n\n\n\n\n\n\n\n\n\n\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\np2\n\n\n\n\n\n\n\n\n\n\n\np3 &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH, y = SCIENCE)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"Science scores versus English scores\")\n\np3\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2 Creating Composite Graphics: pathwork methods\nPatchwork package has a very simple syntax where we can create layouts super easily. Here’s the general syntax that combines:\n\nTwo-Column Layout using the Plus Sign +.\nParenthesis () to create a subplot group.\nTwo-Row Layout using the Division Sign /\n\n\nCombining two plotsCombining three plotsAdding tagUse inset_element()Adding ggthemes\n\n\n\np1+p2\n\n\n\n\n\n\n\n\n\n\n\n(p1 / p2) | p3\n\n\n\n\n\n\n\n\n\n\nObserve that the plots are now numbered by Roman numerals:\n\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\n\n\n\nTo place plots freely on top or below of other plots.\n\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n\n\n\n\n\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex2/Hands-on_Ex2.html#references",
    "title": "Hands-on Exercise 2",
    "section": "6 References",
    "text": "6 References\n\nR for Visual Analytics: Beyond ggplot2 Fundamentals."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "This exercise introduces the basic principles and essential components of ggplot2 package, which is based on the Layered Grammar of Graphics. Here, we present some ways that this package can be used to plot a variation of elegant and functional statistical graphics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#learning-outcome",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#learning-outcome",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "This exercise introduces the basic principles and essential components of ggplot2 package, which is based on the Layered Grammar of Graphics. Here, we present some ways that this package can be used to plot a variation of elegant and functional statistical graphics."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "title": "Hands-on Exercise 1",
    "section": "2 Getting started",
    "text": "2 Getting started\n\n2.1 Loading libraries\nThe required R package for this exercise is tidyverse.\n\npacman::p_load(tidyverse)\n\n\n\n2.2 Importing data\nThe data used is the year end examination grades of a cohort of primary 3 students from a local school.\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\nA brief exploration of the data shows that:\n\nUsing dim, we see that the data has 322 rows (entries) and 7 columns (attributes).\nUsing str, we see which attributes are categorical (of class chr) and which are continuous (of class num).\n\n\ndim(exam_data)\n\n[1] 322   7\n\nstr(exam_data)\n\nspc_tbl_ [322 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ID     : chr [1:322] \"Student321\" \"Student305\" \"Student289\" \"Student227\" ...\n $ CLASS  : chr [1:322] \"3I\" \"3I\" \"3H\" \"3F\" ...\n $ GENDER : chr [1:322] \"Male\" \"Female\" \"Male\" \"Male\" ...\n $ RACE   : chr [1:322] \"Malay\" \"Malay\" \"Chinese\" \"Chinese\" ...\n $ ENGLISH: num [1:322] 21 24 26 27 27 31 31 31 33 34 ...\n $ MATHS  : num [1:322] 9 22 16 77 11 16 21 18 19 49 ...\n $ SCIENCE: num [1:322] 15 16 16 31 25 16 25 27 15 37 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ID = col_character(),\n  ..   CLASS = col_character(),\n  ..   GENDER = col_character(),\n  ..   RACE = col_character(),\n  ..   ENGLISH = col_double(),\n  ..   MATHS = col_double(),\n  ..   SCIENCE = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt;"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#introducing-ggplot",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#introducing-ggplot",
    "title": "Hands-on Exercise 1",
    "section": "3 Introducing ggplot",
    "text": "3 Introducing ggplot\nThis package is part of the tidyverse family, specifically used for visual exploration.\nFor more detail, visit ggplot2 link.\nBelow, we present a quick comparison between a boxplot plotted with R Graphics and with ggplot. Charts customized by referring to https://r-charts.com/.\n\nR Graphicsggplot2\n\n\n\nboxplot(exam_data$ENGLISH, \n        main = \"Distribution of English scores\",\n        xlab = \"ENGLISH\",\n        ylab = \"Score\",\n        col = \"#30638E\")\n\n\n\n\n\n\n\n\n\n\n\nggplot(exam_data, aes(x=\"\", y = ENGLISH)) + \n  stat_boxplot(geom = \"errorbar\",\n               width = 0.15) + \n  geom_boxplot(fill=\"#30638E\",\n               color = \"black\") +\n  ggtitle(\"Distribution of English scores\") +\n  theme(panel.background = element_rect(fill = \"grey\")) +\n  labs(x = \"ENGLISH\", y = \"Score\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#grammar-of-graphics",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#grammar-of-graphics",
    "title": "Hands-on Exercise 1",
    "section": "4 Grammar of Graphics",
    "text": "4 Grammar of Graphics\nggplot is an implementation of Leland Wilkinson’s Grammar of Graphics.\nThere are two principles in Grammar of Graphics:\n\nGraphics = distinct layers of grammatical elements\nMeaningful plots through aesthetic mapping\n\nThe table below describes the seven grammars of ggplot2.\n\n\n\n\n\n\n\nBuilding Block\nDescription\n\n\n\n\nData\nThe dataset being plotted.\n\n\nAesthetics\nMap data attributes to visual properties like position, color, size, shape, and transparency\n\n\nGeometric\nVisual elements such as point / bar / line\n\n\nFacets\nDivide data into subsets for parallel displays of graphs (also known as paneling)\n\n\nStatistics\nStatistical summaries such as averages or confidence intervals\n\n\nCoordinate Systems\nPlane of data mapped on the graphic\n\n\nThemes\nCustomize non-data plot elements like titles, axis labels, and legend aesthetics"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#essential-grammatical-elements-in-ggplot2",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#essential-grammatical-elements-in-ggplot2",
    "title": "Hands-on Exercise 1",
    "section": "5 Essential Grammatical Elements in ggplot2",
    "text": "5 Essential Grammatical Elements in ggplot2\nIn this section, we explore all of the grammatical elements.\n\n5.1 Data and Aesthetics\nLearning points:\n\nggplot() initializes a ggplot object.\nThe data argument defines the dataset to be used for plotting.\nThe aes() function is used to specify all aesthetics of the plot\n\nThe aesthetic mappings take attributes of the data and and use them to influence visual characteristics, such as position, colour, size, shape, or transparency. Each visual characteristic can thus encode an aspect of the data and be used to convey information.\n\n\n\nggplot(data=exam_data, aes(x=\"ENGLISH\"))\n\n\n\n\n\n\n\n\n\n5.1.1 Defining aes in ggplot() or in geom objects\nWhile we can define aes within ggplot()() function, each geom layer can also have its own aes specification.\nIn this code chunk, we plot two layers geom_smooth() and geom_point.The aesthetics are defined globally in the ggplot() function. The geom_point() layer will inherit these aesthetics automatically.\n\n# ggplot(data=exam_data, aes(x = ENGLISH, y = SCIENCE)) +\n#   geom_point() +\n#   geom_smooth()\n\nggplot(data=exam_data, aes(x = ENGLISH, y = SCIENCE)) +\n  geom_smooth() +\n  geom_point()\n\n\n\n\n\n\n\n\nIn this code chunk, the aesthetics are defined within geom_smooth(). The geom_point() layer will not inherit these aesthetics and will need them specified separately.\n\nggplot(data=exam_data) +\n  geom_smooth(aes(x = ENGLISH, y = SCIENCE),\n              method = \"lm\") +\n  geom_point(aes(x = ENGLISH, y = SCIENCE, colour = RACE))\n\n\n\n\n\n\n\n\n\n\n\n5.2 Geometric Objects\nLearning points:\n\nGeometric objects are the actual marks we put on a plot.\nA plot must have at least one geom. Add a geom using + operator.\n\nThe image shows some examples of geometric objects that we can put on a plot.\n\nOther examples include geom_smooth (for drawing smoothed lines), geom_bar (for drawing bars), geom_polygon (for drawing arbitrary shapes), geom_map (for drawing polygons in the shape of a map).\nFor complete list, please refer to here.\n\n5.2.1 geom_bar()\ngeom_bar() can be used to plot bar charts.\nBy default, this function counts the number of occurrences for each level of a categorical variable. The code chunk below demonstrates how the function counts the occurrences of each level (Male/Female) of the categorical variable GENDER.\n\nggplot(data=exam_data, \n       aes(x=GENDER)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nIf your data already contains the count for each group, you will need to pass stat = \"identity\" inside geom_bar. The code chunk below demonstrates this using a different dataframe that already contains the counts of males and females in a cohort.\n\ngender_data &lt;- data.frame(GENDER = c(\"Male\", \"Female\"), count = c(152, 170))\n\ngender_data\n\n  GENDER count\n1   Male   152\n2 Female   170\n\nggplot(gender_data, aes(x = GENDER, y = count)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\n\n5.2.2 geom_dotplot()\ngeom_dotplot can be used to plot dot plots.\nThe width of a dot corresponds to the bin width (or maximum width). The dots are stacked, with each dot representing one observation.\nHowever, the y scale can be misleading due to technical limitations of ggplot2.\n\nscale_y_continuous() can be used to turn off the y-axis\nbinwidth argument can be used to change the binwidth\n\nThe dotplot below illustrates the distribution of scores for ENGLISH across the cohort. Each dot represents a student’s score, while the X axis denotes the range of scores achieved.\n\nggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot(dotsize = 0.5)\n\n\n\n\n\n\n\n\nHere we remove the misleading y-axis and change the binwidth to 2.5.\n\nggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot(binwidth=2.5,\n               dotsize = 0.5) +      \n  scale_y_continuous(NULL,           \n                     breaks = NULL)\n\n\n\n\n\n\n\n\n\n\n5.2.3 geom_histogram()\ngeom_histogram() creates histograms.\nIn this section, we also demonstrate how a geometric object can be modified by changing geom() and aes().\n\nSimple HistogramChanging geom()Changing aes()\n\n\nWe first plot a simple histogram of the ENGLISH scores.\n\nggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_histogram() \n\n\n\n\n\n\n\n\n\n\nIn this code chunk, we change geom() in the following ways:\n\nbins argument is used to change the number of bins to 15\nfill argument is used to shade the histogram\ncolor argument is used to change the outline colour of the bars in black\n\n\nggplot(data=exam_data, \n       aes(x= ENGLISH)) +\n  geom_histogram(bins=15, \n                 fill=\"#30638E\",\n                 color=\"black\")  \n\n\n\n\n\n\n\n\n\n\nWe can also visualize the distribution based on the categories of GENDER. Using aes(), we add the GENDER variable to the fill argument.\n\nggplot(data=exam_data, \n       aes(x= ENGLISH, \n           fill = GENDER)) +\n  geom_histogram(bins=15,\n                 color =\"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2.4 geom_density()\ngeom-density()computes and plots kernel density estimate. It is an alternative to the histogram, for continuous data that comes from an underlying smooth distribution.\n\nKernel density estimateChanging aes()\n\n\n\nggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_density()           \n\n\n\n\n\n\n\n\n\n\nWe can change aes() by adding the GENDER variable to the colour argument. The density plot will display different colors for the density lines of male and female students.\n\nggplot(data=exam_data, \n       aes(x = ENGLISH, \n           colour = GENDER)) +\n  geom_density()\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2.5 geom_boxplot\ngeom_boxplotdisplays continuous value list. It visualises five summary statistics (the median, two hinges and two whiskers), and all “outlying” points individually.\n\nBox PlotBox Plot with Notches\n\n\nThe code chunk plots a boxplot that compares the distribution of ENGLISH scores across different categories of GENDER.\n\nggplot(data=exam_data, \n       aes(y = ENGLISH,       \n           x= GENDER)) +    \n  geom_boxplot()            \n\n\n\n\n\n\n\n\n\n\nNotches help visually assess whether the medians of distributions differ.\nIf the notches do not overlap, this is evidence that the medians are different.\n\nggplot(data=exam_data, \n       aes(y = ENGLISH, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2.6 geom_violin\ngeom_violin is used to create violin plots that compare multiple data distributions side by side.\nThe code chunk below plots the distribution of ENGLISH score by GENDER in violin plot.\n\nggplot(data=exam_data, \n       aes(y = ENGLISH, \n           x= GENDER)) +\n  geom_violin()\n\n\n\n\n\n\n\n\n\n\n5.2.7 geom_point()\ngeom_point() is useful in creating scatter plots.\nThe scatterplot below shows the ENGLISH and SCIENCE scores of the cohort.\n\nggplot(data=exam_data, \n       aes(x= ENGLISH, \n           y=SCIENCE)) +\n  geom_point()            \n\n\n\n\n\n\n\n\n\n\n5.2.8 xgeom objects can be combined\nThe code chunk below plots the data points on the violin plots by using both geom_violin() and geom_point().\n\nggplot(data=exam_data, \n       aes(y = ENGLISH, \n           x= GENDER)) +\n  geom_violin() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)          \n\n\n\n\n\n\n\n\n\n\n\n5.3 Statistics\nLearning points:\n\nBy adding stat_() functions, we can statistically transform data, usually as some form of summary. This overrides the default geom.\nBy adding geom_() functions, we can override the default stat.\n\n\nBox Plot without MeanWorking with stat - the stat_summary() methodWorking with stat - the geom() method\n\n\nThe boxplots below are incomplete because the positions of the means were not shown.\n\nggplot(data=exam_data, \n       aes(y = ENGLISH, x= GENDER)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nThe code chunk below adds mean values by using stat_summary().\n\nggplot(data=exam_data, \n       aes(y = ENGLISH, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun.y=\"mean\",         \n               colour =\"red\",        \n               size=4)               \n\n\n\n\n\n\n\n\n\n\nThe code chunk below adding mean values by using geom_() function and overriding the default stat.\n\nggplot(data=exam_data, \n       aes(y = ENGLISH, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun.y=\"mean\",           \n             colour =\"red\",          \n             size=4)          \n\n\n\n\n\n\n\n\n\n\n\n\n5.3.1 Adding a best fit curve on a scatterplot\nWe can improve the interpretability of a scatterplot graph by adding a best fit curve.\nThis can be done using geom_smooth().\n\nThe default smoothing method used is loess.\nThis can be overridden by adding the method argument. See here for details.\n\n\nggplot(data=exam_data, \n       aes(x= ENGLISH, \n           y=SCIENCE)) +\n  geom_point() +\n  geom_smooth(linewidth=0.5)\n\n\n\n\n\n\n\n\n\n\n\n5.4 Facets\nLearning points:\n\nFaceting creates a series of small, individual plots, with each representing a specific subset of the data. This technique serves as alternative to using aesthetics when visualizing additional discrete variables.\nggplot2 supports two types of factes, namely: facet_wrap and facet_grid().\n\n\n5.4.1 facet_wrap()\nfacet_wrap wraps a 1d sequence of panels into 2d. This is generally a better use of screen space than facet_grid because most displays are roughly rectangular.\n\nggplot(data=exam_data, \n       aes(x= ENGLISH)) +\n  geom_histogram(bins=15) +\n    facet_wrap(~ CLASS)\n\n\n\n\n\n\n\n\n\n\n5.4.2 facet_grid()\nfacet_grid() forms a matrix of panels defined by row and column facetting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data.\n\nggplot(data=exam_data, \n       aes(x= ENGLISH)) +\n  geom_histogram(bins=15) +\n    facet_grid(~ CLASS)\n\n\n\n\n\n\n\n\n\n\n\n5.5 Coordinates\nThe Coordinates functions map the position of objects onto the plane of the plot.\nThere are a number of different possible coordinate systems to use, they are:\n\ncoord_cartesian(): the default cartesian coordinate systems, where you specify x and y values (e.g. allows you to zoom in or out).\ncoord_flip(): a cartesian system with the x and y flipped.\ncoord_fixed(): a cartesian system with a “fixed” aspect ratio (e.g. 1.78 for a “widescreen” plot).\ncoord_quickmap(): a coordinate system that approximates a good aspect ratio for maps.\n\n\n5.5.1 coord_flip()\nBy the default, the bar chart of ggplot2 is in vertical form.\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nWe can flip the horizontal bar chart into vertical bar chart by using coord_flip().\n\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n5.5.2 coord_cartesian()\nThe scatterplot below is slightly misleading because the y-axis and x-axis ranges are not equal.\n\nggplot(data=exam_data, \n       aes(x= ENGLISH, y=SCIENCE)) +\n  geom_point() +\n  geom_smooth(method=lm, linewidth=0.5)\n\n\n\n\n\n\n\n\nWe can use coord_cartesian() to fix the range of both the y-axis and x-axis, to range from 0-100.\n\nggplot(data=exam_data, \n       aes(x= ENGLISH, y=SCIENCE)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\n\n\n\n\n\n\n\n\n\n\n5.6 Themes\nLearning Points:\n\nThemes control elements of the graph not related to the data. For example:\n\nbackground colour\nsize of fonts\ngridlines\ncolour of labels\n\nEach theme element can be conceived of as either a line (e.g. x-axis), a rectangle (e.g. graph background), or text (e.g. axis title).\n\nRefer to this link for a list of themes.\n\ntheme_gray()theme_bw()theme_void()\n\n\nThis is also the default theme.\n\nggplot(data=exam_data) +\n  geom_point(aes(x = ENGLISH, y = SCIENCE, colour = RACE)) +\n  theme_gray()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data,\n       aes(x = ENGLISH, y = SCIENCE, colour = RACE)) +\n  geom_point() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=exam_data,\n       aes(x = ENGLISH, y = SCIENCE, colour = RACE)) +\n  geom_point() +\n  theme_void()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex1/Hands-on_Ex1.html#references",
    "title": "Hands-on Exercise 1",
    "section": "6 References",
    "text": "6 References\n\nR for Visual Analytics: A Layered Grammar of Graphics: ggplot2 methods.\nggplot2 - Tidyverse\nR CHARTS"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html",
    "title": "Hands-on Exercise 3",
    "section": "",
    "text": "In this exercises I will learn how to create interactive data visualisation by using functions provided by ggiraph and plotlyr packages.\n\n\n\n\n\nThe following libraries will be used:\n\nggiraph for making ‘ggplot’ graphics interactive.\nplotly, R library for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork for combining multiple ggplot2 graphs into one figure.\n\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse) \n\n\n\n\nThe data used is the year end examination grades of a cohort of primary 3 students from a local school. From Hands-On Exercise 1, we know that the data has:\n\nFour categorical attributes: ID, CLASS, GENDER, RACE\nThree continuous attributes: MATHS, ENGLISH, SCIENCE\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")\n\n\n\n\n\nggiraph is a html widget and ggplot2 extension, that allows ggplot graphics to be interactive:\nWe can makes ggplot graphics interactive with the following three arguments:\n\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements.\nData_id: a column of data-sets that contain an id to be associated with elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\n\n\n\nLet’s first demonstrate how we can make some graphs interactive with a tooltip.\n\n\nThree steps to plotting a scatter plot with tooltip that shows studeNT ID:\n\nUse geom_point_interactive() instead of geom_point()\nProvide the aesthetic tooltip with ID\nCall function girafe with the ggplot object so that the graphic is translated as a web interactive graphic\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Create ggplot object\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS, y = SCIENCE)) +\n  geom_point_interactive(\n    aes(tooltip = ID)) +  # Provide tooltip aesthetic\n    scale_x_continuous(limits = c(0, NA)) + \n  scale_y_continuous(limits = c(0, NA)) +\n  labs(title = \"Positive relationship betweeen Science and Maths scores\",\n       subtitle = \"Scatter plot of science and math scores of students\",\n       y = \"Science\\n Scores\",\n       x = \"Math Scores\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        axis.title.y = element_text(size = 8, angle = 360, vjust = 0.5),\n        axis.title.x = element_text(size = 8))\n\n# Pass ggplot object into girafe\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\n\n\nThe content of the tooltip can be customised by including a list object, that is passed into the aesthetic for tooltip.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Create new column for tooltip information \nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS,\n  \"\\n Race = \", exam_data$RACE)) \n\n# Create ggplot object\np &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), # Pass in list object to tooltip aesthetic\n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL) +\n    labs(title = \"Distribution of Science Scores\",\n       subtitle = \"Dotplot\",\n       x = \"Science Scores\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8))\n\n# Pass ggplot object into girafe\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\n\n\nWe can use opts_tooltip() to further customize the tooltip, such as changing opacity of background, or adding css declarations.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Create new column for tooltip information\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS,\n  \"\\n Race = \", exam_data$RACE)) \n\n# Create ggplot object\np &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL) +\n    labs(title = \"Distribution of Science Scores\",\n       subtitle = \"Dotplot\",\n       x = \"Science Scores\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8))\n\n# Create object for tooltip styling\ntooltip_css &lt;- \"background-color:#93c7c2; #&lt;&lt;\nfont-style:bold; color:#464646;\" #&lt;&lt;\n\n# Pass ggplot object and tooltip styling into girafe\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)   \n\n\n\n\n\n\n\nWe can also display derived statistics.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Create object for tooltip information  \ntooltip &lt;- function(ymin, lower, middle, upper, ymax, accuracy = .01) {\n  sem &lt;- scales::number((ymax - ymin) / 2, accuracy = accuracy) # SEM for 90% CI\n  median &lt;- scales::number(middle, accuracy = accuracy)\n  lower_quartile &lt;- scales::number(lower, accuracy = accuracy)\n  upper_quartile &lt;- scales::number(upper, accuracy = accuracy)\n  \n  paste(\"Median:\", median, \n        \"\\n25th Quartile:\", lower_quartile, \n        \"\\n75th Quartile:\", upper_quartile,\n        \"\\n95% CI: [\", scales::number(ymin, accuracy = accuracy), \n        \", \", scales::number(ymax, accuracy = accuracy), \"]\")\n}\n\n# Create ggplot object\ngg_boxplot &lt;- ggplot(data = exam_data, aes(x = GENDER, y = ENGLISH)) +\n  geom_boxplot_interactive(\n    aes(tooltip = after_stat(tooltip(ymin, lower, middle, upper, ymax))), # Pass in after_stat of tooltip information \n    stat = \"boxplot\"\n  ) +\n  labs(title = \"Boxplot of English Scores by Gender\",\n       x = \"Gender\",\n       y = \"English Scores\")\n\n# Pass in ggobject into girafe\ngirafe(ggobj = gg_boxplot,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\nIn the example below, elements associated with a data_id (i.e GENDER) will be highlighted upon mouse over.\nWe also demonstrate how css codes are used to customize the highlighting effect.\nUpdated on 3 Feb 2024: Added tooltip for data_id aesthetic\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Plot ggplot object\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS, y = SCIENCE)) +\n  \n  geom_point_interactive(\n    aes(tooltip = GENDER, # Update on 03 Feb: Added in tooltip\n        data_id = GENDER)) + # Pass in data_id\n  scale_x_continuous(limits = c(0, NA)) + \n  scale_y_continuous(limits = c(0, NA)) +\n  labs(title = \"Positive Relationship between Science and Maths Scores\",\n       subtitle = \"Scatter Plot of Science and Math Scores of Students\",\n       y = \"Science\\n Scores\",\n       x = \"Math Scores\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        axis.title.y = element_text(size = 8, angle = 360, vjust = 0.5),\n        axis.title.x = element_text(size = 8))\n\n# Pass ggoplot object into girafe\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618,\n  options = list(\n    opts_hover(css = \"fill: #f5bc5f;\"), \n    opts_hover_inv(css = \"opacity:0.2;\")\n  )\n)\n\n\n\n\n\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Plot ggplot object\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS, y = SCIENCE)) +\n  geom_point_interactive(\n    aes(tooltip = GENDER, # Pass in tooltip\n        data_id = GENDER)) + # Pass in data_id\n  scale_x_continuous(limits = c(0, NA)) + \n  scale_y_continuous(limits = c(0, NA)) +\n  labs(title = \"Positive Relationship between Science and Maths Scores\",\n       subtitle = \"Scatter Plot of Science and Math Scores of Students\",\n       y = \"Science\\n Scores\",\n       x = \"Math Scores\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        axis.title.y = element_text(size = 8, angle = 360, vjust = 0.5),\n        axis.title.x = element_text(size = 8))\n\n# Create object for tooltip styling\ntooltip_css &lt;- \"background-color:#f53b5d; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\n# Pass ggoplot object into girafe\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618,\n  options = list(\n    opts_hover(css = \"fill: #f5bc5f;\"), \n    opts_hover_inv(css = \"opacity:0.2;\"),\n    opts_tooltip(css = tooltip_css)\n  )\n)\n\n\n\n\n\n\n\n\nonclick argument of ggiraph provides hotlink interactivity on the web.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick, # Pass in onclick aesthetic\n        data_id = ID),   # Pass in data_id to show hover effect before clicking in           \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618) \n\n\n\n\n\n\n\nWe can use patchwork function to create interactive coordinated multiple views, that are linked by student ID.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Update on 03 Feb 2024: Add in tooltip\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Math Score = \", exam_data$MATHS,\n  \"\\n Science Score = \", exam_data$SCIENCE,\n  \"\\n English Score = \", exam_data$ENGLISH)) \n\n# Plot first ggplot object\np1 &lt;- ggplot(data=exam_data, \n            aes(x = MATHS, y = SCIENCE)) +\n  geom_point_interactive(\n    aes(tooltip = exam_data$tooltip, \n        data_id = ID)) + # Pass in data_id\n  scale_x_continuous(limits = c(0, NA)) + \n  scale_y_continuous(limits = c(0, NA)) +\n  labs(y = \"Science\\n Scores\",\n       x = \"Math Scores\") +\n  theme(axis.title.y = element_text(size = 8, angle = 360, vjust = 0.5),\n        axis.title.x = element_text(size = 8))\n\n# Plot second ggplot object\np2 &lt;- ggplot(data=exam_data, \n            aes(x = MATHS, y = ENGLISH)) +\n  geom_point_interactive(\n    aes(tooltip = exam_data$tooltip,\n        data_id = ID)) + # Pass in data_id\n  scale_x_continuous(limits = c(0, NA)) + \n  scale_y_continuous(limits = c(0, NA)) +\n  labs(y = \"English\\n Scores\",\n       x = \"Math Scores\") +\n  theme(axis.title.y = element_text(size = 8, angle = 360, vjust = 0.5),\n        axis.title.x = element_text(size = 8))\n\n# Pass both ggplot objects into girafe, using patchwork\ngirafe(code = print(p1 + p2), # Take note that print is required\n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(\n         opts_hover(css = \"fill: #f5bc5f;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\n\n\n\nThere are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()\n\n\n\nBelow we explore a basic way to plot an interactive plot.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~SCIENCE, \n        y = ~MATHS,\n        type = 'scatter',\n        mode = 'markers') \n\n\n\n\n\n\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~SCIENCE, \n        y = ~MATHS, \n        color = ~RACE,\n        type = 'scatter',\n        mode = 'markers')\n\n\n\n\nHere is another example for a bar chart. In this example we first preprocess the data to get the counts of students.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Preprocess the data to get counts per Race and Gender\nprocessed_data &lt;- exam_data %&gt;%\n  group_by(RACE, GENDER) %&gt;%\n  summarise(Count = n(), .groups = 'drop')\n\nplot_ly(data = processed_data, \n             x = ~RACE, \n             y = ~Count, \n             type = 'bar',\n             color = ~GENDER,\n             hoverinfo = 'x+y') %&gt;%\n  layout(yaxis = list(title = 'Count of Students'),\n         barmode = 'group',\n         xaxis = list(title = 'Race'))\n\n\n\n\n\n\n\n\nWe replicate the bar chart using ggplotly().\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Preprocess the data to get counts per Race and Gender\nprocessed_data &lt;- exam_data %&gt;%\n  group_by(RACE, GENDER) %&gt;%\n  summarise(Count = n(), .groups = 'drop')\n\n# Create the bar chart with ggplot2\np &lt;- ggplot(processed_data, aes(x = RACE, y = Count, fill = GENDER)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(y = \"Count of Students\", x = \"Race\") +\n  theme_minimal()\n\n# Convert the ggplot2 chart to an interactive plotly chart\nggplotly(p)\n\n\n\n\n\n\n\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\n# Create object for margins\nmrg &lt;- list(l = 80, r = 30,\n          b = 50, t = 100,\n          pad =15)\n\nsubplot(ggplotly(p1), ggplotly(p2), \n        nrows = 1, margin = 0.07,\n        titleY = TRUE,\n        titleX = TRUE) %&gt;%\n  layout(title = paste(\"Linear relationship between Math scores and other scores\", \n                       \"&lt;br&gt;&lt;sub&gt;Scatter Plots Showing Relationship between Math Scores and English or Science Scores&lt;/sub&gt;\"),\n         margin = mrg)\n\n\n\n\nBelow is another attempt to create multiple views of plots that are not linked.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH,\n                fill = RACE)) +\n  geom_point(size=3) +\n  coord_cartesian(xlim=c(0,NA),\n                  ylim=c(0,NA)) +\n  labs(y = \"English Scores\", x = \"Math Scores\")\n\nprocessed_data &lt;- exam_data %&gt;%\n  group_by(RACE, GENDER) %&gt;%\n  summarise(Count = n(), .groups = 'drop')\n\np2 &lt;- ggplot(processed_data, aes(x = GENDER, y = Count, fill = RACE)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(y = \"Count of Students\", x = \"Race\") +\n  theme_minimal()\n\n# Combine the plots into a single interactive display\ncombined_plot &lt;- subplot(ggplotly(p1), ggplotly(p2), \n                         nrows = 1, margin = 0.07, \n                         shareX = TRUE, shareY = FALSE, \n                         titleX = TRUE, titleY = TRUE)\n\n# Create object for margins\nmrg &lt;- list(l = 80, r = 30,\n          b = 50, t = 100,\n          pad =15)\n\n\n# Add a title and a subtitle using HTML for formatting\ncombined_plot %&gt;%\n  layout(title = paste(\"Multiple plots\", \n                       \"&lt;br&gt;&lt;sub&gt;Scatter Plot of English and Math Scores and Bar Plot of Count of Students by Race and Gender&lt;/sub&gt;\"),\n         margin = mrg)\n\n\n\n\n\n\n\n\nCrosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\nBesides placing plots with the datatable, we also demo the usage of highlight_key(),which simply creates an object of class crosstalk::SharedData.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p), on = \"plotly_selected\", off = \"plotly_deselect\")\n \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5) \n\n\n\n\nThings to learn from the above code chunk:\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#overview",
    "title": "Hands-on Exercise 3",
    "section": "",
    "text": "In this exercises I will learn how to create interactive data visualisation by using functions provided by ggiraph and plotlyr packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#getting-started",
    "title": "Hands-on Exercise 3",
    "section": "",
    "text": "The following libraries will be used:\n\nggiraph for making ‘ggplot’ graphics interactive.\nplotly, R library for plotting interactive statistical graphs.\nDT provides an R interface to the JavaScript library DataTables that create interactive table on html page.\ntidyverse, a family of modern R packages specially designed to support data science, analysis and communication task including creating static statistical graphs.\npatchwork for combining multiple ggplot2 graphs into one figure.\n\n\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse) \n\n\n\n\nThe data used is the year end examination grades of a cohort of primary 3 students from a local school. From Hands-On Exercise 1, we know that the data has:\n\nFour categorical attributes: ID, CLASS, GENDER, RACE\nThree continuous attributes: MATHS, ENGLISH, SCIENCE\n\n\nexam_data &lt;- read_csv(\"data/Exam_data.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#interactive-data-visualisation---ggiraph-methods",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#interactive-data-visualisation---ggiraph-methods",
    "title": "Hands-on Exercise 3",
    "section": "",
    "text": "ggiraph is a html widget and ggplot2 extension, that allows ggplot graphics to be interactive:\nWe can makes ggplot graphics interactive with the following three arguments:\n\nTooltip: a column of data-sets that contain tooltips to be displayed when the mouse is over elements.\nData_id: a column of data-sets that contain an id to be associated with elements.\nOnclick: a column of data-sets that contain a JavaScript function to be executed when elements are clicked.\n\n\n\nLet’s first demonstrate how we can make some graphs interactive with a tooltip.\n\n\nThree steps to plotting a scatter plot with tooltip that shows studeNT ID:\n\nUse geom_point_interactive() instead of geom_point()\nProvide the aesthetic tooltip with ID\nCall function girafe with the ggplot object so that the graphic is translated as a web interactive graphic\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Create ggplot object\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS, y = SCIENCE)) +\n  geom_point_interactive(\n    aes(tooltip = ID)) +  # Provide tooltip aesthetic\n    scale_x_continuous(limits = c(0, NA)) + \n  scale_y_continuous(limits = c(0, NA)) +\n  labs(title = \"Positive relationship betweeen Science and Maths scores\",\n       subtitle = \"Scatter plot of science and math scores of students\",\n       y = \"Science\\n Scores\",\n       x = \"Math Scores\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        axis.title.y = element_text(size = 8, angle = 360, vjust = 0.5),\n        axis.title.x = element_text(size = 8))\n\n# Pass ggplot object into girafe\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\n\n\nThe content of the tooltip can be customised by including a list object, that is passed into the aesthetic for tooltip.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Create new column for tooltip information \nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS,\n  \"\\n Race = \", exam_data$RACE)) \n\n# Create ggplot object\np &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), # Pass in list object to tooltip aesthetic\n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL) +\n    labs(title = \"Distribution of Science Scores\",\n       subtitle = \"Dotplot\",\n       x = \"Science Scores\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8))\n\n# Pass ggplot object into girafe\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\n\n\nWe can use opts_tooltip() to further customize the tooltip, such as changing opacity of background, or adding css declarations.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Create new column for tooltip information\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS,\n  \"\\n Race = \", exam_data$RACE)) \n\n# Create ggplot object\np &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL) +\n    labs(title = \"Distribution of Science Scores\",\n       subtitle = \"Dotplot\",\n       x = \"Science Scores\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        axis.title.x = element_text(size = 8))\n\n# Create object for tooltip styling\ntooltip_css &lt;- \"background-color:#93c7c2; #&lt;&lt;\nfont-style:bold; color:#464646;\" #&lt;&lt;\n\n# Pass ggplot object and tooltip styling into girafe\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)   \n\n\n\n\n\n\n\nWe can also display derived statistics.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Create object for tooltip information  \ntooltip &lt;- function(ymin, lower, middle, upper, ymax, accuracy = .01) {\n  sem &lt;- scales::number((ymax - ymin) / 2, accuracy = accuracy) # SEM for 90% CI\n  median &lt;- scales::number(middle, accuracy = accuracy)\n  lower_quartile &lt;- scales::number(lower, accuracy = accuracy)\n  upper_quartile &lt;- scales::number(upper, accuracy = accuracy)\n  \n  paste(\"Median:\", median, \n        \"\\n25th Quartile:\", lower_quartile, \n        \"\\n75th Quartile:\", upper_quartile,\n        \"\\n95% CI: [\", scales::number(ymin, accuracy = accuracy), \n        \", \", scales::number(ymax, accuracy = accuracy), \"]\")\n}\n\n# Create ggplot object\ngg_boxplot &lt;- ggplot(data = exam_data, aes(x = GENDER, y = ENGLISH)) +\n  geom_boxplot_interactive(\n    aes(tooltip = after_stat(tooltip(ymin, lower, middle, upper, ymax))), # Pass in after_stat of tooltip information \n    stat = \"boxplot\"\n  ) +\n  labs(title = \"Boxplot of English Scores by Gender\",\n       x = \"Gender\",\n       y = \"English Scores\")\n\n# Pass in ggobject into girafe\ngirafe(ggobj = gg_boxplot,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\nIn the example below, elements associated with a data_id (i.e GENDER) will be highlighted upon mouse over.\nWe also demonstrate how css codes are used to customize the highlighting effect.\nUpdated on 3 Feb 2024: Added tooltip for data_id aesthetic\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Plot ggplot object\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS, y = SCIENCE)) +\n  \n  geom_point_interactive(\n    aes(tooltip = GENDER, # Update on 03 Feb: Added in tooltip\n        data_id = GENDER)) + # Pass in data_id\n  scale_x_continuous(limits = c(0, NA)) + \n  scale_y_continuous(limits = c(0, NA)) +\n  labs(title = \"Positive Relationship between Science and Maths Scores\",\n       subtitle = \"Scatter Plot of Science and Math Scores of Students\",\n       y = \"Science\\n Scores\",\n       x = \"Math Scores\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        axis.title.y = element_text(size = 8, angle = 360, vjust = 0.5),\n        axis.title.x = element_text(size = 8))\n\n# Pass ggoplot object into girafe\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618,\n  options = list(\n    opts_hover(css = \"fill: #f5bc5f;\"), \n    opts_hover_inv(css = \"opacity:0.2;\")\n  )\n)\n\n\n\n\n\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Plot ggplot object\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS, y = SCIENCE)) +\n  geom_point_interactive(\n    aes(tooltip = GENDER, # Pass in tooltip\n        data_id = GENDER)) + # Pass in data_id\n  scale_x_continuous(limits = c(0, NA)) + \n  scale_y_continuous(limits = c(0, NA)) +\n  labs(title = \"Positive Relationship between Science and Maths Scores\",\n       subtitle = \"Scatter Plot of Science and Math Scores of Students\",\n       y = \"Science\\n Scores\",\n       x = \"Math Scores\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        axis.title.y = element_text(size = 8, angle = 360, vjust = 0.5),\n        axis.title.x = element_text(size = 8))\n\n# Create object for tooltip styling\ntooltip_css &lt;- \"background-color:#f53b5d; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\n# Pass ggoplot object into girafe\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618,\n  options = list(\n    opts_hover(css = \"fill: #f5bc5f;\"), \n    opts_hover_inv(css = \"opacity:0.2;\"),\n    opts_tooltip(css = tooltip_css)\n  )\n)\n\n\n\n\n\n\n\n\nonclick argument of ggiraph provides hotlink interactivity on the web.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\nexam_data$onclick &lt;- sprintf(\"window.open(\\\"%s%s\\\")\",\n\"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\nas.character(exam_data$ID))\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(onclick = onclick, # Pass in onclick aesthetic\n        data_id = ID),   # Pass in data_id to show hover effect before clicking in           \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618) \n\n\n\n\n\n\n\nWe can use patchwork function to create interactive coordinated multiple views, that are linked by student ID.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Update on 03 Feb 2024: Add in tooltip\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Math Score = \", exam_data$MATHS,\n  \"\\n Science Score = \", exam_data$SCIENCE,\n  \"\\n English Score = \", exam_data$ENGLISH)) \n\n# Plot first ggplot object\np1 &lt;- ggplot(data=exam_data, \n            aes(x = MATHS, y = SCIENCE)) +\n  geom_point_interactive(\n    aes(tooltip = exam_data$tooltip, \n        data_id = ID)) + # Pass in data_id\n  scale_x_continuous(limits = c(0, NA)) + \n  scale_y_continuous(limits = c(0, NA)) +\n  labs(y = \"Science\\n Scores\",\n       x = \"Math Scores\") +\n  theme(axis.title.y = element_text(size = 8, angle = 360, vjust = 0.5),\n        axis.title.x = element_text(size = 8))\n\n# Plot second ggplot object\np2 &lt;- ggplot(data=exam_data, \n            aes(x = MATHS, y = ENGLISH)) +\n  geom_point_interactive(\n    aes(tooltip = exam_data$tooltip,\n        data_id = ID)) + # Pass in data_id\n  scale_x_continuous(limits = c(0, NA)) + \n  scale_y_continuous(limits = c(0, NA)) +\n  labs(y = \"English\\n Scores\",\n       x = \"Math Scores\") +\n  theme(axis.title.y = element_text(size = 8, angle = 360, vjust = 0.5),\n        axis.title.x = element_text(size = 8))\n\n# Pass both ggplot objects into girafe, using patchwork\ngirafe(code = print(p1 + p2), # Take note that print is required\n       width_svg = 6,\n       height_svg = 6*0.618,\n       options = list(\n         opts_hover(css = \"fill: #f5bc5f;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#interactive-data-visualisation---plotly-methods",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#interactive-data-visualisation---plotly-methods",
    "title": "Hands-on Exercise 3",
    "section": "",
    "text": "There are two ways to create interactive graph by using plotly, they are:\n\nby using plot_ly(), and\nby using ggplotly()\n\n\n\nBelow we explore a basic way to plot an interactive plot.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~SCIENCE, \n        y = ~MATHS,\n        type = 'scatter',\n        mode = 'markers') \n\n\n\n\n\n\nIn the code chunk below, color argument is mapped to a qualitative visual variable (i.e. RACE).\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\nplot_ly(data = exam_data, \n        x = ~SCIENCE, \n        y = ~MATHS, \n        color = ~RACE,\n        type = 'scatter',\n        mode = 'markers')\n\n\n\n\nHere is another example for a bar chart. In this example we first preprocess the data to get the counts of students.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Preprocess the data to get counts per Race and Gender\nprocessed_data &lt;- exam_data %&gt;%\n  group_by(RACE, GENDER) %&gt;%\n  summarise(Count = n(), .groups = 'drop')\n\nplot_ly(data = processed_data, \n             x = ~RACE, \n             y = ~Count, \n             type = 'bar',\n             color = ~GENDER,\n             hoverinfo = 'x+y') %&gt;%\n  layout(yaxis = list(title = 'Count of Students'),\n         barmode = 'group',\n         xaxis = list(title = 'Race'))\n\n\n\n\n\n\n\n\nWe replicate the bar chart using ggplotly().\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n# Preprocess the data to get counts per Race and Gender\nprocessed_data &lt;- exam_data %&gt;%\n  group_by(RACE, GENDER) %&gt;%\n  summarise(Count = n(), .groups = 'drop')\n\n# Create the bar chart with ggplot2\np &lt;- ggplot(processed_data, aes(x = RACE, y = Count, fill = GENDER)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(y = \"Count of Students\", x = \"Race\") +\n  theme_minimal()\n\n# Convert the ggplot2 chart to an interactive plotly chart\nggplotly(p)\n\n\n\n\n\n\n\nThe creation of a coordinated linked plot by using plotly involves three steps:\n\nhighlight_key() of plotly package is used as shared data.\ntwo scatterplots will be created by using ggplot2 functions.\nlastly, subplot() of plotly package is used to place them next to each other side-by-side.\n\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\n# Create object for margins\nmrg &lt;- list(l = 80, r = 30,\n          b = 50, t = 100,\n          pad =15)\n\nsubplot(ggplotly(p1), ggplotly(p2), \n        nrows = 1, margin = 0.07,\n        titleY = TRUE,\n        titleX = TRUE) %&gt;%\n  layout(title = paste(\"Linear relationship between Math scores and other scores\", \n                       \"&lt;br&gt;&lt;sub&gt;Scatter Plots Showing Relationship between Math Scores and English or Science Scores&lt;/sub&gt;\"),\n         margin = mrg)\n\n\n\n\nBelow is another attempt to create multiple views of plots that are not linked.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH,\n                fill = RACE)) +\n  geom_point(size=3) +\n  coord_cartesian(xlim=c(0,NA),\n                  ylim=c(0,NA)) +\n  labs(y = \"English Scores\", x = \"Math Scores\")\n\nprocessed_data &lt;- exam_data %&gt;%\n  group_by(RACE, GENDER) %&gt;%\n  summarise(Count = n(), .groups = 'drop')\n\np2 &lt;- ggplot(processed_data, aes(x = GENDER, y = Count, fill = RACE)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(y = \"Count of Students\", x = \"Race\") +\n  theme_minimal()\n\n# Combine the plots into a single interactive display\ncombined_plot &lt;- subplot(ggplotly(p1), ggplotly(p2), \n                         nrows = 1, margin = 0.07, \n                         shareX = TRUE, shareY = FALSE, \n                         titleX = TRUE, titleY = TRUE)\n\n# Create object for margins\nmrg &lt;- list(l = 80, r = 30,\n          b = 50, t = 100,\n          pad =15)\n\n\n# Add a title and a subtitle using HTML for formatting\ncombined_plot %&gt;%\n  layout(title = paste(\"Multiple plots\", \n                       \"&lt;br&gt;&lt;sub&gt;Scatter Plot of English and Math Scores and Bar Plot of Count of Students by Race and Gender&lt;/sub&gt;\"),\n         margin = mrg)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#interactive-data-visualisation---crosstalk-methods",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#interactive-data-visualisation---crosstalk-methods",
    "title": "Hands-on Exercise 3",
    "section": "",
    "text": "Crosstalk is an add-on to the htmlwidgets package. It extends htmlwidgets with a set of classes, functions, and conventions for implementing cross-widget interactions (currently, linked brushing and filtering).\n\n\n\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\nBesides placing plots with the datatable, we also demo the usage of highlight_key(),which simply creates an object of class crosstalk::SharedData.\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p), on = \"plotly_selected\", off = \"plotly_deselect\")\n \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5) \n\n\n\n\nThings to learn from the above code chunk:\n\nhighlight() is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#overview-1",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#overview-1",
    "title": "Hands-on Exercise 3",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nIn this exercise I will learn\n\nhow to create animated data visualisation by using gganimate and plotly\nhow to reshape data by using tidyr\nhow to process, wrangle and transform data by using dplyr"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#getting-started-1",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#getting-started-1",
    "title": "Hands-on Exercise 3",
    "section": "2.2 Getting Started",
    "text": "2.2 Getting Started\n\n2.2.1 Loading libraries\n\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\n2.2.2 Importing data\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\n\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(all_of(col), as.factor)) %&gt;% \n  mutate(Year = as.integer(Year))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#animated-data-visualisation-gganimate-methods",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#animated-data-visualisation-gganimate-methods",
    "title": "Hands-on Exercise 3",
    "section": "2.3 Animated Data Visualisation: gganimate methods",
    "text": "2.3 Animated Data Visualisation: gganimate methods\ngganimate() provides a range of new grammar classes that can be added to the plot object in order to customise how it should animate by changing with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n2.3.1 Building a static plot\nWe first build a static line chart showing the population for all countries in Asia for the year of 2020. .\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(filter(globalPop, Year == 2020, Continent == \"Asia\"), aes(x = reorder(Country, Population), y = Population)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = 'Population by Country in 2020', x = 'Country', y = 'Population')\n\n\n\n\n\n\n2.3.2 Building the animated bar plot\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(filter(globalPop, Continent == \"Asia\"), aes(x = reorder(Country, Population), y = Population, fill = Country)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +  # Flip coordinates to make the country names horizontal\n  labs(title = 'Population by Country in Asia, Year: {frame_time}', x = '', y = 'Population') +\n  theme_minimal() +\n  theme(legend.position = \"none\")  +\n  transition_time(Year) +       \n  ease_aes('linear') \n\n\n\n\nYou can further customize the animation:\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Build the plot\np &lt;- ggplot(filter(globalPop, Continent == \"Asia\"), aes(x = reorder(Country, Population), y = Population, fill = Country)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +  # Flip coordinates to make the country names horizontal\n  labs(title = 'Population by Country in Asia, Year: {frame_time}', x = '', y = 'Population') +\n  theme_minimal() +\n  theme(legend.position = \"none\")  +\n  transition_time(Year) +       \n  ease_aes('linear')\n\n# Animate the plot\nanim &lt;- p + transition_time(Year) +\n  ease_aes('linear')\n\n# Render the animation\nanimate(anim, nframes = 100, height = 600, width = 1000, fps = 5)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#animated-data-visualisation-plotly",
    "href": "Hands-on_Ex/Hands-on_Ex3/Hands-on_Ex3.html#animated-data-visualisation-plotly",
    "title": "Hands-on Exercise 3",
    "section": "2.4 Animated Data Visualisation: plotly",
    "text": "2.4 Animated Data Visualisation: plotly\n\n2.4.1 Building an animated bubble plot: ggplotly() method\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\ngg &lt;- ggplot(filter(globalPop, Continent == \"Asia\"),\n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') +\n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\n2.4.2 Building an animated bubble plot: plot_ly() method\n\nThe PlotThe Code Chunk\n\n\n\n\n\n\n\n\n\n\n\nbp &lt;- globalPop %&gt;%\n  plot_ly(x = ~Old, \n          y = ~Young, \n          size = ~Population, \n          color = ~Continent,\n          sizes = c(2, 100),\n          frame = ~Year, \n          text = ~Country, \n          hoverinfo = \"text\",\n          type = 'scatter',\n          mode = 'markers'\n          ) %&gt;%\n  layout(showlegend = FALSE)\nbp"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html",
    "title": "Hands-on Exercise 5a",
    "section": "",
    "text": "In this exercise I will practice building ternary plots by visualizing the population structure of Singapore.\nThe following packages will be used:\n\ndplyr: Use mutate() function to derive three new measures\nggtern: Use package to build a static ternary plot\nplotly: Use package to build an interactive ternary plot\n\n\n\n\n\npacman::p_load('tidyverse', 'ggtern', 'plotly')\n\n\n\n\n\n\n\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nWe will use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\n\n\n\n\nWe can first plot a simple ternary plot.\n\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title = \"Population Distribution by Age Groups in 2018: A Ternary Perspective\") +\n  theme(plot.title = element_text(size = 12))\n\n\n\n\n\n\n\n\nWe further customise the themes. We can find a list of ggtern themes here.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2018\") +\n  theme_tropical()\n\n\n\n\n\n\n\n\n\n\n\n\n# Create annotation \nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n# Create a combined label column\nagpop_mutated &lt;- agpop_mutated %&gt;%\n  mutate(label = paste(PA, SZ, sep = \", \"))  # Combine 'PA' and 'SZ'\n\n# Format axes\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD,\n  text = ~label,\n  color = I(\"black\"), \n  type = \"scatterternary\",\n  mode = \"markers\",\n  marker = list( \n      symbol = 100,\n      color = '#DB7365',\n      size = 2,\n      line = list('width' = 2)\n    )# Add text to the markers\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html#overview",
    "title": "Hands-on Exercise 5a",
    "section": "",
    "text": "In this exercise I will practice building ternary plots by visualizing the population structure of Singapore.\nThe following packages will be used:\n\ndplyr: Use mutate() function to derive three new measures\nggtern: Use package to build a static ternary plot\nplotly: Use package to build an interactive ternary plot"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html#loading-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html#loading-libraries",
    "title": "Hands-on Exercise 5a",
    "section": "",
    "text": "pacman::p_load('tidyverse', 'ggtern', 'plotly')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html#data-preparation",
    "title": "Hands-on Exercise 5a",
    "section": "",
    "text": "pop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\nWe will use the mutate() function of dplyr package to derive three new measures, namely: young, active, and old.\n\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html#plotting-ternary-diagram-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5a.html#plotting-ternary-diagram-with-r",
    "title": "Hands-on Exercise 5a",
    "section": "",
    "text": "We can first plot a simple ternary plot.\n\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title = \"Population Distribution by Age Groups in 2018: A Ternary Perspective\") +\n  theme(plot.title = element_text(size = 12))\n\n\n\n\n\n\n\n\nWe further customise the themes. We can find a list of ggtern themes here.\n\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2018\") +\n  theme_tropical()\n\n\n\n\n\n\n\n\n\n\n\n\n# Create annotation \nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n# Create a combined label column\nagpop_mutated &lt;- agpop_mutated %&gt;%\n  mutate(label = paste(PA, SZ, sep = \", \"))  # Combine 'PA' and 'SZ'\n\n# Format axes\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD,\n  text = ~label,\n  color = I(\"black\"), \n  type = \"scatterternary\",\n  mode = \"markers\",\n  marker = list( \n      symbol = 100,\n      color = '#DB7365',\n      size = 2,\n      line = list('width' = 2)\n    )# Add text to the markers\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "Here I practice plotting static and interactive heatmap for visualising and analysing multivariate data.\n\n\n\n\npacman::p_load(seriation, dendextend, heatmaply, tidyverse)\n\n\n\n\n\n\n\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nChange the rows by country name instead of row number: The row number has been replaced into the country name.\n\nrow.names(wh) &lt;- wh$Country\n\nThen the dataframe is transformed into a data matrix (R Matrix format).\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\n\n\n\n\nThere are many packages that can be used to draw heatmaps:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nFor this exercise we will learn how to use heatmap() of R Stats package.\n\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\nNote that the heatmap does a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\nTo make the heatmap more informative, we need to perform normalization with scale argument. This is because some of the Happiness Score variables have relatively higher values, what makes that the other variables with small values all look the same.\nWe normalise the matrix column-wise. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))\n\n\n\n\n\n\n\n\n\n\n\n\nWe use heatmaply to build interactive cluster heatmap that can be shared online as a standalone html file.\nRead the Introduction to Heatmaply and its user manual to have an overall understanding of the features and functions of Heatmaply package.\n\n\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)]) \n\n\n\n\n\n\n\n\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentize.\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\n\nHierarchical clustering doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized.\n\n“OLO”: Optimal leaf ordering optimizes the above criterion (in O(n^4))\n“GW” Gruvaeus and Wainer heuristic to optimize the Hamiltonian path length that is restricted by the dendrogram structure\n“mean”: sorts the matrix based on the reorderfun using marginal means of the matrix\n“none”\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\nVary the color palette using colors argument.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#overview",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "Here I practice plotting static and interactive heatmap for visualising and analysing multivariate data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#loading-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#loading-libraries",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "pacman::p_load(seriation, dendextend, heatmaply, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#import-and-prepare-data",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#import-and-prepare-data",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "wh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nChange the rows by country name instead of row number: The row number has been replaced into the country name.\n\nrow.names(wh) &lt;- wh$Country\n\nThen the dataframe is transformed into a data matrix (R Matrix format).\n\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#static-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#static-heatmap",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "There are many packages that can be used to draw heatmaps:\n\nheatmap()of R stats package. It draws a simple heatmap.\nheatmap.2() of gplots R package. It draws an enhanced heatmap compared to the R base function.\npheatmap() of pheatmap R package. pheatmap package also known as Pretty Heatmap. The package provides functions to draws pretty heatmaps and provides more control to change the appearance of heatmaps.\nComplexHeatmap package of R/Bioconductor package. The package draws, annotates and arranges complex heatmaps (very useful for genomic data analysis). The full reference guide of the package is available here.\nsuperheat package: A Graphical Tool for Exploring Complex Datasets Using Heatmaps. A system for generating extendable and customizable heatmaps for exploring complex datasets, including big data and data with multiple data types. The full reference guide of the package is available here.\n\nFor this exercise we will learn how to use heatmap() of R Stats package.\n\n\nBy default, heatmap() plots a cluster heatmap. The arguments Rowv=NA and Colv=NA are used to switch off the option of plotting the row and column dendrograms.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code chunk below.\nNote that the heatmap does a reordering using clusterisation: it calculates the distance between each pair of rows and columns and try to order them by similarity. Moreover, the corresponding dendrogram are provided beside the heatmap.\n\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\nTo make the heatmap more informative, we need to perform normalization with scale argument. This is because some of the Happiness Score variables have relatively higher values, what makes that the other variables with small values all look the same.\nWe normalise the matrix column-wise. Also note that margins argument is used to ensure that the entire x-axis labels are displayed completely and, cexRow and cexCol arguments are used to define the font size used for y-axis and x-axis labels respectively.\n\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 4))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#interactive-heatmap",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5c.html#interactive-heatmap",
    "title": "Hands-on Exercise 5c",
    "section": "",
    "text": "We use heatmaply to build interactive cluster heatmap that can be shared online as a standalone html file.\nRead the Introduction to Heatmaply and its user manual to have an overall understanding of the features and functions of Heatmaply package.\n\n\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)]) \n\n\n\n\n\n\n\n\nWhen analysing multivariate data set, it is very common that the variables in the data sets includes values that reflect different types of measurement. In general, these variables’ values have their own range. In order to ensure that all the variables have comparable values, data transformation are commonly used before clustering.\nThree main data transformation methods are supported by heatmaply(), namely: scale, normalise and percentize.\n\n\n\nWhen all variables are came from or assumed to come from some normal distribution, then scaling (i.e.: subtract the mean and divide by the standard deviation) would bring them all close to the standard normal distribution.\nIn such a case, each value would reflect the distance from the mean in units of standard deviation.\nThe scale argument in heatmaply() supports column and row scaling.\n\nThe code chunk below is used to scale variable values columewise.\n\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n\nWhen variables in the data comes from possibly different (and non-normal) distributions, the normalize function can be used to bring data to the 0 to 1 scale by subtracting the minimum and dividing by the maximum of all observations.\nThis preserves the shape of each variable’s distribution while making them easily comparable on the same “scale”.\n\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nThis is similar to ranking the variables, but instead of keeping the rank values, divide them by the maximal rank.\nThis is done by using the ecdf of the variables on their own values, bringing each value to its empirical percentile.\nThe benefit of the percentize function is that each value has a relatively clear interpretation, it is the percent of observations that got that value or below it.\n\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\nheatmaply supports a variety of hierarchical clustering algorithm. The main arguments provided are:\n\ndistfun: function used to compute the distance (dissimilarity) between both rows and columns. Defaults to dist. The options “pearson”, “spearman” and “kendall” can be used to use correlation-based clustering, which uses as.dist(1 - cor(t(x))) as the distance metric (using the specified correlation method).\nhclustfun: function used to compute the hierarchical clustering when Rowv or Colv are not dendrograms. Defaults to hclust.\ndist_method default is NULL, which results in “euclidean” to be used. It can accept alternative character strings indicating the method to be passed to distfun. By default distfun is “dist”” hence this can be one of “euclidean”, “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.\nhclust_method default is NULL, which results in “complete” method to be used. It can accept alternative character strings indicating the method to be passed to hclustfun. By default hclustfun is hclust hence this can be one of “ward.D”, “ward.D2”, “single”, “complete”, “average” (= UPGMA), “mcquitty” (= WPGMA), “median” (= WPGMC) or “centroid” (= UPGMC).\n\nIn general, a clustering model can be calibrated either manually or statistically.\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\nIn order to determine the best clustering method and number of cluster the dend_expend() and find_k() functions of dendextend package will be used.\nFirst, the dend_expend() will be used to determine the recommended clustering method to be used.\n\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\n\nHierarchical clustering doesn’t actually place the rows in a definite order, it merely constrains the space of possible orderings.\nheatmaply uses the seriation package to find an optimal ordering of rows and columns. Optimal means to optimize the Hamiltonian path length that is restricted by the dendrogram structure. This, in other words, means to rotate the branches so that the sum of distances between each adjacent leaf (label) will be minimized.\n\n“OLO”: Optimal leaf ordering optimizes the above criterion (in O(n^4))\n“GW” Gruvaeus and Wainer heuristic to optimize the Hamiltonian path length that is restricted by the dendrogram structure\n“mean”: sorts the matrix based on the reorderfun using marginal means of the matrix\n“none”\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\nVary the color palette using colors argument.\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\nBeside providing a wide collection of arguments for meeting the statistical analysis needs, heatmaply also provides many plotting features to ensure cartographic quality heatmap can be produced.\nIn the code chunk below the following arguments are used:\n\nk_row is used to produce 5 groups.\nmargins is used to change the top margin to 60 and row margin to 200.\nfontsizw_row and fontsize_col are used to change the font size for row and column labels to 4.\nmain is used to write the main title of the plot.\nxlab and ylab are used to write the x-axis and y-axis labels respectively.\n\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "In this exercise, I will gain hands-on experiences on designing treemap using appropriate R packages.:\n\nManipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package\nPlot static treemap by using treemap package.\nDesign interactive treemap by using d3treeR package.\n\n\n\n\n\npacman::p_load(treemap, treemapify, tidyverse) \n\n\n\n\n\n\n\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\n\n\n\nHere we will prepare a data.frame that is appropriate for treemap visualization:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\ntreemap() offers at least 43 arguments.\n\n\nHere we plot a treemap showing distribution of median unit prices and total units sold of resale condominium by geographic hierachy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because its values will be used to map the sizes of the rectangles of the treemaps.\n\n\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\nThere are two arguments that determine the mapping to color palettes: mapping and palette.\nThe only difference between “value” and “manual” is the default value for mapping.\nThe “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color.\nThe “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu, there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\nThe value range is mapped linearly to the colour palette.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very confusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n\nBefore getting started, you should read Introduction to “treemapify” and its user guide.\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`), # Group by Planning Region\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) +  #Group by Planning Area\n\n  geom_treemap()\n\n\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\", # Adding boundary line\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(d3treeR)\n\n\n\n\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#overview",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "In this exercise, I will gain hands-on experiences on designing treemap using appropriate R packages.:\n\nManipulate transaction data into a treemap strcuture by using selected functions provided in dplyr package\nPlot static treemap by using treemap package.\nDesign interactive treemap by using d3treeR package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#loading-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#loading-libraries",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "pacman::p_load(treemap, treemapify, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#import-and-prepare-data",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#import-and-prepare-data",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "realis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\n\n\n\nHere we will prepare a data.frame that is appropriate for treemap visualization:\n\ngroup transaction records by Project Name, Planning Region, Planning Area, Property Type and Type of Sale, and\ncompute Total Unit Sold, Total Area, Median Unit Price and Median Transacted Price by applying appropriate summary statistics on No. of Units, Area (sqm), Unit Price ($ psm) and Transacted Price ($) respectively.\n\n\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#designing-treemap-with-treemap",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#designing-treemap-with-treemap",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "treemap() offers at least 43 arguments.\n\n\nHere we plot a treemap showing distribution of median unit prices and total units sold of resale condominium by geographic hierachy in 2017.\nFirst, we will select records belongs to resale condominium property type from realis2018_selected data frame.\n\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\nThe code chunk below designed a treemap by using three core arguments of treemap(), namely: index, vSize and vColor.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the three arguments used:\n\nindex\n\nThe index vector must consist of at least two column names or else no hierarchy treemap will be plotted.\nIf multiple column names are provided, such as the code chunk above, the first name is the highest aggregation level, the second name the second highest aggregation level, and so on.\n\nvSize\n\nThe column must not contain negative values. This is because its values will be used to map the sizes of the rectangles of the treemaps.\n\n\nThe treemap above was wrongly coloured. For a correctly designed treemap, the colours of the rectagles should be in different intensity showing, in our case, median unit prices.\nFor treemap(), vColor is used in combination with the argument type to determines the colours of the rectangles. Without defining type, like the code chunk above, treemap() assumes type = index, in our case, the hierarchy of planning areas.\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the conde chunk above.\n\nThe rectangles are coloured with different intensity of green, reflecting their respective median unit prices.\nThe legend reveals that the values are binned into ten bins, i.e. 0-5000, 5000-10000, etc. with an equal interval of 5000.\n\n\n\n\nThere are two arguments that determine the mapping to color palettes: mapping and palette.\nThe only difference between “value” and “manual” is the default value for mapping.\nThe “value” treemap considers palette to be a diverging color palette (say ColorBrewer’s “RdYlBu”), and maps it in such a way that 0 corresponds to the middle color (typically white or yellow), -max(abs(values)) to the left-end color, and max(abs(values)), to the right-end color.\nThe “manual” treemap simply maps min(values) to the left-end color, max(values) to the right-end color, and mean(range(values)) to the middle color.\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nalthough the colour palette used is RdYlBu, there are no red rectangles in the treemap above. This is because all the median unit prices are positive.\nThe reason why we see only 5000 to 45000 in the legend is because the range argument is by default c(min(values, max(values)) with some pretty rounding.\n\n\n\n\nThe value range is mapped linearly to the colour palette.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\nThe colour scheme used is very confusing. This is because mapping = (min(values), mean(range(values)), max(values)). It is not wise to use diverging colour palette such as RdYlBu if the values are all positive or negative\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n\ntreemap() supports two popular treemap layouts, namely: “squarified” and “pivotSize”. The default is “pivotSize”.\nThe squarified treemap algorithm (Bruls et al., 2000) produces good aspect ratios, but ignores the sorting order of the rectangles (sortID). The ordered treemap, pivot-by-size, algorithm (Bederson et al., 2002) takes the sorting order (sortID) into account while aspect ratios are still acceptable.\n\n\n\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\nWhen “pivotSize” algorithm is used, sortID argument can be used to dertemine the order in which the rectangles are placed from top left to bottom right.\n\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#designing-treemap-using-treemapify-package",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#designing-treemap-using-treemapify-package",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "Before getting started, you should read Introduction to “treemapify” and its user guide.\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`), # Group by Planning Region\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) +  #Group by Planning Area\n\n  geom_treemap()\n\n\n\n\n\n\n\n\n\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"gray40\", # Adding boundary line\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"gray20\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#designing-interactive-treemap-using-d3treer",
    "href": "Hands-on_Ex/Hands-on_Ex5/Hands-on_Ex5e.html#designing-interactive-treemap-using-d3treer",
    "title": "Hands-on Exercise 5e",
    "section": "",
    "text": "library(d3treeR)\n\n\n\n\nThe codes below perform two processes.\n\ntreemap() is used to build a treemap by using selected variables in condominium data.frame. The treemap created is save as object called tm.\n\n\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nThen d3tree() is used to build an interactive treemap.\n\n\nd3tree(tm,rootname = \"Singapore\" )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\n\ntidyverse, and\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven)\n\n\n\n\n\n\n\nNote\n\n\n\npacman::p_load(tidyverse, haven) is calling the pacman package, since we have not yet loaded this package. If pacman package was already loaded, we can load tidyverse and haven with p_load(tidyverse, haven)."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#loading-r-packages",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#loading-r-packages",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\n\ntidyverse, and\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven)\n\n\n\n\n\n\n\nNote\n\n\n\npacman::p_load(tidyverse, haven) is calling the pacman package, since we have not yet loaded this package. If pacman package was already loaded, we can load tidyverse and haven with p_load(tidyverse, haven)."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-pisa-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-pisa-data",
    "title": "In-class Exercise 1",
    "section": "2 Importing PISA data",
    "text": "2 Importing PISA data\nThe code chunk below uses read_sas() of haven to import PISA data into R environment.\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe code chunk below filters the data where country code (CNT) is Singapore (SGP).\n\nstu_qqq_SG &lt;- stu_qqq %&gt;% \n  filter(CNT == \"SGP\")\n\nThe code chunk below then saves this filtered data as an RData file. This smaller file can be used in the future.\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\nTo load this file in the future, follow the code in the code chunk below.\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608 Visual Analytics and Applications",
    "section": "",
    "text": "ISSS608 Visual Analytics and Applications\nWelcome to my portfolio for the Visual Analytics and Applications course, taught by Prof. Kam Tin Seong. This website is a collection of exercises and assignments completed throughout the course."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "In this exercise, we select a peer submission of Take-home Exercise 1. For each visualiation within the submission, we will:\n\ncritique it for clarity and aesthetics,\npresent a sketch and the proposed modifications, and\npresent the remake of the visualization."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#overview",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#overview",
    "title": "Take-home Exercise 2",
    "section": "",
    "text": "In this exercise, we select a peer submission of Take-home Exercise 1. For each visualiation within the submission, we will:\n\ncritique it for clarity and aesthetics,\npresent a sketch and the proposed modifications, and\npresent the remake of the visualization."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#getting-started",
    "title": "Take-home Exercise 2",
    "section": "Getting started",
    "text": "Getting started\n\nLoad packages\nFirst, we load packages required:\n\npacman::p_load(tidyverse, haven,\n               ggrepel, ggthemes,\n               ggridges, ggdist,\n               patchwork, ggpattern,\n               hrbrthemes)\n\n\n\nImport data\nNext, we import data. The data used in this exercise is the same as the data used in the Take-home Exercise 1.\nWe import this dataset as stu_qqq_SG:\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")\n\n\n\nData pre-processing\nSince the focus of the exercise is on critiquing the visualizations, instead of the variable selection or data pre-processing, we will first pre-process the data to a similar format used for the original data visualizations.\nThis is performed in the code chunk below.\n\n\nCode\n# Select columns relating to this analysis\nselect_df &lt;- stu_qqq_SG[c(3:4,7,26,1039,1167:1196)]\n\n# Combine 10 PVs for each subject\nmath_long &lt;- select_df %&gt;%\n  pivot_longer(\n    cols = contains(\"MATH\"),\n    names_to = \"MATH\",\n    values_to = \"MATH_score\"\n  )\nmath_long &lt;- math_long %&gt;% \n  arrange(CNTSCHID, CNTSTUID, STRATUM, ST004D01T, ESCS)\nmath_long_selected &lt;- select(math_long, -contains(\"READ\"), -contains(\"SCIE\"))\n\nread_long &lt;- select_df %&gt;%\n  pivot_longer(\n    cols = contains(\"READ\"),\n    names_to = \"READ\",\n    values_to = \"READ_score\"\n  )\nread_long &lt;- read_long %&gt;% \n  arrange(CNTSCHID, CNTSTUID, STRATUM, ST004D01T, ESCS)\nread_long_selected &lt;- select(read_long, -contains(\"MATH\"), -contains(\"SCIE\"), -one_of(names(math_long_selected)))\n\nscie_long &lt;- select_df %&gt;%\n  pivot_longer(\n    cols = contains(\"SCIE\"),\n    names_to = \"SCIENCE\",\n    values_to = \"SCIENCE_score\"\n  )\nscie_long &lt;- scie_long %&gt;% \n  arrange(CNTSCHID, CNTSTUID, STRATUM, ST004D01T, ESCS)\nscie_long_selected &lt;- select(scie_long, -contains(\"MATH\"), -contains(\"READ\"), -one_of(names(math_long_selected)))\n\ncombined_long_df &lt;- bind_cols(math_long_selected, read_long_selected, scie_long_selected)\n\n# Translate column names and Group ESCS status\nschool_map &lt;- c(\"SGP01\" = \"Public/Secondary\",\n                \"SGP02\" = \"Public/Post-secondary\",\n                \"SGP03\" = \"Private/Secondary\",\n                \"SGP97\" = \"Undisclosed\")\ngender_map &lt;- c(\"1\" = \"Female\",\n                \"2\" = \"Male\")\n\nclean_df &lt;- combined_long_df %&gt;%\n  mutate(SCHOOL = school_map[STRATUM],\n         GENDER = gender_map[ST004D01T],\n         Math = as.numeric(MATH_score),\n         Read = as.numeric(READ_score),\n         Science = as.numeric(SCIENCE_score),\n         ESC_status = round(((ESCS - min(ESCS, na.rm = TRUE))/\n                              (max(ESCS, na.rm = TRUE)-min(ESCS, na.rm = TRUE)))*100, digits = 0),\n         ESC_status = case_when(\n           ESC_status &gt;= 0 & ESC_status &lt; 25 ~ \"Low\",\n           ESC_status &gt;= 25 & ESC_status &lt; 50 ~ \"Lower-Middle\",\n           ESC_status &gt;= 50 & ESC_status &lt; 75 ~ \"Upper-Middle\",\n           ESC_status &gt;= 75 & ESC_status &lt;= 100 ~ \"High\",\n           TRUE ~ as.character(ESC_status)\n         ))\n\n# Remove unused columns\nclean_short_df &lt;- clean_df %&gt;%\n  select(-c(CNTSCHID, STRATUM, ST004D01T, ESCS, MATH_score, READ_score, SCIENCE_score))\n\n\nWe will move on to reviewing and remaking the 4 visualizations for the exercise."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#visualization-1-distribution-of-performance-on-each-subject",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#visualization-1-distribution-of-performance-on-each-subject",
    "title": "Take-home Exercise 2",
    "section": "Visualization 1: Distribution of Performance on each subject",
    "text": "Visualization 1: Distribution of Performance on each subject\n\nOriginal Design\nThis visualization is intended to show the distribution of performance for each subject. This is done using histograms for each subject, along with annotations to provide some statistical detail.\nThe original design is shown below.\n\n\n\nCritique\nClarity:\nWhy it is clear:\n\nThe annotations for median, 1st quantile, 3rd quantile and outliers provide additional detail about the distributions that a histogram does not provide.\nBoth the X and Y axes start at 0, and the 2D columns are easy to gauge.\n\nWhy it can be confusing:\n\nThe “2+1” layout places the three histograms on different Y-axis scales, making it difficult to visually compare the frequency of scores across subjects. Viewers might misinterpret the data, or have to spend extra time interpreting the scales for each histogram correctly.\nThe plot title is generic. A more descriptive and meaningful title, supported by a sub-title, would make the message or insight of the visualization clearer.\nThe X and Y-axes are not labeled, hence there is no context for what is plotted. With labels, viewers can quickly understand the data without having to make assumptions.\n\nAesthetics\nWhy it is beautiful:\n\nThe plots have a simple and clear color scheme, which provides good readability. The grey histograms against a white background with faint grid lines provides a clear contrast, distinguishing the bars easily.\nEach histogram is clearly and consistently labeled with the subject name at the X-axis, helping the viewer to identify the subject matter without ambiguity.\nDifferent colors were used for the annotations, making these elements stand out and easier for the viewer to identify.\n\nWhy it can be ugly:\n\nThe plot title seems to be the same size as the axes label, and is hence not the most prominent element. It should be more prominent than other elements since it provides an overview of the plot.\nThe placements of annotations make the numbers difficult to read. Some numbers overlap with the details of the histogram, while others overlap with the ticks on the X-axis.\n\n\n\nSketch\nA sketch for the alternative design is shown below.\n\nTo improve clarity:\n\nThe histograms are arranged in a single column with the same y-axis scale.\nTitles, subtitles and axis titles are added to provide context to the visual.\nAs additional improvement, the X-axis are configured to have labeled major tick marks every 200 units and minor, unlabeled tick marks every 100 units, providing a clear and detailed scale for the axis from 0 to 1000.\n\nTo improve aesthetics:\n\nFont sizes and formatting of titles, subtitles and axis titles have been added to make the visualization look more professional.\nReplace annotations on histogram with annotated boxplots for each subject, so that the statistical details can be more clearly seen.\n\n\n\nRemake\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Reshape the data from wide to long format\nlong_df &lt;- clean_short_df %&gt;%\n  gather(key = \"Subject\", value = \"Score\", c(\"Math\", \"Science\", \"Read\"))\n\n# Calculate the quantiles for each subject\nquantiles &lt;- long_df %&gt;%\n  group_by(Subject) %&gt;%\n  summarise(\n    Q1 = quantile(Score, probs = 0.25, na.rm = TRUE),\n    Q3 = quantile(Score, probs = 0.75, na.rm = TRUE),\n    Median = median(Score, na.rm = TRUE)\n  )\n\n# Merge the quantiles back into the original long dataframe\nlong_df &lt;- left_join(long_df, quantiles, by = \"Subject\")\n\n# Create the plot with facet_wrap\nggplot(data = long_df, aes(x = Score)) +\n  geom_histogram(bins = 15, color = \"grey40\", fill = \"grey95\", linewidth = 0.5) + \n  geom_boxplot(width = 2000, fill = \"white\", color = \"black\", \n               outlier.colour = \"darkgoldenrod1\", outlier.fill = \"darkgoldenrod1\", outlier.size = 1, outlier.alpha = 0.2) +\n  facet_wrap(~factor(Subject, levels = c(\"Math\", \"Science\", \"Read\")), scales = \"free\", ncol = 1) + # Facet by subject in a single column\n  geom_vline(data = quantiles, aes(xintercept = Median, group = Subject), \n             color = \"darkred\", size = 0.5, linetype = \"dashed\") +\n  geom_label(data = quantiles, aes(x = Median, y = 12000, label = paste(\"Median=\", Median)), \n            color = \"darkred\", size = 3, hjust = \"right\") +\n  geom_segment(data = quantiles, aes(x = Q1, y = 0, xend = Q1, yend = 3000, group = Subject), \n               color = \"blue\", size = 0.5, linetype = \"dashed\") + # Draw line for Q1\n  geom_label(data = quantiles, aes(x = Q1, y = 3000, label = paste(\"Q1=\", Q1), group = Subject), \n             fill = \"white\", color = \"navyblue\", fontface = \"bold\", size = 2.5, hjust = \"right\") +\n  geom_segment(data = quantiles, aes(x = Q3, y = 0, xend = Q3, yend = 3000, group = Subject), \n               color = \"blue\", size = 0.5, linetype = \"dashed\") + # Draw line for Q3\n  geom_label(data = quantiles, aes(x = Q3, y = 3000, label = paste(\"Q3=\", Q3), group = Subject), \n             fill = \"white\", color = \"navyblue\", fontface = \"bold\", size = 2.5, hjust = \"left\") +\n  coord_cartesian(xlim = c(0, 1000)) +\n  scale_x_continuous(breaks=seq(0, 1000, by=200),  \n                     minor_breaks=seq(0, 1000, by=100))  +\n  labs(title = \"Distribution of scores by subject for students in Singapore\",\n       subtitle = \"Most students performed well in math, followed by science, then reading.\\nAll the distribution of performance shows relatively normal, without significant skewness.\",\n       x = \"Scores\",\n       y = \"Number of Students\",\n       caption = \"Data source: PISA 2022\") + \n  theme_minimal(base_size = 8) +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        strip.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 8, face = \"bold\"),\n        axis.text = element_text(size = 6))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#visualization-2-relationship-bw-gender-and-performance",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#visualization-2-relationship-bw-gender-and-performance",
    "title": "Take-home Exercise 2",
    "section": "Visualization 2: Relationship b/w Gender and Performance",
    "text": "Visualization 2: Relationship b/w Gender and Performance\n\nOriginal Design\nThis visualization is intended to showcase the relationship between gender and performance for each subject. This is done using density plots for each subject, colouring the overlapping density plots by gender\nThe original design is shown below.\n\n\n\nCritique\nClarity:\nWhy it is clear:\n\nThe use of different colors to represent different genders helps viewers to quickly identify which curve corresponds to which gender. Moreover, the legend clearly communicates the meaning of the colors, ensuring that there is no confusion about which color corresponds to which gender.\nFor the Math plot, the striped pattern fill draws attention to the areas where there is a notable difference between genders.\nFor the Math plot, the text annotations clearly point out key findings, such as which gender leads in mid or high scores.\nThe X and Y axes are clearly labeled with the subjects being compared (Science, Read, Math), and the y-axis is labeled with ‘Density’, which indicates the probability distribution of the scores.\n\nWhy it can be confusing:\n\nThe “2+1” layout places the three density plots on different Y-axis scales, making it difficult to visually compare across subjects.\nThe plot title is generic and does not capture the relationship between gender and performance.\nThe plot only uses the striped pattern for highlighting gender differences in the Math scores, but not for Science and Read. Consistency in design across all comparisons would provide a clearer and more cohesive visual message.\n\nAesthetics\nWhy it is beautiful:\n\nThe plots have a simple and clear color scheme, which provides good readability. The colored density plots against a white background with faint grid lines provides a clear contrast. The choice of contrasting colors in representing genders are intuitive.\nThe X and Y axes of each plot is clearly and consistently labeled.\nThe annotations are highlighted with colors that match those assigned to each gender, creating a visual connection that shows relevance of these elements to their respective genders.\n\nWhy it can be ugly:\n\nThe plot title seems to be the same size as the axes label, and is hence not the most prominent element. It should be more prominent than other elements since it provides an overview of the plot.\n\n\n\nSketch\nA sketch for the alternative design is shown below.\n\nTo improve clarity:\n\nThe density plots are arranged in a single column with the same Y-axis scale.\nTitles, subtitles and axis titles are added to provide context to the visual.\nAs additional improvement, the X-axis are configured to have labeled major tick marks every 200 units and minor, unlabeled tick marks every 100 units, providing a clear and detailed scale for the axis from 0 to 1000.\nRemoval of the striped pattern to show gender differences between distributions. The color of the distribution is clear enough in showing which distribution is higher.\n\nTo improve aesthetics:\n\nFont sizes and formatting of titles, subtitles and axis titles have been added to make the visualization look more professional.\nReplace annotations on plot with a descriptive title so that the visualization would not look cluttered with annotations for each density plot.\n\n\n\nRemake\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Calculate densities for each subject and gender\ndensities &lt;- list()\nsubjects &lt;- c(\"Math\", \"Read\", \"Science\")\ngenders &lt;- c(\"Female\", \"Male\")\n\nfor(subject in subjects) {\n  for(gender in genders) {\n    key &lt;- paste(subject, gender, sep = \"_\")\n    densities[[key]] &lt;- density(clean_short_df[[subject]][clean_short_df$GENDER == gender], n = 512)\n  }\n}\n\n# Create a data frame for shaded areas where one gender's density is higher than the other\nribbon_data &lt;- list()\n\nfor(subject in subjects) {\n  female_key &lt;- paste(subject, \"Female\", sep = \"_\")\n  male_key &lt;- paste(subject, \"Male\", sep = \"_\")\n  \n  # Create a comparison to find where female density is greater than male density\n  female_greater &lt;- densities[[female_key]]$y &gt; densities[[male_key]]$y\n  male_greater &lt;- densities[[male_key]]$y &gt; densities[[female_key]]$y\n  \n  ribbon_data[[subject]] &lt;- data.frame(\n    x = densities[[female_key]]$x,\n    ymin = ifelse(female_greater, densities[[male_key]]$y, densities[[female_key]]$y),\n    ymax = ifelse(female_greater, densities[[female_key]]$y, densities[[male_key]]$y),\n    gender = ifelse(female_greater, \"Female\", \"Male\")\n  )\n}\n\n# Bind all ribbon data together\nribbon_data &lt;- do.call(rbind, ribbon_data)\nribbon_data$Subject &lt;- factor(rep(subjects, each = length(densities[[paste(subjects[1], genders[1], sep = \"_\")]]$x)), levels = subjects)\n\n# Reshape the original data from wide to long format\nlong_df &lt;- clean_short_df %&gt;%\n  pivot_longer(cols = c(\"Math\", \"Read\", \"Science\"), names_to = \"Subject\", values_to = \"Score\")\n\n# Plot with facet_wrap\nggplot(data = long_df, aes(x = Score, fill = GENDER)) +\n  geom_density(alpha = 0.4) +\n  geom_ribbon(data = ribbon_data, aes(x = x, ymin = ymin, ymax = ymax, fill = gender), alpha = 0.6) +\n  facet_wrap(~factor(Subject, levels = c(\"Math\", \"Science\", \"Read\")), scales = \"free_x\", ncol = 1)  + # Facet by subject in a single column\n  scale_fill_manual(values = c(\"Male\" = \"skyblue3\",\"Female\" = \"lightpink\")) +\n  coord_cartesian(xlim = c(0, 1000), ylim = c(0, 0.005)) +\n  scale_x_continuous(breaks = seq(0, 1000, by = 200), minor_breaks = seq(0, 1000, by = 100)) +\n  labs(title = \"Distribution of scores across subjects by gender, for students in Singapore\",\n       subtitle = \"Male students generally had higher scores for math and science,\\nwhile females generally had higher scores in reading.\",\n       x = \"Scores\",\n       y = \"Density\",\n       caption = \"Data source: PISA 2022\") +\n  theme_minimal(base_size = 8) +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        strip.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 8, face = \"bold\"),\n        axis.text = element_text(size = 6))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#visualization-3-relationship-bw-school-types-and-performance",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#visualization-3-relationship-bw-school-types-and-performance",
    "title": "Take-home Exercise 2",
    "section": "Visualization 3: Relationship b/w school types and performance",
    "text": "Visualization 3: Relationship b/w school types and performance\n\nOriginal Design\nThis visualization is intended to showcase the relationship between school type and performance for each subject. For each subject, two boxplots representing the two types of school (public and private) are plotted.\nThe original design is shown below.\n\n\n\nCritique\nClarity:\nWhy it is clear:\n\nThe layout is consistent across the three plots of the subjects, which facilitates quick comparison across the same two categories of school type (Private/Secondary vs. Public/Secondary).\nNotches were used to make the differences in median of school types clearer.\nThe axes are clearly labeled, with the subjects listed on the y-axis and the school types on the x-axis.\n\nWhy it can be confusing:\n\nThe boxplots are on different Y-axis scales, making it difficult to visually compare across subjects or interpret the variation clearly.\nThe plot title is generic, and does not convey the relationship between school type and performance.\n\nAesthetics\nWhy it is beautiful:\n\nThe plots have a simple and clear color scheme, which provides good readability.\n\nWhy it can be ugly:\n\nThe sideways orientation of the titles for each subject (Math, Reading, and Science) is challenging to read and also not prominent.\nThe plot title seems to be the same size as the axes label, and is hence not the most prominent element. It should be more prominent than other elements since it provides an overview of the plot.\nAlthough the boxplots for the two school types are indicated on the X-axis, there was no color or pattern distinction that could help the viewer to differentiate them easily.\n\n\n\nSketch\nA sketch for the alternative design is shown below.\n\nTo improve clarity:\n\nThe Y-axes of the boxplots are placed on consistent scales starting at 0.\nTitles, subtitles and axis titles are added to provide context to the visual.\nAs additional improvement, the Y-axis are configured to have labeled major tick marks every 200 units and minor, unlabeled tick marks every 100 units, providing a clear and detailed scale for the axis from 0 to 1000.\n\nTo improve aesthetics:\n\nThe titles for each subject are displayed horizontally to make it easier to read.\nFont sizes and formatting of titles, subtitles and axis titles have been added to make the visualization look more professional.\nThe school types are given different colors to make differentiating between the two school types easier.\n\n\n\nRemake\n\nThe PlotThe Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Reshape the data from wide to long format\nlong_df &lt;- clean_short_df %&gt;%\n  pivot_longer(cols = c(\"Math\", \"Science\", \"Read\"), names_to = \"Subject\", values_to = \"Score\") %&gt;%\n  mutate(SCHOOL = recode(SCHOOL, \n                         \"Private/Secondary\" = \"Private schools\", \n                         \"Public/Secondary\" = \"Public schools\"))\n\n# Plot with facet_wrap ensuring the same y-axis scale\nggplot(data = long_df, aes(y = Score, x = SCHOOL, fill = SCHOOL)) +\n  geom_boxplot(notch = TRUE) +\n  facet_wrap(~factor(Subject, levels = c(\"Math\", \"Science\", \"Read\")), scales = \"free_x\")  + # Facet by subject in a single column. free_x allows different x-axis per facet (for different school types)\n  coord_cartesian(ylim = c(0, 1000)) +\n  scale_y_continuous(breaks=seq(0, 1000, by=200),  \n                     minor_breaks=seq(0, 1000, by=100))  +\n  scale_fill_manual(values = c(\"Private schools\" = \"plum\", \"Public schools\" = \"palegreen\")) +\n  labs(title = \"Distribution of scores across subjects by school type, for students in Singapore\",\n       subtitle = \"Students from private school perform better than those from public school in all the subjects,\\nand public schools had a greater variation in student performance across all subjects.\",\n       x = \"School Types\",\n       y = \"Scores\",\n       caption = \"Data source: PISA 2022\") +\n  theme_minimal(base_size = 8) +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        strip.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 8, face = \"bold\"),\n        axis.text = element_text(size = 6))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#visualization-4-relationship-bw-socioeconomic-status-and-performance",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#visualization-4-relationship-bw-socioeconomic-status-and-performance",
    "title": "Take-home Exercise 2",
    "section": "Visualization 4: Relationship b/w socioeconomic status and performance",
    "text": "Visualization 4: Relationship b/w socioeconomic status and performance\n\nOriginal Design\nThis visualization is intended to showcase the relationship between socioeconomic status and performance for each subject.\nThe original design is shown below.\n\n\n\nCritique\nClarity:\nWhy it is clear:\n\nPlacing the boxplots for each subject in a single row allows viewers to easily compare the categories side-by-side.\n\nWhy it can be confusing:\n\nThe order of the categorical variables is not intuitive or sorted, since it goes from High -&gt; Low -&gt; Lower-Middle -&gt; Upper-Middle.\nThe scale for Y-axis is inconsistent across the plots. This makes comparing across the subjects inaccurate.\nThe plot title is generic and does not convey the implications of the ESC status on performance.\n\nAesthetics\nWhy it is beautiful:\n\nThe design of boxplots for each category and for the subjects is consistent.\n\nWhy it can be ugly:\n\nThe sideways orientation of the titles for each subject (Math, Reading, and Science) is challenging to read and also not prominent.\nThe X-axis title is flushed to the left of the plot, which seems disconnected from the categories that it labels.\nThe boxplots do not use color or shading to differentiate between the ESCS statuses. Using different colors or patterns could help distinguish the groups more clearly.\nThe plot title seems to be the same size as the axes label, and is hence not the most prominent element. It should be more prominent than other elements since it provides an overview of the plot.\n\n\n\nSketch\nA sketch for the alternative design is shown below.\n\nTo improve clarity:\n\nThe categorical variables for ESC status are ordered, making the plot more intuitive while also helping to clearly communicate the influence of status on performance.\nThe Y-axes of the boxplots are placed on consistent scales starting at 0.\nTitles, subtitles and axis titles are added to provide context to the visual.\nAs additional improvement, the Y-axis are configured to have labeled major tick marks every 200 units and minor, unlabeled tick marks every 100 units, providing a clear and detailed scale for the axis from 0 to 1000.\n\nTo improve aesthetics:\n\nThe titles for each subject are displayed horizontally to make it easier to read.\nThe X-axis title is centered on the plot to show that the title pertains to the data directly above it.\nThe ESC statuses are given different colors to make differentiating between the statuses easier. Additionally choosing the same colour in different hues helps to convey the order of the ESC statuses.\nFont sizes and formatting of titles, subtitles and axis titles have been added to make the visualization look more professional.\n\n\n\nRemake\n\nThe PlotThe Code\n\n\n\n# Reshape the data from wide to long format\nlong_df &lt;- na.omit(clean_short_df) %&gt;%\n  pivot_longer(cols = c(\"Math\", \"Read\", \"Science\"), names_to = \"Subject\", values_to = \"Scores\")\n\n# Order the ESC_status factor in the desired order\nlong_df$ESC_status &lt;- factor(long_df$ESC_status, levels = c(\"Low\", \"Lower-Middle\", \"Upper-Middle\", \"High\"))\n\n# Plot with facet_wrap for all three subjects\nggplot(data = long_df, aes(y = Scores, x = ESC_status, fill = ESC_status)) +\n  geom_boxplot(notch = TRUE) +\n  facet_wrap(~factor(Subject, levels = c(\"Math\", \"Science\", \"Read\")), scales = \"free_x\") +\n  scale_fill_brewer(palette = \"Blues\") + # Adds color to the boxplots\n  coord_cartesian(ylim = c(0, 1000)) +\n  scale_y_continuous(breaks=seq(0, 1000, by=200),  \n                     minor_breaks=seq(0, 1000, by=100))  +\n  labs(title = \"Distribution of scores across subjects by socioeconomic status,\\nfor students in Singapore\",\n       subtitle = \"Students from more advantaged socioeconomic backgrounds tend to have higher performance scores.\",\n       x = \"Socioeconomic Status\",\n       y = \"Scores\",\n       caption = \"Data source: PISA 2022\") +\n  theme_minimal(base_size = 8) +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        strip.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 8, face = \"bold\"),\n        axis.text = element_text(size = 6),\n        legend.position = \"bottom\",\n        legend.title = element_text(face = \"bold\")) \n\n\n\n\n\n\n\n\n\n\n\n# Reshape the data from wide to long format\nlong_df &lt;- na.omit(clean_short_df) %&gt;%\n  pivot_longer(cols = c(\"Math\", \"Read\", \"Science\"), names_to = \"Subject\", values_to = \"Scores\")\n\n# Order the ESC_status factor in the desired order\nlong_df$ESC_status &lt;- factor(long_df$ESC_status, levels = c(\"Low\", \"Lower-Middle\", \"Upper-Middle\", \"High\"))\n\n# Plot with facet_wrap for all three subjects\nggplot(data = long_df, aes(y = Scores, x = ESC_status, fill = ESC_status)) +\n  geom_boxplot(notch = TRUE) +\n  facet_wrap(~factor(Subject, levels = c(\"Math\", \"Science\", \"Read\")), scales = \"free_x\") +\n  scale_fill_brewer(palette = \"Blues\") + # Adds color to the boxplots\n  coord_cartesian(ylim = c(0, 1000)) +\n  scale_y_continuous(breaks=seq(0, 1000, by=200),  \n                     minor_breaks=seq(0, 1000, by=100))  +\n  labs(title = \"Distribution of scores across subjects by socioeconomic status,\\nfor students in Singapore\",\n       subtitle = \"Students from more advantaged socioeconomic backgrounds tend to have higher performance scores.\",\n       x = \"Socioeconomic Status\",\n       y = \"Scores\",\n       caption = \"Data source: PISA 2022\") +\n  theme_minimal(base_size = 8) +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10),\n        strip.text = element_text(size = 10, face = \"bold\"),\n        axis.title = element_text(size = 8, face = \"bold\"),\n        axis.text = element_text(size = 6),\n        legend.position = \"bottom\",\n        legend.title = element_text(face = \"bold\"))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#summary-and-conclusion",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#summary-and-conclusion",
    "title": "Take-home Exercise 2",
    "section": "Summary and conclusion",
    "text": "Summary and conclusion\nIn this exercise, we performed the first peer data visualization makeover. Focusing on the clarity and aesthetics while looking at my peer’s visualizations made me more aware of details such as hierarchy of visual elements (e.g. titles should draw readers’ attention), being intuitive in designing visualizations (e.g. categorical variables should be ordered in a sensible way). I better understand the impact of thoughtful design on making visualizations compelling, clear and aesthetically pleasing. It also sharpened my skills in using the relevant libraries for making visualizations."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#references",
    "href": "Take-home_Ex/Take-home_Ex2/Take-home_Ex2.html#references",
    "title": "Take-home Exercise 2",
    "section": "References",
    "text": "References\n\nTake-home Exercise 1: Study on Singapore students’ performance by WEI YANRUI"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "",
    "text": "In this exercise I will learn how to plot functional and truthful choropleth maps by using an R package called tmap package.\n\n\n\nWe will use the tmap package that can create thematic maps, such as choropleths and bubble maps; and the sf package for handling geospatial data.\n\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\nWe first use the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\spacebun\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex7\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nStudent’s Notes\n\n\n\nThere are 323 features (rows) and 15 fields (columns).\n\n\nLet’s examine the content and first ten records of mpsz:\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\n\n\n\nStudent’s Notes\n\n\n\nEach feature could represent a different area, like a subzone or district. Alongside spatial information, each feature has 15 additional attributes or fields. These fields contain non-spatial data associated with each feature, such as names, codes, and other identifiers.\nGeometry type: MULTIPOLYGON: Each feature’s geometry is stored as a MULTIPOLYGON, indicating that the spatial data for each area consists of one or more polygons. A polygon is a shape enclosed by a boundary that defines the area of the feature. MULTIPOLYGONs are used for complex shapes that might consist of several disjointed parts.\nDimension: XY This indicates that the spatial data is in two dimensions, using X and Y coordinates to represent locations on a plane. This is typical for most mapping and GIS applications.\nBounding box: xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33 The bounding box gives you the minimum and maximum coordinates of the dataset in the X (longitude) and Y (latitude) dimensions. This provides a rough idea of the spatial extent covered by your data.\nProjected CRS: SVY21 The Coordinate Reference System defines how the two-dimensional, flat map relates to real places on the Earth. SVY21 is a specific type of CRS that is likely tailored for a particular region e.g. Singapore.\n\n\n\n\n\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\nstr(popdata)\n\nspc_tbl_ [984,656 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PA  : chr [1:984656] \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" ...\n $ SZ  : chr [1:984656] \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" ...\n $ AG  : chr [1:984656] \"0_to_4\" \"0_to_4\" \"0_to_4\" \"0_to_4\" ...\n $ Sex : chr [1:984656] \"Males\" \"Males\" \"Males\" \"Males\" ...\n $ TOD : chr [1:984656] \"HDB 1- and 2-Room Flats\" \"HDB 3-Room Flats\" \"HDB 4-Room Flats\" \"HDB 5-Room and Executive Flats\" ...\n $ Pop : num [1:984656] 0 10 30 50 0 0 40 0 0 10 ...\n $ Time: num [1:984656] 2011 2011 2011 2011 2011 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PA = col_character(),\n  ..   SZ = col_character(),\n  ..   AG = col_character(),\n  ..   Sex = col_character(),\n  ..   TOD = col_character(),\n  ..   Pop = col_double(),\n  ..   Time = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n\n\n\n\nStudent’s Notes\n\n\n\nData consists of 984,656 rows and 7 columns.\nRecall that PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile. It seems like\n\nPA corresponds to PLN_AREA_N\nSZ corresponds to SUBZONE_N\n\nPA: Area / Region in SG SZ: Specific area within PA AG: Age group Sex: Gender TOD: Type of Dwelling Pop: Population for this age group for this area and TOD. Time: Year of info\n\n\n\n\n\nWe first need to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;% # Filter data for year 2020\n  group_by(PA, SZ, AG) %&gt;% # Group by PA SZ and AG. Not grouped: Sex , TOD. So we will see sum of age across each TOD and Gender for group. \n  summarise(`POP` = sum(`Pop`)) %&gt;% # Allows us to see population for each of the group, adding up for all type of TOD and Gender. \n  ungroup() %&gt;% \n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;% # One row per combi of PA and SZ. Add columns for the Age Groups. Each value is the sum Pop for that age group. \n  mutate(YOUNG = rowSums(.[3:6]) # Create new column YOUNG that calculates sum of values in columns 3 to 6 for each row. The . notation is a placeholder that refers to the current data frame being manipulated. \n         +rowSums(.[12])) %&gt;% # This is age group 5-9 which has weird position in column 12. \nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+ # Create new col again \nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%  # Create new col again \nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  # Create new col again but sum across all age groups\nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`) # New col for the ratio between young and aged against economy active group\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, # Used to only select relevant cols so all the age Group cols are dropped. \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n# popdata2020 &lt;- popdata2020 %&gt;%\n#   mutate_at(.vars = vars(PA, SZ),  # .vars argument specifies the variables (or columns) on which the function specified by .funs will be applied. \n#           .funs = funs(toupper)) %&gt;% # .funs argument specifies the function(s) to apply to the selected variables. toupper function will be applied to each of the selected variables.\n#   filter(`ECONOMY ACTIVE` &gt; 0) # only include those where the value of the ECONOMY ACTIVE column is greater than 0.\n\n# As `funs()` was deprecated in dplyr 0.8.0, we will use this\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate(across(.cols = c(PA, SZ), .fns = toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\n\n\n\n\nStudent’s Notes\n\n\n\nThere is no more SZ column in the dataframe mpsz_pop2020.\nIf the popdata2020 data does not have a specified SZ that corresponds to SUBZONE_N, the values for PA, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY columns will reflect as NA.\n\n\nSave output to .rds:\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\n\n\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\") # Set the tmap mode: \"plot\" or \"view\"\nqtm(mpsz_pop2020, # qtm is a quick plotting method to Plot a thematic map\n    fill = \"DEPENDENCY\") # either a color to fill the polygons, or name of the data variable in shp to draw a choropleth. Only applicable when shp contains polygons.  Set fill = NULL to draw only polygon borders. \n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")  \nqtm(mpsz_pop2020,\n    fill = NULL) # Set fill = NULL to draw only polygon borders. \n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map.\n\nFor interactive mode, “view” option should be used. If the mode is set to “view”, the map is shown interactively as an htmlwidget. However it does not work for this data.\n\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control.\nTo draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+ # Creates a tmap-element that specifies a spatial data object, which we refer to as shape. Also the projection and covered area (bounding box) can be set. It is possible to use multiple shape objects within one plot (see tmap-element).\n  tm_fill(\"DEPENDENCY\",  # AES Layer. Create a polygon layer (without borders)\n          style = \"quantile\",  #  define a data classification method. \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\", # layout element. Adjust the layout (main function)\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) + # AES Layer. Create polygon borders.\n  tm_compass(type=\"8star\", size = 2) + #Attribute. Create a map compass\n  tm_scale_bar() + # Attribute. Create a scale bar\n  tm_grid(alpha =0.2) + # Attribute. Create grid lines\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) # Attribute. Create a text for credits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudent’s Notes\n\n\n\nTrying tm_facets() wil argument along set to “SUBZONE_N” created a lot of plots with each plot being a specified area.\n\n\n\n\nThe basic building block of tmap is tm_shape(), followed by one or more layer elements such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to the aes layer tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided later on.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme later on\nBy default, missing values will be shaded in grey.\n\n\n\n\n\nActually, tm_polygons() is a wrapper of tm_fill() and tm_border().\ntm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\n\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\n\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\n\n\n\n\nStudent’s Notes\n\n\n\nFrom documentation on style\nThis is a method to process the color scale when col is a numeric variable.\nDiscrete gradient options are “cat”, “fixed”, “sd”, “equal”, “pretty”, “quantile”, “kmeans”, “hclust”, “bclust”, “fisher”, “jenks”, “dpih”, “headtails”, and “log10_pretty”.\nA numeric variable is processed as a categorical variable when using “cat”, i.e. each unique value will correspond to a distinct category. For the other discrete gradient options (except “log10_pretty”), see the details in classIntervals (extra arguments can be passed on via style.args). Continuous gradient options are “cont”, “order”, and “log10”. The first maps the values of col to a smooth gradient, the second maps the order of values of col to a smooth gradient, and the third uses a logarithmic transformation.\n\n\nMore on some of the styles from classIntervals:\n\nThe “equal” style divides the range of the variable into n parts.\nThe “pretty” style chooses a number of breaks not necessarily equal to n using pretty, but likely to be legible; arguments to pretty may be passed through ....\nThe “quantile” style provides quantile breaks; arguments to quantile may be passed through ....\nThe “kmeans” style uses kmeans to generate the breaks; it may be anchored using set.seed; the pars attribute returns the kmeans object generated; if kmeans fails, a jittered input vector containing rtimes replications of var is tried — with few unique values in var, this can prove necessary; arguments to kmeans may be passed through ....\nThe “hclust” style uses hclust to generate the breaks using hierarchical clustering; the pars attribute returns the hclust object generated, and can be used to find other breaks using getHclustClassIntervals; arguments to hclust may be passed through ....\nThe “bclust” style uses bclust to generate the breaks using bagged clustering; it may be anchored using set.seed; the pars attribute returns the bclust object generated, and can be used to find other breaks using getBclustClassIntervals; if bclust fails, a jittered input vector containing rtimes replications of var is tried — with few unique values in var, this can prove necessary; arguments to bclust may be passed through ....\nThe “fisher” style uses the algorithm proposed by W. D. Fisher (1958) and discussed by Slocum et al. (2005) as the Fisher-Jenks algorithm; added here thanks to Hisaji Ono. This style will subsample by default for more than 3000 observations. This style should always be preferred to “jenks” as it uses the original Fortran code and runs nested for-loops much faster.\nThe “jenks” style has been ported from Jenks’ code, and has been checked for consistency with ArcView, ArcGIS, and MapInfo (with some remaining differences); added here thanks to Hisaji Ono (originally reported as Basic, now seen as Fortran (as described in a talk last seen at http://www.irlogi.ie/wp-content/uploads/2016/11/NUIM_ChoroHarmful.pdf, slides 26-27)). Note that the sense of interval closure is reversed from the other styles, and in this implementation has to be right-closed - use cutlabels=TRUE in findColours on the object returned to show the closure clearly, and use findCols to extract the classes for each value. This style will subsample by default for more than 3000 observations.\nThe “dpih” style uses the dpih() function from KernSmooth (Wand, 1995) implementing direct plug-in methodology to select the bin width of a histogram.\nThe “headtails” style uses the algorithm proposed by Bin Jiang (2013), in order to find groupings or hierarchy for data with a heavy-tailed distribution. This classification scheme partitions all of the data values around the mean into two parts and continues the process iteratively for the values (above the mean) in the head until the head part values are no longer heavy-tailed distributed. Thus, the number of classes and the class intervals are both naturally determined. By default the algorithm uses thr = 0.4, meaning that when the head represents more than 40% of the observations the distribution is not considered heavy-tailed.\n\n\n\nThe code chunk below shows a quantile data classification that uses 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used. Notice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") + # divides the range of the variable into 5 parts\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nWarning: Maps Lie!\n\n\nDIY: Using what you havelearned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\n\n\npretty\n\nSeems similar to equal method\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"pretty\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nkmeans\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nhclust\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\n\n\nn = 5\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nn = 4\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nn = 3\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 3,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nMap layout refers to the combination of all map elements into a cohensive map.\nMap elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios.\nColour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the cobalt style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"cobalt\")\n\n\n\n\n\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nIn this example, small multiple choropleth maps are created by defining columns in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), # Apply different style. \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", # specify the column to group by here. \n            free.coords=TRUE, # Set free.coords to TRUE to zoom into that region\n            drop.units=FALSE) + # Set drop.units to FALSE to retain the other shapes. \n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntmap_mode(\"plot\")\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nTesting tmap_mode(“view”) which make plots interactive.\n\ntmap_mode(\"view\")\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5) +\n  tmap_options(check.and.fix = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#overview",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "",
    "text": "In this exercise I will learn how to plot functional and truthful choropleth maps by using an R package called tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#loading-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#loading-libraries",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "",
    "text": "We will use the tmap package that can create thematic maps, such as choropleths and bubble maps; and the sf package for handling geospatial data.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#importing-data",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "",
    "text": "Two data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\nWe first use the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\spacebun\\ISSS608-VAA\\Hands-on_Ex\\Hands-on_Ex7\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nStudent’s Notes\n\n\n\nThere are 323 features (rows) and 15 fields (columns).\n\n\nLet’s examine the content and first ten records of mpsz:\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n\n\n\n\nStudent’s Notes\n\n\n\nEach feature could represent a different area, like a subzone or district. Alongside spatial information, each feature has 15 additional attributes or fields. These fields contain non-spatial data associated with each feature, such as names, codes, and other identifiers.\nGeometry type: MULTIPOLYGON: Each feature’s geometry is stored as a MULTIPOLYGON, indicating that the spatial data for each area consists of one or more polygons. A polygon is a shape enclosed by a boundary that defines the area of the feature. MULTIPOLYGONs are used for complex shapes that might consist of several disjointed parts.\nDimension: XY This indicates that the spatial data is in two dimensions, using X and Y coordinates to represent locations on a plane. This is typical for most mapping and GIS applications.\nBounding box: xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33 The bounding box gives you the minimum and maximum coordinates of the dataset in the X (longitude) and Y (latitude) dimensions. This provides a rough idea of the spatial extent covered by your data.\nProjected CRS: SVY21 The Coordinate Reference System defines how the two-dimensional, flat map relates to real places on the Earth. SVY21 is a specific type of CRS that is likely tailored for a particular region e.g. Singapore.\n\n\n\n\n\nNext, we will import respopagsex2011to2020.csv file into RStudio and save the file into an R dataframe called popagsex.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\nstr(popdata)\n\nspc_tbl_ [984,656 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PA  : chr [1:984656] \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" \"Ang Mo Kio\" ...\n $ SZ  : chr [1:984656] \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" \"Ang Mo Kio Town Centre\" ...\n $ AG  : chr [1:984656] \"0_to_4\" \"0_to_4\" \"0_to_4\" \"0_to_4\" ...\n $ Sex : chr [1:984656] \"Males\" \"Males\" \"Males\" \"Males\" ...\n $ TOD : chr [1:984656] \"HDB 1- and 2-Room Flats\" \"HDB 3-Room Flats\" \"HDB 4-Room Flats\" \"HDB 5-Room and Executive Flats\" ...\n $ Pop : num [1:984656] 0 10 30 50 0 0 40 0 0 10 ...\n $ Time: num [1:984656] 2011 2011 2011 2011 2011 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PA = col_character(),\n  ..   SZ = col_character(),\n  ..   AG = col_character(),\n  ..   Sex = col_character(),\n  ..   TOD = col_character(),\n  ..   Pop = col_double(),\n  ..   Time = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n\n\n\n\nStudent’s Notes\n\n\n\nData consists of 984,656 rows and 7 columns.\nRecall that PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile. It seems like\n\nPA corresponds to PLN_AREA_N\nSZ corresponds to SUBZONE_N\n\nPA: Area / Region in SG SZ: Specific area within PA AG: Age group Sex: Gender TOD: Type of Dwelling Pop: Population for this age group for this area and TOD. Time: Year of info\n\n\n\n\n\nWe first need to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;% # Filter data for year 2020\n  group_by(PA, SZ, AG) %&gt;% # Group by PA SZ and AG. Not grouped: Sex , TOD. So we will see sum of age across each TOD and Gender for group. \n  summarise(`POP` = sum(`Pop`)) %&gt;% # Allows us to see population for each of the group, adding up for all type of TOD and Gender. \n  ungroup() %&gt;% \n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;% # One row per combi of PA and SZ. Add columns for the Age Groups. Each value is the sum Pop for that age group. \n  mutate(YOUNG = rowSums(.[3:6]) # Create new column YOUNG that calculates sum of values in columns 3 to 6 for each row. The . notation is a placeholder that refers to the current data frame being manipulated. \n         +rowSums(.[12])) %&gt;% # This is age group 5-9 which has weird position in column 12. \nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+ # Create new col again \nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%  # Create new col again \nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  # Create new col again but sum across all age groups\nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`) # New col for the ratio between young and aged against economy active group\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, # Used to only select relevant cols so all the age Group cols are dropped. \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n# popdata2020 &lt;- popdata2020 %&gt;%\n#   mutate_at(.vars = vars(PA, SZ),  # .vars argument specifies the variables (or columns) on which the function specified by .funs will be applied. \n#           .funs = funs(toupper)) %&gt;% # .funs argument specifies the function(s) to apply to the selected variables. toupper function will be applied to each of the selected variables.\n#   filter(`ECONOMY ACTIVE` &gt; 0) # only include those where the value of the ECONOMY ACTIVE column is greater than 0.\n\n# As `funs()` was deprecated in dplyr 0.8.0, we will use this\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate(across(.cols = c(PA, SZ), .fns = toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\n\n\n\n\nStudent’s Notes\n\n\n\nThere is no more SZ column in the dataframe mpsz_pop2020.\nIf the popdata2020 data does not have a specified SZ that corresponds to SUBZONE_N, the values for PA, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY columns will reflect as NA.\n\n\nSave output to .rds:\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "",
    "text": "Two approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\") # Set the tmap mode: \"plot\" or \"view\"\nqtm(mpsz_pop2020, # qtm is a quick plotting method to Plot a thematic map\n    fill = \"DEPENDENCY\") # either a color to fill the polygons, or name of the data variable in shp to draw a choropleth. Only applicable when shp contains polygons.  Set fill = NULL to draw only polygon borders. \n\n\n\n\n\n\n\n\n\ntmap_mode(\"plot\")  \nqtm(mpsz_pop2020,\n    fill = NULL) # Set fill = NULL to draw only polygon borders. \n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map.\n\nFor interactive mode, “view” option should be used. If the mode is set to “view”, the map is shown interactively as an htmlwidget. However it does not work for this data.\n\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n\n\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control.\nTo draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+ # Creates a tmap-element that specifies a spatial data object, which we refer to as shape. Also the projection and covered area (bounding box) can be set. It is possible to use multiple shape objects within one plot (see tmap-element).\n  tm_fill(\"DEPENDENCY\",  # AES Layer. Create a polygon layer (without borders)\n          style = \"quantile\",  #  define a data classification method. \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\", # layout element. Adjust the layout (main function)\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) + # AES Layer. Create polygon borders.\n  tm_compass(type=\"8star\", size = 2) + #Attribute. Create a map compass\n  tm_scale_bar() + # Attribute. Create a scale bar\n  tm_grid(alpha =0.2) + # Attribute. Create grid lines\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) # Attribute. Create a text for credits\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudent’s Notes\n\n\n\nTrying tm_facets() wil argument along set to “SUBZONE_N” created a lot of plots with each plot being a specified area.\n\n\n\n\nThe basic building block of tmap is tm_shape(), followed by one or more layer elements such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons.\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to the aes layer tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided later on.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme later on\nBy default, missing values will be shaded in grey.\n\n\n\n\n\nActually, tm_polygons() is a wrapper of tm_fill() and tm_border().\ntm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\n\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\n\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n\n\n\n\n\nStudent’s Notes\n\n\n\nFrom documentation on style\nThis is a method to process the color scale when col is a numeric variable.\nDiscrete gradient options are “cat”, “fixed”, “sd”, “equal”, “pretty”, “quantile”, “kmeans”, “hclust”, “bclust”, “fisher”, “jenks”, “dpih”, “headtails”, and “log10_pretty”.\nA numeric variable is processed as a categorical variable when using “cat”, i.e. each unique value will correspond to a distinct category. For the other discrete gradient options (except “log10_pretty”), see the details in classIntervals (extra arguments can be passed on via style.args). Continuous gradient options are “cont”, “order”, and “log10”. The first maps the values of col to a smooth gradient, the second maps the order of values of col to a smooth gradient, and the third uses a logarithmic transformation.\n\n\nMore on some of the styles from classIntervals:\n\nThe “equal” style divides the range of the variable into n parts.\nThe “pretty” style chooses a number of breaks not necessarily equal to n using pretty, but likely to be legible; arguments to pretty may be passed through ....\nThe “quantile” style provides quantile breaks; arguments to quantile may be passed through ....\nThe “kmeans” style uses kmeans to generate the breaks; it may be anchored using set.seed; the pars attribute returns the kmeans object generated; if kmeans fails, a jittered input vector containing rtimes replications of var is tried — with few unique values in var, this can prove necessary; arguments to kmeans may be passed through ....\nThe “hclust” style uses hclust to generate the breaks using hierarchical clustering; the pars attribute returns the hclust object generated, and can be used to find other breaks using getHclustClassIntervals; arguments to hclust may be passed through ....\nThe “bclust” style uses bclust to generate the breaks using bagged clustering; it may be anchored using set.seed; the pars attribute returns the bclust object generated, and can be used to find other breaks using getBclustClassIntervals; if bclust fails, a jittered input vector containing rtimes replications of var is tried — with few unique values in var, this can prove necessary; arguments to bclust may be passed through ....\nThe “fisher” style uses the algorithm proposed by W. D. Fisher (1958) and discussed by Slocum et al. (2005) as the Fisher-Jenks algorithm; added here thanks to Hisaji Ono. This style will subsample by default for more than 3000 observations. This style should always be preferred to “jenks” as it uses the original Fortran code and runs nested for-loops much faster.\nThe “jenks” style has been ported from Jenks’ code, and has been checked for consistency with ArcView, ArcGIS, and MapInfo (with some remaining differences); added here thanks to Hisaji Ono (originally reported as Basic, now seen as Fortran (as described in a talk last seen at http://www.irlogi.ie/wp-content/uploads/2016/11/NUIM_ChoroHarmful.pdf, slides 26-27)). Note that the sense of interval closure is reversed from the other styles, and in this implementation has to be right-closed - use cutlabels=TRUE in findColours on the object returned to show the closure clearly, and use findCols to extract the classes for each value. This style will subsample by default for more than 3000 observations.\nThe “dpih” style uses the dpih() function from KernSmooth (Wand, 1995) implementing direct plug-in methodology to select the bin width of a histogram.\nThe “headtails” style uses the algorithm proposed by Bin Jiang (2013), in order to find groupings or hierarchy for data with a heavy-tailed distribution. This classification scheme partitions all of the data values around the mean into two parts and continues the process iteratively for the values (above the mean) in the head until the head part values are no longer heavy-tailed distributed. Thus, the number of classes and the class intervals are both naturally determined. By default the algorithm uses thr = 0.4, meaning that when the head represents more than 40% of the observations the distribution is not considered heavy-tailed.\n\n\n\nThe code chunk below shows a quantile data classification that uses 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nIn the code chunk below, equal data classification method is used. Notice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") + # divides the range of the variable into 5 parts\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nWarning: Maps Lie!\n\n\nDIY: Using what you havelearned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\n\n\npretty\n\nSeems similar to equal method\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"pretty\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nkmeans\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nhclust\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\n\n\nn = 5\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nn = 4\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nn = 3\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 3,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nMap layout refers to the combination of all map elements into a cohensive map.\nMap elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios.\nColour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the cobalt style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"cobalt\")\n\n\n\n\n\n\n\n\n\n\n\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\n\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n\nIn this example, small multiple choropleth maps are created by defining columns in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), # Apply different style. \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", # specify the column to group by here. \n            free.coords=TRUE, # Set free.coords to TRUE to zoom into that region\n            drop.units=FALSE) + # Set drop.units to FALSE to retain the other shapes. \n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntmap_mode(\"plot\")\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\nTesting tmap_mode(“view”) which make plots interactive.\n\ntmap_mode(\"view\")\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5) +\n  tmap_options(check.and.fix = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#overview-1",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#overview-1",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nProportional symbol maps (also known as graduate symbol maps) are a class of maps that use the visual variable of size to represent differences in the magnitude of a discrete, abruptly changing phenomenon, e.g. counts of people. Like choropleth maps, you can create classed or unclassed versions of these maps. The classed ones are known as range-graded or graduated symbols, and the unclassed are called proportional symbols, where the area of the symbols are proportional to the values of the attribute being mapped.\nIn this hands-on exercise, I will learn how to create a proportional symbol map showing the number of wins by Singapore Pools’ outlets using an R package called tmap. I will learn:\n\nTo import an aspatial data file into R.\nTo convert it into simple point feature data frame and at the same time, to assign an appropriate projection reference to the newly create simple point feature data frame.\nTo plot interactive proportional symbol maps."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#loading-libraries-1",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#loading-libraries-1",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "2.2 Loading libraries",
    "text": "2.2 Loading libraries\nWe will use the tmap package that can create thematic maps, such as choropleths and bubble maps; and the sf package for handling geospatial data.\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#importing-data-1",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#importing-data-1",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "2.3 Importing data",
    "text": "2.3 Importing data"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "2.3 Geospatial Data Wrangling",
    "text": "2.3 Geospatial Data Wrangling\n\n2.3.1 Importing data\nThe data set use for this hands-on exercise is called SGPools_svy21. The data is in csv file format.\nFigure below shows the first 15 records of SGPools_svy21.csv. It consists of seven columns. The XCOORD and YCOORD columns are the x-coordinates and y-coordinates of SingPools outlets and branches. They are in Singapore SVY21 Projected Coordinates System.\n\nThe code chunk below uses read_csv() function of readr package to import SGPools_svy21.csv into R as a tibble data frame called sgpools.\n\nsgpools &lt;- read_csv(\"data/aspatial/SGPools_svy21.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly. The code chunk below shows list() is used to do the job.\nNotice that the sgpools data is in tibble data frame and not the common R data frame.\n\nlist(sgpools)\n\n[[1]]\n# A tibble: 306 × 7\n   NAME           ADDRESS POSTCODE XCOORD YCOORD `OUTLET TYPE` `Gp1Gp2 Winnings`\n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Mar… 2 Bayf…    18972 30842. 29599. Branch                        5\n 2 Livewire (Res… 26 Sen…    98138 26704. 26526. Branch                       11\n 3 SportsBuzz (K… Lotus …   738078 20118. 44888. Branch                        0\n 4 SportsBuzz (P… 1 Sele…   188306 29777. 31382. Branch                       44\n 5 Prime Serango… Blk 54…   552542 32239. 39519. Branch                        0\n 6 Singapore Poo… 1A Woo…   731001 21012. 46987. Branch                        3\n 7 Singapore Poo… Blk 64…   370064 33990. 34356. Branch                       17\n 8 Singapore Poo… Blk 88…   370088 33847. 33976. Branch                       16\n 9 Singapore Poo… Blk 30…   540308 33910. 41275. Branch                       21\n10 Singapore Poo… Blk 20…   560202 29246. 38943. Branch                       25\n# ℹ 296 more rows\n\n\n\n\n2.3.2 Creating a sf data frame from an aspatial data frame\nThe code chunk below converts sgpools data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\n\n\n\n\n\nStudent’s Notes\n\n\n\nFor our own project, we will likely need to do similarly, to convert our weather dataframe into a simple feature dataframe to plot geospatial data.\n\n\n\nsgpools_sf &lt;- st_as_sf(sgpools, \n                       coords = c(\"XCOORD\", \"YCOORD\"),\n                       crs= 3414)\n\nThings to learn from the arguments above:\n\nThe coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\nThe crs argument required you to provide the coordinates system in epsg format. EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by refering to epsg.io.\n\nFigure below shows the data table of sgpools_sf. Notice that a new column called geometry has been added into the data frame.\n\nYou can display the basic information of the newly created sgpools_sf by using the code chunk below.\nThe output shows that sgppols_sf is in POINT: point feature class. Its epsg ID is 3414.\nThe bounding box provides information of the extent of the geospatial data.\n\nlist(sgpools_sf)\n\n[[1]]\nSimple feature collection with 306 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 7844.194 ymin: 26525.7 xmax: 45176.57 ymax: 47987.13\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 306 × 6\n   NAME                         ADDRESS POSTCODE `OUTLET TYPE` `Gp1Gp2 Winnings`\n * &lt;chr&gt;                        &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;\n 1 Livewire (Marina Bay Sands)  2 Bayf…    18972 Branch                        5\n 2 Livewire (Resorts World Sen… 26 Sen…    98138 Branch                       11\n 3 SportsBuzz (Kranji)          Lotus …   738078 Branch                        0\n 4 SportsBuzz (PoMo)            1 Sele…   188306 Branch                       44\n 5 Prime Serangoon North        Blk 54…   552542 Branch                        0\n 6 Singapore Pools Woodlands C… 1A Woo…   731001 Branch                        3\n 7 Singapore Pools 64 Circuit … Blk 64…   370064 Branch                       17\n 8 Singapore Pools 88 Circuit … Blk 88…   370088 Branch                       16\n 9 Singapore Pools Anchorvale … Blk 30…   540308 Branch                       21\n10 Singapore Pools Ang Mo Kio … Blk 20…   560202 Branch                       25\n# ℹ 296 more rows\n# ℹ 1 more variable: geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#drawing-proportional-symbol-map",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#drawing-proportional-symbol-map",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "2.4 Drawing Proportional Symbol Map",
    "text": "2.4 Drawing Proportional Symbol Map\nTo create an interactive proportional symbol map in R, the view mode of tmap will be used.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\n\n2.4.1 It all started with an interactive point symbol map\nThe code chunk below is used to create an interactive point symbol map.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = 1,\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n2.4.2 Let’s make it proportional\nTo draw a proportional symbol map, we need to assign a numerical variable to the size visual attribute. The code chunks below show that the variable Gp1Gp2Winnings is assigned to size visual attribute.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"red\",\n           size = \"Gp1Gp2 Winnings\",\n           border.col = \"black\",\n           border.lwd = 1)\n\n\n\n\n\n\n\n2.4.3 Let’s give it a different colour\nThe proportional symbol map can be further improved by using the colour visual attribute. In the code chunks below, OUTLET_TYPE variable is used as the colour attribute variable.\n\ntm_shape(sgpools_sf)+\ntm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1)\n\n\n\n\n\n\n\n2.4.4 I have a twin brothers :)\nAn impressive and little-know feature of tmap’s view mode is that it also works with faceted plots. The argument sync in tm_facets() can be used in this case to produce multiple maps with synchronised zoom and pan settings.\n\ntm_shape(sgpools_sf) +\n  tm_bubbles(col = \"OUTLET TYPE\", \n          size = \"Gp1Gp2 Winnings\",\n          border.col = \"black\",\n          border.lwd = 1) +\n  tm_facets(by= \"OUTLET TYPE\",\n            nrow = 1,\n            sync = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBefore you end the session, it is wiser to switch tmap’s Viewer back to plot mode by using the code chunk below.\n\n\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#overview-2",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#overview-2",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nBy the end of this exercise, I will be able to use appropriate functions of tmap and tidyverse to perform the following tasks:\n\nImporting geospatial data in rds format into R environment.\nCreating cartographic quality choropleth maps by using appropriate tmap functions.\nCreating rate map\nCreating percentile map\nCreating boxmap"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#loading-libraries-2",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#loading-libraries-2",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "3.2 Loading libraries",
    "text": "3.2 Loading libraries\n\npacman::p_load(tmap, tidyverse, sf)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#importing-data-2",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#importing-data-2",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "3.3 Importing data",
    "text": "3.3 Importing data\nFor the purpose of this hands-on exercise, a prepared data set called NGA_wp.rds will be used. The data set is a polygon feature data.frame providing information on water point of Nigeria at the LGA level. You can find the data set in the rds sub-direct of the hands-on data folder.\n\nNGA_wp &lt;- read_rds(\"data/rds/NGA_wp.rds\")\n\n\nlist(NGA_wp)\n\n[[1]]\nSimple feature collection with 774 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 26662.71 ymin: 30523.38 xmax: 1344157 ymax: 1096029\nProjected CRS: Minna / Nigeria Mid Belt\nFirst 10 features:\n          ADM2_EN ADM2_PCODE                   ADM1_EN ADM1_PCODE\n1       Aba North   NG001001                      Abia      NG001\n2       Aba South   NG001002                      Abia      NG001\n3          Abadam   NG008001                     Borno      NG008\n4           Abaji   NG015001 Federal Capital Territory      NG015\n5            Abak   NG003001                 Akwa Ibom      NG003\n6       Abakaliki   NG011001                    Ebonyi      NG011\n7  Abeokuta North   NG028001                      Ogun      NG028\n8  Abeokuta South   NG028002                      Ogun      NG028\n9             Abi   NG009001               Cross River      NG009\n10    Aboh-Mbaise   NG017001                       Imo      NG017\n                         geometry total_wp wp_functional wp_nonfunctional\n1  MULTIPOLYGON (((548795.5 11...       17             7                9\n2  MULTIPOLYGON (((547286.1 11...       71            29               35\n3  MULTIPOLYGON (((1248985 104...        0             0                0\n4  MULTIPOLYGON (((510864.9 57...       57            23               34\n5  MULTIPOLYGON (((594269 1209...       48            23               25\n6  MULTIPOLYGON (((660767 2522...      233            82               42\n7  MULTIPOLYGON (((78621.56 37...       34            16               15\n8  MULTIPOLYGON (((106627.7 35...      119            72               33\n9  MULTIPOLYGON (((632244.2 21...      152            79               62\n10 MULTIPOLYGON (((540081.3 15...       66            18               26\n   wp_unknown\n1           1\n2           7\n3           0\n4           0\n5           0\n6         109\n7           3\n8          14\n9          11\n10         22"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#basic-choropleth-mapping",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#basic-choropleth-mapping",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "3.5 Basic Choropleth Mapping",
    "text": "3.5 Basic Choropleth Mapping\n\n3.5.1 Visualising distribution of total, functional and non-functional water point\n\np1 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total water points by LGAs\",\n            legend.outside = FALSE)\n\n\np2 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water points by LGAs\",\n            legend.outside = FALSE)\n\n\np3 &lt;- tm_shape(NGA_wp) +\n  tm_fill(\"wp_nonfunctional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of non-functional water points by LGAs\",\n            legend.outside = FALSE)\n\n\ntmap_arrange(p2, p3, p1, nrow = 2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#section",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#section",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "3.4 ",
    "text": "3.4"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#choropleth-map-for-rates",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#choropleth-map-for-rates",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "3.6 Choropleth Map for Rates",
    "text": "3.6 Choropleth Map for Rates\nIn much of our readings we have now seen the importance to map rates rather than counts of things, and that is for the simple reason that water points are not equally distributed in space.\nThat means that if we do not account for how many water points are somewhere, we end up mapping total water point size rather than our topic of interest.\n\n3.6.1 Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  mutate(pct_functional = wp_functional/total_wp) %&gt;%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\n\n3.6.2 Plotting map of rate\nPlot a choropleth map showing the distribution of percentage functional water point by LGA.\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) + # show histogram legend\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of percentage functional water point by LGA\",\n            legend.outside = TRUE) # important, to add the legends outside the map."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#extreme-value-maps",
    "href": "Hands-on_Ex/Hands-on_Ex7/Hands-on_Ex7.html#extreme-value-maps",
    "title": "Hands-on Exercise 7 (Work in Progress)",
    "section": "3.7 Extreme Value Maps",
    "text": "3.7 Extreme Value Maps\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\n\n3.7.1 Percentile Map\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\n\n3.7.1.1 Data Preparation\nStep 1: Exclude records with NA by using the code chunk below.\n\nNGA_wp &lt;- NGA_wp %&gt;%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values\n\npercent &lt;- c(0,.01,.1,.5,.9,.99,1)\nvar &lt;- NGA_wp[\"pct_functional\"] %&gt;%\n  st_set_geometry(NULL) # used to drop geometry field in event quantile() cannot handle geospatial data. \nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen variables are extracted from an sf data.frame, the geometry is extracted as well. For mapping and spatial manipulation, this is the expected behavior, but many base R functions cannot deal with the geometry. Specifically, the quantile() gives an error. As a result st_set_geomtry(NULL) is used to drop geometry field.\n\n\n\n\n3.7.1.2 Creating the get.var function\nFirstly, we will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% \n    st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n3.7.1.3 A percentile mapping function\nNext, we will write a percentile mapping function by using the code chunk below.\n\npercentmap &lt;- function(vname, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent &lt;- c(0,.01,.1,.5,.9,.99,1)\n  var &lt;- get.var(vname, df)\n  bperc &lt;- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vname,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"&lt; 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"&gt; 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\n\n3.7.1.4 Test drive the percentile mapping function\nTo run the function, type the code chunk as shown below.\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n\nNote that this is just a bare bones implementation. Additional arguments such as the title, legend positioning just to name a few of them, could be passed to customise various features of the map.\n\n\n\n3.7.2 Box map\nIn essence, a box map is an augmented quartile map, with an additional lower and upper category. When there are lower outliers, then the starting point for the breaks is the minimum value, and the second break is the lower fence. In contrast, when there are no lower outliers, then the starting point for the breaks will be the lower fence, and the second break is the minimum value (there will be no observations that fall in the interval between the lower fence and the minimum value).\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nWe will try the following:\n\nDisplaying summary statistics on a choropleth map by using the basic principles of boxplot.\nTo create a box map, a custom breaks specification will be used. However, there is a complication. The break points for the box map vary depending on whether lower or upper outliers are present.\n\n\n3.7.2.1 Creating the boxbreaks function\nThe code chunk below is an R function that creating break points for a box map.\n\narguments:\n\nv: vector with observations\nmult: multiplier for IQR (default 1.5)\n\nreturns:\n\nbb: vector with 7 break points compute quartile and fences\n\n\n\nboxbreaks &lt;- function(v,mult=1.5) {\n  qv &lt;- unname(quantile(v))\n  iqr &lt;- qv[4] - qv[2]\n  upfence &lt;- qv[4] + mult * iqr\n  lofence &lt;- qv[2] - mult * iqr\n  # initialize break points vector\n  bb &lt;- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence &lt; qv[1]) {  # no lower outliers\n    bb[1] &lt;- lofence\n    bb[2] &lt;- floor(qv[1])\n  } else {\n    bb[2] &lt;- lofence\n    bb[1] &lt;- qv[1]\n  }\n  if (upfence &gt; qv[5]) { # no upper outliers\n    bb[7] &lt;- upfence\n    bb[6] &lt;- ceiling(qv[5])\n  } else {\n    bb[6] &lt;- upfence\n    bb[7] &lt;- qv[5]\n  }\n  bb[3:5] &lt;- qv[2:4]\n  return(bb)\n}\n\n\n\n3.7.2.2 Creating the get.var function\nThe code chunk below is an R function to extract a variable as a vector out of an sf data frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var &lt;- function(vname,df) {\n  v &lt;- df[vname] %&gt;% st_set_geometry(NULL)\n  v &lt;- unname(v[,1])\n  return(v)\n}\n\n\n\n3.7.2.3 Test drive the newly created function\nLet’s test the newly created function:\n\nvar &lt;- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\n\n3.7.2.4 Boxmap function\nThe code chunk below is an R function to create a box map.\narguments:\n\nvnam: variable name (as character, in quotes)\ndf: simple features polygon layer\nlegtitle: legend title\nmtitle: map title\nmult: multiplier for IQR\nreturns: - a tmap-element (plots a map)\n\n\nboxmap &lt;- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var &lt;- get.var(vnam,df)\n  bb &lt;- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"&lt; 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"&gt; 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\nboxmap(\"wp_nonfunctional\", NGA_wp)"
  }
]