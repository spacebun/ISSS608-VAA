---
title: "Take-home Exercise 4 (Work in Progress)"
subtitle: "Prototyping Modules for Visual Analytics Shiny Application"
date: February 25, 2024
date-modified:  last-modified
format:
  html:
    toc: true
    number-sections: true
    code-line-numbers: true
    
execute: 
  eval: true
  echo: true
  warning: false  
---

# Overview

In this exercise, I will use selected modules of the Shiny application for the Group Project component to fulfill the following objectives:

-   To evaluate and determine what are the necessary R packages needed for the Shiny application,

-   To prepare and test the specific R codes can be run and returned the correct output as expected,

-   To determine the parameters and outputs that will be exposed on the Shiny applications, and

-   To select the appropriate Shiny UI components for exposing the parameters determined above.

# Project details

For our project, we aim to create a Shiny app with user-friendly functionalities, to effectively visualize and analyze climate data.

The R Shiny app will consists of four modules:

-   Homepage module
-   EDA and CDA modul
-   Univariate forecasting module
-   Geospatial module: Two spatial interpolation techniques are presented for user to estimate weather conditions at unmonitored locations.  our understanding of climate patterns across the region by leveraging data from 11 stations. This approach allows for a more comprehensive and continuous spatial analysis of climate variability.
-   Clustering and Group Forecasting module: Users can explore grouping stations with similar weather patterns or grouping time periods with similar patterns. The user would then be able to choose different models to forecast the grouped data.


For this exercise, we will focus on the last two modules.

# Getting started

## Load packages

```{r}
pacman::p_load(gstat)
```

```{r}
pacman::p_load(tidyverse, naniar, imputeTS, DT, knitr, lubridate,
               sf, terra, viridis, automap,
               ggplot2, patchwork, ggthemes, ggiraph, plotly, 
               ggHoriPlot, 
               dtwclust, dendextend, ggraph, factoextra, ggdendro,
               tsibble, fable, feasts
               )

library(tmap)
```

# Data Preparation

::: callout-note
To skip ahead to view the cleaned data, you can click @sec-viewcleaneddataset.
:::

## Import data

```{r}
raw_weather_data <- read_csv("data/climate_historical_daily_records.csv")
```

Details of dataset:

| Dataset            | Description                                                   | Period    | Source                                              |
|------------------|-------------------|------------------|------------------|
| *raw_weather_data* | Climate Historical Daily Records for 63 stations in Singapore | 2014-2023 | http://www.weather.gov.sg/climate-historical-daily/ |

This dataset was retrieved from the [Meteorological Service Singapore](http://www.weather.gov.sg/climate-historical-daily/) site, and had some basic [pre-processing steps performed in python](https://isss608-airweatheranalytics.netlify.app/code/data_retrieval) due to the large amount of files:

-   Combine all downloaded CSV files into one dataframe.

-   Performing cleaning to merge data of columns with slightly different names due to case sensitivity (e.g., “min” vs. “Min”)

-   (‘Highest 30 **Min** Rainfall (mm)’, ‘Highest 30 **min** Rainfall (mm)’)

-   (‘Highest 60 **Min** Rainfall (mm)’, ‘Highest 60 **min** Rainfall (mm)’)

-   (‘Highest 120 **Min** Rainfall (mm)’, ‘Highest 120 **min** Rainfall (mm)’)

-   Add the latitude and longitude of each station to the dataframe.

## Check structure with `glimpse()`

```{r}
glimpse(raw_weather_data)
```

There are 202, 976 rows, and 15 columns in the dataset. In the next few steps, we will drop specific columns and rows based on the project focus.

## Filter dataset for desired period

While the dataset contains 10 years of data from 2014 to 2023, we will focus on the most recent dataset for a 3 year period, from **2021 to 2023**. This period was chosen to maximise the overall availability of data across the stations.

```{r}
raw_weather_data <- raw_weather_data %>%
  filter(Year >= 2021)
print(paste("The dataset covers the period from", min(raw_weather_data$Year, na.rm = TRUE), "to", max(raw_weather_data$Year, na.rm = TRUE), "."))
```

## Drop unused columns

We will not be using all 15 columns for this project. The following columns will be dropped:

-   `Highest 30 Min Rainfall (mm)`
-   `Highest 60 Min Rainfall (mm)`
-   `Highest 1200 Min Rainfall (mm)`
-   `Mean Wind Speed (km/h)`
-   `Max Wind Speed (km/h)`

```{r}
# Drop columns
raw_weather_data <- raw_weather_data %>%
  select(-c(`Highest 30 Min Rainfall (mm)`, 
            `Highest 60 Min Rainfall (mm)`, 
            `Highest 120 Min Rainfall (mm)`,
            `Mean Wind Speed (km/h)`,
            `Max Wind Speed (km/h)`))
```

## Remove rows for specific Stations

The Meteorological Service Singapore also provides a file, [Station Records](http://www.weather.gov.sg/wp-content/uploads/2022/06/Station_Records.pdf) that has some information on the availability of data for each station. After examining the station records file, we found that 41 stations had missing information for some variables. We will hence drop rows for these stations.

```{r}
#| code-fold: true
#| code-summary: "show code"
# Drop rows of 41 stations
# Define the station names to remove
stations_to_remove <- c("Macritchie Reservoir", "Lower Peirce Reservoir", "Pasir Ris (West)", "Kampong Bahru", "Jurong Pier", "Ulu Pandan", "Serangoon", "Jurong (East)", "Mandai", "Upper Thomson", "Buangkok", "Boon Lay (West)", "Bukit Panjang", "Kranji Reservoir", "Tanjong Pagar", "Admiralty West", "Queenstown", "Tanjong Katong", "Chai Chee", "Upper Peirce Reservoir", "Kent Ridge", "Somerset (Road)", "Punggol", "Tuas West", "Simei", "Toa Payoh", "Tuas", "Bukit Timah", "Yishun", "Buona Vista", "Pasir Ris (Central)", "Jurong (North)", "Choa Chu Kang (West)", "Serangoon North", "Lim Chu Kang", "Marine Parade", "Choa Chu Kang (Central)", "Dhoby Ghaut", "Nicoll Highway", "Botanic Garden", "Whampoa")

# Remove rows with the specified station names
raw_weather_data <- raw_weather_data[!raw_weather_data$Station %in% stations_to_remove, ]

# Print the number of stations left
print(sprintf("There were %d stations removed.There are %d stations left.", length(stations_to_remove), n_distinct(raw_weather_data$Station)))
```

## Check for duplicates

```{r}
#| code-fold: true
#| code-summary: "show code"
# Identify duplicates
duplicates <- raw_weather_data[duplicated(raw_weather_data[c("Station", "Year", "Month", "Day")]) | duplicated(raw_weather_data[c("Station", "Year", "Month", "Day")], fromLast = TRUE), ]

# Check if 'duplicates' dataframe is empty
if (nrow(duplicates) == 0) {
  print("The combination of Station Name, Year, Month, and Day is unique.")
} else {
  print("There are duplicates in the combination of Station Name, Year, Month, and Day. Showing duplicated rows:")
  print(duplicates)
}
```

## Check and handle missing values

### First check for missing values

Missing values in this dataset can be represented by:

-   `\u0097`

-   `NA`

-   `-`

We first replace these values with actual NA values:

```{r}
raw_weather_data <- raw_weather_data %>%
  mutate(across(where(is.character), ~na_if(.x, "\u0097"))) %>%
  mutate(across(where(is.character), ~na_if(.x, "NA"))) %>%
  mutate(across(where(is.character), ~na_if(.x, "-")))
```

Next, we visualize the missing values in the dataset:

```{r}
#| code-fold: true
#| code-summary: "show code"
vis_miss(raw_weather_data)
```

We will take steps to handle the missing data.

### Remove Stations with significant missing data

We have identified two checks to make:

-   Check which stations have no recorded data for entire months.

-   Check which stations have more than 7 consecutive days of missing data

For both these checks, we will remove the entire station from the dataset as it would not be practical to impute such large amounts of missing values.

#### Identify and remove Stations with no recorded data for entire months

Some stations have no recorded data for entire months, as summarised in the table below:

```{r}
#| code-fold: true
#| code-summary: "show code"
# Create complete combination of Station, Year, and Month
all_combinations <- expand.grid(
  Station = unique(raw_weather_data$Station),
  Year = 2021:2023,
  Month = 1:12
)

# Left join this with the original weather data to identify missing entries
missing_months <- all_combinations %>%
  left_join(raw_weather_data, by = c("Station", "Year", "Month")) %>%
  # Use is.na() to check for rows that didn't have a match in the original data
  filter(is.na(Day)) %>%
  # Select only the relevant columns for the final output
  select(Station, Year, Month)

# Create a summary table that lists out the missing months
missing_months_summary <- missing_months %>%
  group_by(Station, Year) %>%
  summarise(MissingMonths = toString(sort(unique(Month))), .groups = 'drop')

kable(missing_months_summary)
```

We hence drop these stations from our dataset:

```{r}
#| code-fold: true
#| code-summary: "show code"
raw_weather_data <- anti_join(raw_weather_data, missing_months, by = "Station")

print(sprintf("The folowing %d stations were dropped: %s", n_distinct(missing_months$Station), paste(unique(missing_months$Station), collapse = ", ")))

print(sprintf("There are %d stations left: ", n_distinct(raw_weather_data$Station)))

kable(unique(raw_weather_data$Station),
      row.names = TRUE,
      col.names = "Station",
      caption = "List of Remaining Stations")
```

#### Identify and remove Stations with excessive missing values

If there are any missing values, we can try to impute these missing values. However, if there are 7 or more consecutive values missing, we will remove these stations first.

```{r}
#| code-fold: true
#| code-summary: "show code"
# Define a helper function to count the number of 7 or more consecutive NAs
count_seven_consecutive_NAs <- function(x) {
  na_runs <- rle(is.na(x))
  total_consecutive_NAs <- sum(na_runs$lengths[na_runs$values & na_runs$lengths >= 7])
  return(total_consecutive_NAs)
}

# Apply the helper function to each relevant column within grouped data
weather_summary <- raw_weather_data %>%
  group_by(Station, Year, Month) %>%
  summarise(across(-Day, ~ count_seven_consecutive_NAs(.x), .names = "count_consec_NAs_{.col}"), .groups = "drop")

# Filter to keep only rows where there is at least one column with 7 or more consecutive missing values
weather_summary_with_consecutive_NAs <- weather_summary %>%
  filter(if_any(starts_with("count_consec_NAs_"), ~ . > 0))

# View the result
print(sprintf("There are %d stations with 7 or more consecutive missing values.", n_distinct(weather_summary_with_consecutive_NAs$Station)))

# kable(weather_summary_with_consecutive_NAs)
datatable(weather_summary_with_consecutive_NAs, 
            class= "compact",
            rownames = FALSE,
            width="100%", 
            options = list(pageLength = 10, scrollX=T),
          caption = 'Details of stations with >=7 missing values')
```

We hence drop these stations from our dataset:

```{r}
#| code-fold: true
#| code-summary: "show code"
raw_weather_data <- anti_join(raw_weather_data, weather_summary_with_consecutive_NAs, by = "Station")

print(sprintf("The folowing %d stations were dropped: %s", n_distinct(weather_summary_with_consecutive_NAs$Station), paste(unique(weather_summary_with_consecutive_NAs$Station), collapse = ", ")))

print(sprintf("There are %d stations left: ", n_distinct(raw_weather_data$Station)))

kable(unique(raw_weather_data$Station),
      row.names = TRUE,
      col.names = "Station",
      caption = "List of Remaining Stations")
```

### Second check for missing values

From the check below we see there are still missing values in our data. We will impute these values in the next step.

```{r}
#| code-fold: true
#| code-summary: "show code"
vis_miss(raw_weather_data)
```

### Impute missing values

To handle the missing values for the remaining Stations, we will impute missing values using simple moving average from **imputeTS** package.

#### Create Date column

```{r}
raw_weather_data <- raw_weather_data %>%
  mutate(Date = as.Date(paste(Year, Month, Day, sep = "-"))) %>%
  relocate(Date, .after = 1)
```

#### Using `imputeTS` package

```{r}
# Define the weather variables to loop through
weather_variables <- c("Daily Rainfall Total (mm)", "Mean Temperature (°C)", "Maximum Temperature (°C)", "Minimum Temperature (°C)")

# Ensure raw_weather_data is correctly copied to a new data frame for imputation
weather_data_imputed <- raw_weather_data

# Loop through each weather variable to impute missing values
for(variable in weather_variables) {
  # Convert variable to numeric, ensuring that the conversion warnings are handled if necessary
  weather_data_imputed[[variable]] <- as.numeric(as.character(weather_data_imputed[[variable]]))
  
  # Impute missing values using a moving average
  weather_data_imputed <- weather_data_imputed %>%
    group_by(Station) %>%
    arrange(Station, Date) %>%
    mutate("{variable}" := round(na_ma(.data[[variable]], k = 7, weighting = "simple"), 1)) %>%
    ungroup()
}
```

### Final visual check for missing values

```{r}
#| code-fold: true
#| code-summary: "show code"
vis_miss(weather_data_imputed)
```

### Add specific columns to data

These columns are added as they may be used in plots later.

```{r}
weather_data_imputed <- weather_data_imputed %>% 
  mutate(Date_mine = make_date(2023, month(Date), day(Date)),
         Month_Name = factor(months(Date), levels = month.name),
         Week = isoweek(Date),
         Weekday = wday(Date)) %>%
  select(1:5, Date_mine, Month_Name, Week, Weekday, everything()) # Re-order columns
```

## Summary of cleaned data {#sec-viewcleaneddataset}

### Details of stations and time period of data

```{r}
#| code-fold: true
#| code-summary: "show code"
time_period_start <- min(weather_data_imputed$Date)
time_period_end <- max(weather_data_imputed$Date)
cat("\nThe time period of the dataset is from", format(time_period_start, "%Y-%m-%d"),"to", format(time_period_end, "%Y-%m-%d"), "\n")

print(sprintf("There are %d stations: ", n_distinct(weather_data_imputed$Station)))

kable(unique(weather_data_imputed$Station),
      row.names = TRUE,
      col.names = "Station",
      caption = "List of Stations")
```

### Check structure with `glimpse()`

```{r}
glimpse(weather_data_imputed)
```

### View dataset as interactive table

```{r}
#| code-fold: true
#| code-summary: "show code"
datatable(weather_data_imputed, 
            class= "compact",
            rownames = FALSE,
            width="100%", 
            options = list(pageLength = 10, scrollX=T),
          caption = 'Cleaned and imputed weather dataset')
```

## Save cleaned data to .rds

```{r}
write_rds(weather_data_imputed, "data/weather_imputed_11stations.rds")
```

## Import cleaned data

The below code can be used to import the cleaned data.

```{r}
weather_data <- read_rds("data/weather_imputed_11stations.rds")
```

# Clustering Module: Exploratory Data Analysis of Multiple Time Series

## Horizon Plots

We will utilise Horizon Plots to explore and visualize multiple time series data at a glance.

Horizon plots are a visualization technique used for displaying multiple time series data. Since the data contains data across **multiple years** for **multiple stations**, it would be valuable to allow users to use horizon plots to visually explore patterns or trends across multiple time series data.

Overview of user options:

-   Select daily variable to plot
-   Plot across stations for different time granularity
-   Choose months to plot (Compare 1 month, Compare 3 months, Compare 6 months)
-   Choose years to plot (Compare 1 year, Compare 2 years, Compare 3 years)
-   Plot across different time periods for a single stations

### Horizon Plot with reproducible code

```{r}
#| code-fold: true
#| code-summary: "show code"
#| fig-width: 10
#| fig-height: 4
#| code-eval: false


compareAcrossHorizon = "Stations"
selectedYear = 2021 # Need to update below code for selection of year
selected_var = "Mean Temperature (°C)"
date_breaks = "3 month"
title <- ifelse(compareAcrossHorizon == "Years",
                paste(selected_var, "for", selectedStation, "across the years 2021 to 2023"),
                paste(selected_var, "for", selectedYear, "across station(s)"))

# Compute origin and  horizon scale cutpoints: 
cutpoints <- weather_data %>% 
  mutate(
    outlier = between(
      .data[[selected_var]], 
      quantile(.data[[selected_var]], 0.25, na.rm = TRUE) -
        1.5 * IQR(.data[[selected_var]], na.rm = TRUE),
      quantile(.data[[selected_var]], 0.75, na.rm = TRUE) +
        1.5 * IQR(.data[[selected_var]], na.rm = TRUE))) %>% 
  filter(outlier)

# ori <- sum(range(cutpoints[[selected_var]]))/2
# sca <- seq(range(cutpoints[[selected_var]])[1], range(cutpoints[[selected_var]])[2], length.out = seq_length)[-seq_length/2]

ori <- sum(range(cutpoints[[selected_var]]))/2
sca <- seq(range(cutpoints[[selected_var]])[1], range(cutpoints[[selected_var]])[2], length.out = 7)[-4]


ori <- round(ori, 2) # The origin, rounded to 2 decimal places
sca <- round(sca, 2) # The horizon scale cutpoints

weather_data %>% ggplot() +
  geom_horizon(aes(x = Date,
                   y = .data[[selected_var]],
                   fill = after_stat(Cutpoints)),
               origin = ori, horizonscale = sca) +
  scale_fill_hcl(palette = 'RdBu', reverse = T) +
  facet_grid(`Station`~.) +
  theme_few() +
  theme(panel.spacing.y=unit(0, "lines"), strip.text.y = element_text(
    size = 5, angle = 0, hjust = 0),
    axis.text.y = element_blank(),
    axis.text.x = element_text(size=7),
    axis.title.y = element_blank(),
    axis.title.x = element_blank(),
    axis.ticks.y = element_blank(),
    panel.border = element_blank()
    ) +
    scale_x_date(expand=c(0,0), date_breaks = date_breaks, date_labels = "%b%y") +
  ggtitle(title)
```

### Horizon Plot of Mean Temperature (°C) across different weather stations, for 2021 - 2023

```{r}
#| code-fold: true
#| code-summary: "show code"
#| fig-width: 10
#| fig-height: 4

# Step 1: compute origin and  horizon scale cutpoints: 
cutpoints <- weather_data %>% 
  mutate(
    outlier = between(
      `Mean Temperature (°C)`, 
      quantile(`Mean Temperature (°C)`, 0.25, na.rm = TRUE) -
        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE),
      quantile(`Mean Temperature (°C)`, 0.75, na.rm = TRUE) +
        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE))) %>% 
  filter(outlier)
ori <- sum(range(cutpoints$`Mean Temperature (°C)`))/2
sca <- seq(range(cutpoints$`Mean Temperature (°C)`)[1], 
           range(cutpoints$`Mean Temperature (°C)`)[2], 
           length.out = 7)[-4]
ori <- round(ori, 2) # The origin, rounded to 2 decimal places
sca <- round(sca, 2) # The horizon scale cutpoints
weather_data %>% ggplot() +
  geom_horizon(aes(x = Date,
                   y = `Mean Temperature (°C)`,
                   fill = after_stat(Cutpoints)),
               origin = ori, horizonscale = sca) +
  scale_fill_hcl(palette = 'RdBu', reverse = T) +
  facet_grid(`Station`~.) +
  theme_few() +
  theme(panel.spacing.y=unit(0, "lines"), strip.text.y = element_text(
    size = 5, angle = 0, hjust = 0),
    # legend.position = 'none',
    axis.text.y = element_blank(),
    axis.text.x = element_text(size=7),
    axis.title.y = element_blank(),
    axis.title.x = element_blank(),
    axis.ticks.y = element_blank(),
    panel.border = element_blank()
    ) +
    scale_x_date(expand=c(0,0), date_breaks = "3 month", date_labels = "%b%y") +
  ggtitle('Mean Temperature (°C) across different weather stations (2021 - 2023)')
```

### Horizon Plot of Mean Temperature (°C) across different weather stations for a specified year, 2023

```{r}
#| code-fold: true
#| code-summary: "show code"
#| fig-width: 10
#| fig-height: 4
# Ensure data has Year, Month, Date column + Date_mine column
weather_data <- weather_data %>% 
  mutate(Year = year(Date)) %>%
  mutate(Month = month(Date)) %>%
  mutate(Day = day(Date)) %>%
  mutate(Date_mine = make_date(2023, month(Date), day(Date)))

# Filter for specified year only
weather_data_2023 <- weather_data %>% 
  filter(Year == 2023) 

# Step 1: compute origin and  horizon scale cutpoints: 
cutpoints <- weather_data_2023 %>% 
  mutate(
    outlier = between(
      `Mean Temperature (°C)`, 
      quantile(`Mean Temperature (°C)`, 0.25, na.rm = TRUE) -
        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE),
      quantile(`Mean Temperature (°C)`, 0.75, na.rm = TRUE) +
        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE))) %>% 
  filter(outlier)

ori <- sum(range(cutpoints$`Mean Temperature (°C)`))/2
sca <- seq(range(cutpoints$`Mean Temperature (°C)`)[1], 
           range(cutpoints$`Mean Temperature (°C)`)[2], 
           length.out = 7)[-4]

ori <- round(ori, 2) # The origin, rounded to 2 decimal places
sca <- round(sca, 2) # The horizon scale cutpoints

# Plot horizon plot
weather_data_2023 %>% ggplot() +
  geom_horizon(aes(x = Date_mine, 
                   y = `Mean Temperature (°C)`,
                   fill = after_stat(Cutpoints)), 
               origin = ori, horizonscale = sca) +
  scale_fill_hcl(palette = 'RdBu', reverse = T) +
  facet_grid(~Station ~ .) +
  theme_few() +
  theme(
    panel.spacing.y = unit(0, "lines"),
    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.border = element_blank()
    ) +
  scale_x_date(expand = c(0, 0), 
               date_breaks = "1 month", 
               date_labels = "%b") +
  xlab('Date') +
  ggtitle('Mean Temperature (°C) for the year 2023',
          'across the stations')
```

### Horizon Plot of Mean Temperature (°C) For Ang Mo Kio across the years

```{r}
#| code-fold: true
#| code-summary: "show code"
#| fig-width: 10
#| fig-height: 4
# Step 0: Filter for specified station only 
ang_mo_kio_data <- weather_data %>% 
  filter(Station == "Seletar")

# Step 1: compute origin and  horizon scale cutpoints: 
cutpoints <- ang_mo_kio_data %>% 
  mutate(
    outlier = between(
      `Mean Temperature (°C)`, 
      quantile(`Mean Temperature (°C)`, 0.25, na.rm = TRUE) -
        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE),
      quantile(`Mean Temperature (°C)`, 0.75, na.rm = TRUE) +
        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE))) %>% 
  filter(outlier)

ori <- sum(range(cutpoints$`Mean Temperature (°C)`))/2
sca <- seq(range(cutpoints$`Mean Temperature (°C)`)[1], 
           range(cutpoints$`Mean Temperature (°C)`)[2], 
           length.out = 7)[-4]

ori <- round(ori, 2) # The origin, rounded to 2 decimal places
sca <- round(sca, 2) # The horizon scale cutpoints

# Step 2: Ensure data has Year, Month, Date column. Also create a Date_mine column for comparing across years
ang_mo_kio_data <- ang_mo_kio_data %>% 
  mutate(Year = year(Date)) %>%
  mutate(Month = month(Date)) %>%
  mutate(Day = day(Date)) %>%
  mutate(Date_mine = make_date(2023, month(Date), day(Date)))

# Step 3: Plot horizon plot
ang_mo_kio_data %>% ggplot() +
  geom_horizon(aes(x = Date_mine, 
                   y = `Mean Temperature (°C)`,
                   fill = after_stat(Cutpoints)), 
               origin = ori, horizonscale = sca) +
  scale_fill_hcl(palette = 'RdBu', reverse = T) +
  facet_grid(~Year ~ .) +
  theme_few() +
  theme(
    panel.spacing.y = unit(0, "lines"),
    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.border = element_blank()
    ) +
  scale_x_date(expand = c(0, 0), 
               date_breaks = "1 month", 
               date_labels = "%b") +
  xlab('Date') +
  ggtitle('Mean Temperature (°C) For Ang Mo Kio',
          'across the years 2021 to 2023')
```

### Horizon Plot of Mean Temperature (°C) across average of all weather stations from 2021 to 2023

```{r}
#| code-fold: true
#| code-summary: "show code"
#| fig-width: 10
#| fig-height: 4

# Step 0: Compute average of Mean Temperature (°C) across all stations
weather_data_all_stations <- weather_data %>%
  group_by(Date) %>%
  summarise(`Mean Temperature (°C)` = mean(`Mean Temperature (°C)`))

# Step 1: compute origin and  horizon scale cutpoints: 
cutpoints <- weather_data_all_stations %>% 
  mutate(
    outlier = between(
      `Mean Temperature (°C)`, 
      quantile(`Mean Temperature (°C)`, 0.25, na.rm = TRUE) -
        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE),
      quantile(`Mean Temperature (°C)`, 0.75, na.rm = TRUE) +
        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE))) %>% 
  filter(outlier)

ori <- sum(range(cutpoints$`Mean Temperature (°C)`))/2
sca <- seq(range(cutpoints$`Mean Temperature (°C)`)[1], 
           range(cutpoints$`Mean Temperature (°C)`)[2], 
           length.out = 7)[-4]

ori <- round(ori, 2) # The origin, rounded to 2 decimal places
sca <- round(sca, 2) # The horizon scale cutpoints

# Step 2: Ensure data has Year, Month, Date column. Also create a Date_mine column for comparing across years
weather_data_all_stations <- weather_data_all_stations %>% 
  mutate(Year = year(Date)) %>%
  mutate(Month = month(Date)) %>%
  mutate(Day = day(Date)) %>%
  mutate(Date_mine = make_date(2023, month(Date), day(Date)))

# Step 3: Plot horizon plot
weather_data_all_stations %>% ggplot() +
  geom_horizon(aes(x = Date_mine, 
                   y = `Mean Temperature (°C)`,
                   fill = after_stat(Cutpoints)), 
               origin = ori, horizonscale = sca) +
  scale_fill_hcl(palette = 'RdBu', reverse = T) +
  facet_grid(~Year ~ .) +
  theme_few() +
  theme(
    panel.spacing.y = unit(0, "lines"),
    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.border = element_blank()
    ) +
  scale_x_date(expand = c(0, 0), 
               date_breaks = "1 month", 
               date_labels = "%b") +
  xlab('Date') +
  ggtitle('Mean Temperature (°C) across across all weather stations', 
          'from 2021 to 2023')
```

### Horizon Plot of Daily Rainfall Total across all weather stations from 2021 to 2023

This plot may not make sense as we may need to sum up rainfall for each day across all weather stations first before plotting.

```{r}
#| code-fold: true
#| code-summary: "show code"
#| fig-width: 10
#| fig-height: 4
# Step 1: compute origin and  horizon scale cutpoints: 
cutpoints <- weather_data %>% 
  mutate(
    outlier = between(
      `Daily Rainfall Total (mm)`, 
      quantile(`Daily Rainfall Total (mm)`, 0.25, na.rm = TRUE) -
        1.5 * IQR(`Daily Rainfall Total (mm)`, na.rm = TRUE),
      quantile(`Daily Rainfall Total (mm)`, 0.75, na.rm = TRUE) +
        1.5 * IQR(`Daily Rainfall Total (mm)`, na.rm = TRUE))) %>% 
  filter(outlier)

ori <- sum(range(cutpoints$`Daily Rainfall Total (mm)`))/2
sca <- seq(range(cutpoints$`Daily Rainfall Total (mm)`)[1], 
           range(cutpoints$`Daily Rainfall Total (mm)`)[2], 
           length.out = 7)[-4]

ori <- round(ori, 2) # The origin, rounded to 2 decimal places
sca <- round(sca, 2) # The horizon scale cutpoints


# Step 2: Ensure data has Year, Month, Date column. Also create a Date_mine column for comparing across years
weather_data <- weather_data %>% 
  mutate(Year = year(Date)) %>%
  mutate(Month = month(Date)) %>%
  mutate(Day = day(Date)) %>%
  mutate(Date_mine = make_date(2023, month(Date), day(Date)))

# Step 3: Plot horizon plot
weather_data %>% ggplot() +
  geom_horizon(aes(x = Date_mine, 
                   y = `Daily Rainfall Total (mm)`,
                   fill = after_stat(Cutpoints)), 
               origin = ori, horizonscale = sca) +
  scale_fill_hcl(palette = 'RdBu', reverse = T) +
  facet_grid(~Year ~ .) +
  theme_few() +
  theme(
    panel.spacing.y = unit(0, "lines"),
    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.border = element_blank()
    ) +
  scale_x_date(expand = c(0, 0), 
               date_breaks = "1 month", 
               date_labels = "%b") +
  xlab('Date') +
  ggtitle('Daily Rainfall Total across all weather stations', 
          'from 2021 to 2023')

```

### Horizon Plot of Daily Rainfall Total (mm) across all the stations for a specified year

```{r}
#| code-fold: true
#| code-summary: "show code"
#| fig-width: 10
#| fig-height: 4
# Ensure data has Year, Month, Date column
weather_data <- weather_data %>% 
  mutate(Year = year(Date)) %>%
  mutate(Month = month(Date)) %>%
  mutate(Day = day(Date)) %>%
  mutate(Date_mine = make_date(2023, month(Date), day(Date)))

# Filter for specified year only
weather_data_2023 <- weather_data %>% 
  filter(Year == 2023)

# Step 1: compute origin and  horizon scale cutpoints: 
cutpoints <- weather_data_2023 %>% 
  mutate(
    outlier = between(
      `Daily Rainfall Total (mm)`, 
      quantile(`Daily Rainfall Total (mm)`, 0.25, na.rm = TRUE) -
        1.5 * IQR(`Daily Rainfall Total (mm)`, na.rm = TRUE),
      quantile(`Daily Rainfall Total (mm)`, 0.75, na.rm = TRUE) +
        1.5 * IQR(`Daily Rainfall Total (mm)`, na.rm = TRUE))) %>% 
  filter(outlier)

ori <- sum(range(cutpoints$`Daily Rainfall Total (mm)`))/2
sca <- seq(range(cutpoints$`Daily Rainfall Total (mm)`)[1], 
           range(cutpoints$`Daily Rainfall Total (mm)`)[2], 
           length.out = 7)[-4]

ori <- round(ori, 2) # The origin, rounded to 2 decimal places
sca <- round(sca, 2) # The horizon scale cutpoints

# Plot horizon plot
weather_data_2023 %>% ggplot() +
  geom_horizon(aes(x = Date_mine, 
                   y = `Daily Rainfall Total (mm)`,
                   fill = after_stat(Cutpoints)), 
               origin = ori, horizonscale = sca) +
  scale_fill_hcl(palette = 'RdBu', reverse = T) +
  facet_grid(~Station ~ .) +
  theme_few() +
  theme(
    panel.spacing.y = unit(0, "lines"),
    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.border = element_blank()
    ) +
  scale_x_date(expand = c(0, 0), 
               date_breaks = "1 month", 
               date_labels = "%b") +
  xlab('Date') +
  ggtitle('Daily Rainfall Total (mm) for the year 2023',
          'across the stations')
```

# Clustering Module: Clustering of Multiple Time Series

## Overview

Clustering is the practice of finding hidden patterns or similar groups in data.

To perform time series clustering, we will use the function [`tsclust`](https://www.rdocumentation.org/packages/dtwclust/versions/3.1.1/topics/tsclust) from [`dtwclust`](https://www.rdocumentation.org/link/dtwclust?package=dtwclust&version=3.1.1).

https://cran.r-project.org/web/packages/dtwclust/vignettes/dtwclust.pdf

We will divide cluster analysis into the following parts:

-   Format data into list of series for input to clustering function
-   Choosing the clustering method: "partitional", "hierarchical", "tadpole" or "fuzzy"
-   Choosing distance computation: Ignored for type = "tadpole".
-   Determining a measure to quantify the similarity between observations
-   Selecting the desired number of clusters

## Clustering of stations

We will use the variable `temp_data` , and perform cluster analysis of time series of daily temperature readings from different weather stations for the same period.

### Format data into list of series

Users should be allowed to cluster by:

-   Stations, for multiple stations and multiple years
-   Months, for a single station and a given year

::: panel-tabset
#### For a single selected station and year, by months

```{r}
#| code-fold: true
#| code-summary: "show code"
# Extract the target variable, temperature data into a variable
temp_data <- weather_data %>%
  select(c(Station, Date, Year, Month, Month_Name, Day, `Mean Temperature (°C)`))

# Filter for station(s) and year(s) e.g., "Changi", for 2023
stations_selected = "Changi"
years_selected = "2021"
station_temp_data <- temp_data %>% 
  filter(Station %in% stations_selected) %>%
  filter(Year %in% years_selected)

# Ensure the data is ordered by Month_Name and by Date
station_temp_data <- station_temp_data %>%
  arrange(Month_Name, Date)

# Format data into list of series, by Month_Name
list_of_series <- split(station_temp_data$`Mean Temperature (°C)`, station_temp_data$Month_Name)
```

#### For multiple stations and multiple years, by station

```{r}
#| code-fold: true
#| code-summary: "show code"
# Extract the target variable, temperature data into a variable
temp_data <- weather_data %>%
  select(c(Station, Date, Year, Month, Day, `Mean Temperature (°C)`))

# Filter for all stations and years 
stations_selected = unique(weather_data$Station) # note: not station_selected
years_selected = unique(weather_data$Year) # note: not stations_selected
station_temp_data <- temp_data %>% 
  filter(Station %in% stations_selected) %>%
  filter(Year %in% years_selected)


# Group-wise (Station-by-Station) Normalization, applying Min-Max scaling within each station group
station_temp_data <- station_temp_data %>%
  group_by(Station) %>%
  mutate(Normalized_Temp = (`Mean Temperature (°C)` - min(`Mean Temperature (°C)`, na.rm = TRUE)) / 
                            (max(`Mean Temperature (°C)`, na.rm = TRUE) - min(`Mean Temperature (°C)`, na.rm = TRUE))) %>%
  ungroup()


# Ensure the data is ordered by Station and by Date
station_temp_data <- station_temp_data %>%
  arrange(Station, Date)

# Format data into list of series, by Station
list_of_series <- split(station_temp_data$Normalized_Temp, station_temp_data$Station)
```
:::

### Exploring clustering methods

#### Hierarchical

Assume clustering for several stations across 2021 - 2023.

```{r}
#| code-fold: true
#| code-summary: "show code"
n_cluster = 2 # Number of desired clusters. Should not show if cluster_type == "partitional"
cluster_type = "hierarchical" #  "partitional", "hierarchical", "tadpole" or "fuzzy"
distance = "dtw_basic" # Ignored for type = "tadpole"
control = hierarchical_control(method = "complete") 

c <- tsclust(series = list_of_series, 
        type = cluster_type,  
        k = n_cluster, 
        distance = distance, 
        control = control
        )

p <- fviz_dend(c, k = n_cluster, # Cut in n_cluster groups
          cex = 0.6, # label size
          k_colors = c("jco"),
          color_labels_by_k = FALSE,  # color labels by groups
          rect_border = "jco",
          rect = TRUE,
          rect_fill = TRUE,
          horiz = TRUE)
p
```

Varying number of desired clusters, *k*, produces the following:

\<to be updated\>

Keeping *k* at x and varying the distance computation produces the following:

\<to be updated\>

# Clustering Module: Forecasting of grouped time series

\<to be updated\>

Say we have the following groups as a result of clustering:\
![](images/clipboard-2703185754.png)

We will attempt to apply forecasting models to these groups.

The forecasting workflow involves a few basic steps:

1.  Prepare data in a suitable format for time series forecasting

2.  Apply a model or set of models

3.  Forecast the models

4.  Include measures to compare accuracy of models

```{r}
#| code-fold: true
#| code-summary: "show code"
# Extract cluster assignments
clusters <- c@cluster

# Extract series names (stations)
series_names <- names(clusters)

# Create a list to store dataframes for each cluster
cluster_dfs <- list()

for (i in unique(clusters)) {
  # Identify series belonging to the current cluster
  series_in_cluster <- series_names[clusters == i]
  
  # Extract these series from the original data and store in list
  cluster_dfs[[paste("Cluster", i)]] <- weather_data[weather_data$Station %in% series_in_cluster, ]
}

# cluster_dfs is a list of dataframes, each representing one cluster
```

Prepare data for forecasting:

```{r}
#| code-fold: true
#| code-summary: "show code"
cluster_data <- cluster_dfs[[1]]
unique(cluster_data$Station) # Stations in our data

# Calculate mean temperature averages by Date
temp_averages <- cluster_data %>%
  group_by(Date) %>%
  summarise(Average_Mean_Temperature_C = mean(`Mean Temperature (°C)`, na.rm = TRUE)) %>%
  ungroup()

# Retain distinct date-related information
date_info <- cluster_data %>%
  select(Date, Year, Month, Day, Date_mine, Month_Name, Week, Weekday) %>%
  distinct()

# Merge temperature averages with date information
cluster_data_aggregated <- date_info %>%
  left_join(temp_averages, by = "Date")

cluster_data_aggregated <- as_tsibble(cluster_data_aggregated, index = Date)

```

Do forecasting

```{r}
#| code-fold: true
#| code-summary: "show code"
# cluster_data_aggregated %>%
#   autoplot(Average_Mean_Temperature_C) 

fit <- cluster_data_aggregated %>%
  model(
    snaive = SNAIVE(Average_Mean_Temperature_C ),
    ets = ETS(Average_Mean_Temperature_C),
    arima = ARIMA(Average_Mean_Temperature_C)
  )
fc <- fit %>%
  forecast(h = 12)
fc %>%
  autoplot(cluster_data_aggregated, level = NULL) + # show no prediction intervals
  ggtitle("Forecasts") +
  xlab("Date") +
  guides(colour = guide_legend(title = "Forecast"))
```

# Spatial Interpolation Module

This module presents two different methods (Inverse Distance Weighting (IDW), Ordinary Kriging, etc.) to prepare an isohyet map for different variables, by doing spatial interpolation for the data. This map will typically represent data for a specific point in time or an averaged period (a particular day, month, etc.), depending on the analysis goal.

Design considerations for user control:

-   Use radioButtons function to select only one variable
-   Use radioButtons function to select only one type of time window
-   Display original data and both methods for interpolation side by side.
-   For each method, offer variable to tweak:
-   Inverse Distance Weighting (IDW): *nmax*
-   Ordinary Kriging

First, read in planning subzone boundary data (shape file) for Singapore. This step should be done before running UI and server code.

```{r}
# This is a shape file, read in with st_read
mpsz2019 <- st_read(dsn = "data/geospatial", 
                    layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)
```

### Method 1: inverse distance weighted interpolation

We first focus on interpolating data using IDW method for a specified month.

For rainfall, we should aggregate by computing sum of rainfall for the month. For temperature, we should aggregate by computing average temperature per month.

::: panel-tabset
### Daily Rainfall Total (mm)

```{r}
#| code-fold: true
#| code-summary: "show code"
# Indicate user variable
selected_var <- "Daily Rainfall Total (mm)" # "Daily Rainfall Total (mm)" or "Mean Temperature (°C)"
selected_month <- c("February") # User to specify Month
selected_year <- c("2023") # User to specify Year
interpolation_method <- "IDW"

# Create table for chosen variable and time period
variable_data <- weather_data %>%
  filter(Year %in% selected_year,
         Month_Name %in% selected_month) %>%
  group_by(Station, Year, Month, LAT, LONG) %>%
  summarise(MonthlyValue = if(selected_var == "Daily Rainfall Total (mm)") {
                sum(.data[[selected_var]], na.rm = TRUE) # aggregate by computing sum of rainfall for the month. 
              } else if (selected_var == "Mean Temperature (°C)") {
                mean(.data[[selected_var]], na.rm = TRUE) # aggregate by computing average temperature per month.
              }, .groups = 'drop')
  
# Make it spatial data
variable_data_sf <- st_as_sf(variable_data, 
                      coords = c("LONG", # X axis 
                                 "LAT"), # Y axis
                      crs= 4326) %>%
  st_transform(crs = 3414) # To project this into SVY21

# Visualize variable with tmap
tmap_options(check.and.fix = TRUE) # Use this code to fix topo error without changing the data. 
tmap_mode("view") # Make map interactive
tm_shape(mpsz2019) + # Plot boundary map
  tm_borders() +
tm_shape(variable_data_sf) +
  tm_dots(col = 'MonthlyValue') # Use color to differentiate

tmap_mode("plot") # Change mode back

# Create grid data object / Raster data
# Extract bounding box variables from mpsz2019 
bbox <- as.list(st_bbox(mpsz2019))

# Assume user is able to indicate resolution
res = 50 # User Input. The smaller the value the higher the resolution, but the higher the computational requirement (slower processing).
nrows =  (bbox$ymax - bbox$ymin)/res
ncols = (bbox$xmax - bbox$xmin)/res
# Create a raster layer, `grid`,  from an existing spatial object `mpsz2019`
grid <- terra::rast(mpsz2019, 
                   nrows = nrows, 
                   ncols = ncols)
# Generate coordinates for each cell of the raster 
xy <- terra::xyFromCell(grid,
                         1:ncell(grid))
# Converting coordinates of raster into a spatial (sf) object
coop <- st_as_sf(as.data.frame(xy),
                 coords = c("x","y"),
                 crs = st_crs(mpsz2019)) # Assign CRS based on mpsz2019
coop <- st_filter(coop, mpsz2019) # Filter to only only includes points within mpsz2019

# IDW Method
nmax = 3 # Set value
# Create gstat object
res <- gstat(formula = MonthlyValue ~ 1, # specify  dependent variable
             locations = variable_data_sf,
             nmax = nmax, # number of nearest neighbors considered for interpolation. Parameter to expose for UI
             set = list(idp = 0))
# Predict values at new locations (coop) based on the spatial data (rfdata_sf)
resp <- predict(res,coop)
# Plot The Result
# Extract x and y coordinates and the predicted values into resp
resp$x <- st_coordinates(resp)[,1]
resp$y <- st_coordinates(resp)[,2]
resp$pred <- resp$var1.pred
# Predictions are rasterized over a predefined grid, using the mean of predictions where multiple values fall into a single grid cell
# This results in a raster layer pred representing spatially interpolated values
pred <- terra:: rasterize(resp, grid, # resp$pred contains the spatially interpolated variables. grid is raster layer.
                         field="pred",
                         fun="mean")
# map the interpolated variable raster, pred
main_title <- ifelse(selected_var == "Daily Rainfall Total (mm)",
                paste("Distribution for Total Rainfall (mm) for", selected_month, selected_year),
                paste("Distribution for Average Mean Temperature (°C) for", selected_month, selected_year))
raster_title <- ifelse(selected_var == "Daily Rainfall Total (mm)",
                paste("Total Monthly Rainfall (mm)"),
                paste("Average Mean Temperature (°C)"))
tmap_options(check.and.fix = TRUE)
tmap_mode("plot")
tm_shape(pred) + 
  tm_raster(alpha = 0.6, 
            palette = "viridis",
            title = raster_title) +
  tm_layout(main.title = main_title,
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
```

Varying nmax produced the following results:

![](images/clipboard-3262274107.png)

### Mean Temperature (°C)

```{r}
#| code-fold: true
#| code-summary: "show code"
# Indicate user variable
selected_var <- "Mean Temperature (°C)" # "Daily Rainfall Total (mm)" or "Mean Temperature (°C)"
selected_month <- c("February") # User to specify Month
selected_year <- c("2023") # User to specify Year

# Create tbl for chosen variable and time period
variable_data <- weather_data %>%
  filter(Year %in% selected_year,
         Month_Name %in% selected_month) %>%
  group_by(Station, Year, Month, LAT, LONG) %>%
  summarise(MonthlyValue = if(selected_var == "Daily Rainfall Total (mm)") {
                sum(.data[[selected_var]], na.rm = TRUE) # aggregate by computing sum of rainfall for the month. 
              } else if (selected_var == "Mean Temperature (°C)") {
                mean(.data[[selected_var]], na.rm = TRUE) # aggregate by computing average temperature per month.
              }, .groups = 'drop')
  
# Make it spatial data
variable_data_sf <- st_as_sf(variable_data, 
                      coords = c("LONG", # X axis 
                                 "LAT"), # Y axis
                      crs= 4326) %>%
  st_transform(crs = 3414) # To project this into SVY21

# Visualize variable with tmap
tmap_options(check.and.fix = TRUE) # Use this code to fix topo error without changing the data. 
tmap_mode("view") # Make map interactive
tm_shape(mpsz2019) + # Plot boundary map
  tm_borders() +
tm_shape(variable_data_sf) +
  tm_dots(col = 'MonthlyValue') # Use color to differentiate

tmap_mode("plot") # Change mode back

# Create grid data object / Raster data
# Extract bounding box variables from mpsz2019 
bbox <- as.list(st_bbox(mpsz2019))

# Assume user is able to indicate resolution
res = 50 # User Input. The smaller the value the higher the resolution, but the higher the computational requirement (slower processing).
nrows =  (bbox$ymax - bbox$ymin)/res
ncols = (bbox$xmax - bbox$xmin)/res
# Create a raster layer, `grid`,  from an existing spatial object `mpsz2019`
grid <- terra::rast(mpsz2019, 
                   nrows = nrows, 
                   ncols = ncols)
# Generate coordinates for each cell of the raster 
xy <- terra::xyFromCell(grid,
                         1:ncell(grid))
# Converting coordinates of raster into a spatial (sf) object
coop <- st_as_sf(as.data.frame(xy),
                 coords = c("x","y"),
                 crs = st_crs(mpsz2019)) # Assign CRS based on mpsz2019
coop <- st_filter(coop, mpsz2019) # Filter to only only includes points within mpsz2019

# IDW Method
nmax = 3 # Set value
# Create gstat object
res <- gstat(formula = MonthlyValue ~ 1, # specify  dependent variable
             locations = variable_data_sf,
             nmax = nmax, # number of nearest neighbors considered for interpolation. Parameter to expose for UI
             set = list(idp = 0))
# Predict values at new locations (coop) based on the spatial data (rfdata_sf)
resp <- predict(res,coop)
# Plot The Result
# Extract x and y coordinates and the predicted values into resp
resp$x <- st_coordinates(resp)[,1]
resp$y <- st_coordinates(resp)[,2]
resp$pred <- resp$var1.pred
# Predictions are rasterized over a predefined grid, using the mean of predictions where multiple values fall into a single grid cell
# This results in a raster layer pred representing spatially interpolated values
pred <- terra:: rasterize(resp, grid, # resp$pred contains the spatially interpolated variables. grid is raster layer.
                         field="pred",
                         fun="mean")
# map the interpolated variable raster, pred
main_title <- ifelse(selected_var == "Daily Rainfall Total (mm)",
                paste("Distribution for Total Rainfall (mm) for", selected_month, selected_year),
                paste("Distribution for Average Mean Temperature (°C) for", selected_month, selected_year))
raster_title <- ifelse(selected_var == "Daily Rainfall Total (mm)",
                paste("Total Monthly Rainfall (mm)"),
                paste("Average Mean Temperature (°C)"))
tmap_options(check.and.fix = TRUE)
tmap_mode("plot")
tm_shape(pred) + 
  tm_raster(alpha = 0.6, 
            palette = "viridis",
            title = raster_title) +
  tm_layout(main.title = main_title,
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
```

Varying nmax produced the following results:

![](images/clipboard-2558859595.png)
:::

### Method 2: Kriging

Kriging can be understood as a two-step process:

-   first, the spatial covariance structure of the sampled points is determined by fitting a variogram; and
-   second, weights derived from this covariance structure are used to interpolate values for unsampled points or blocks across the spatial field.

The below code chunk show the available variogram models in **gstat**.

```{r}
show.vgms(par.strip.text = list(cex = 0.75))

```

::: panel-tabset
### Daily Rainfall Total (mm)

Data Prep

```{r}
#| code-fold: true
#| code-summary: "show code"
# Indicate user variable
selected_var <- "Daily Rainfall Total (mm)" # "Daily Rainfall Total (mm)" or "Mean Temperature (°C)"
selected_month <- c("February") # User to specify Month
selected_year <- c("2023") # User to specify Year
interpolation_method <- "IDW"

# Create table for chosen variable and time period
variable_data <- weather_data %>%
  filter(Year %in% selected_year,
         Month_Name %in% selected_month) %>%
  group_by(Station, Year, Month, LAT, LONG) %>%
  summarise(MonthlyValue = if(selected_var == "Daily Rainfall Total (mm)") {
                sum(.data[[selected_var]], na.rm = TRUE) # aggregate by computing sum of rainfall for the month. 
              } else if (selected_var == "Mean Temperature (°C)") {
                mean(.data[[selected_var]], na.rm = TRUE) # aggregate by computing average temperature per month.
              }, .groups = 'drop')
  
# Make it spatial data
variable_data_sf <- st_as_sf(variable_data, 
                      coords = c("LONG", # X axis 
                                 "LAT"), # Y axis
                      crs= 4326) %>%
  st_transform(crs = 3414) # To project this into SVY21

# Create grid data object / Raster data
# Extract bounding box variables from mpsz2019 
bbox <- as.list(st_bbox(mpsz2019))

# Assume user is able to indicate resolution
res = 50 # User Input. The smaller the value the higher the resolution, but the higher the computational requirement (slower processing).
nrows =  (bbox$ymax - bbox$ymin)/res
ncols = (bbox$xmax - bbox$xmin)/res
# Create a raster layer, `grid`,  from an existing spatial object `mpsz2019`
grid <- terra::rast(mpsz2019, 
                   nrows = nrows, 
                   ncols = ncols)
# Generate coordinates for each cell of the raster 
xy <- terra::xyFromCell(grid,
                         1:ncell(grid))
# Converting coordinates of raster into a spatial (sf) object
coop <- st_as_sf(as.data.frame(xy),
                 coords = c("x","y"),
                 crs = st_crs(mpsz2019)) # Assign CRS based on mpsz2019
coop <- st_filter(coop, mpsz2019) # Filter to only only includes points within mpsz2019
```

This is the “experimental” variogram that should be shown to user, for user to assess what variables to adjust when fitting the variogram.

```{r}
#| code-fold: true
#| code-summary: "show code"
v <- variogram(MonthlyValue ~ 1, 
               data = variable_data_sf)
plot(v, cex = 1.5) # Experimental plot that should be shown to user
```

The below code chunk is used to tweak the fitted variogram:

```{r}
#| code-fold: true
#| code-summary: "show code"


# The different parameters have been tweaked and the results are presented below. 
# User will choose based on below variables 
fv <- fit.variogram(object = v,
                    model = vgm(psill = 5, model = "Gau", range = 8000, nugget = 0.1))

fv
plot(v, fv, cex = 1.5) #  visualise how well the observed data fit the model by plotting fv 


```

Final surface map

```{r}
#| code-fold: true
#| code-summary: "show code"
# perform spatial interpolation by using the newly derived model 
k <- gstat(formula = MonthlyValue ~ 1,
           data = variable_data_sf,
           model = fv)

# estimate the unknown grids
resp <- predict(k,coop)

resp$x <- st_coordinates(resp)[,1]
resp$y <- st_coordinates(resp)[,2]
resp$pred <- resp$var1.pred

# create a raster surface data object
kpred <- terra::rasterize(resp, grid,
                          field = "pred")

# map the interpolated variable raster, kpred
main_title <- ifelse(selected_var == "Daily Rainfall Total (mm)",
                paste("Distribution for Total Rainfall (mm) for", selected_month, selected_year),
                paste("Distribution for Average Mean Temperature (°C) for", selected_month, selected_year))
raster_title <- ifelse(selected_var == "Daily Rainfall Total (mm)",
                paste("Total Monthly Rainfall (mm)"),
                paste("Average Mean Temperature (°C)"))
tmap_options(check.and.fix = TRUE)
tmap_mode("plot")
tm_shape(kpred) + 
  tm_raster(alpha = 0.6, 
            palette = "viridis",
            title = raster_title) +
  tm_layout(main.title = main_title,
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
```

### Mean Temperature (°C)

Data Prep:

```{r}
#| code-fold: true
#| code-summary: "show code"
# Indicate user variable
selected_var <- "Mean Temperature (°C)" # "Daily Rainfall Total (mm)" or "Mean Temperature (°C)"
selected_month <- c("February") # User to specify Month
selected_year <- c("2023") # User to specify Year
interpolation_method <- "IDW"

# Create table for chosen variable and time period
variable_data <- weather_data %>%
  filter(Year %in% selected_year,
         Month_Name %in% selected_month) %>%
  group_by(Station, Year, Month, LAT, LONG) %>%
  summarise(MonthlyValue = if(selected_var == "Daily Rainfall Total (mm)") {
                sum(.data[[selected_var]], na.rm = TRUE) # aggregate by computing sum of rainfall for the month. 
              } else if (selected_var == "Mean Temperature (°C)") {
                mean(.data[[selected_var]], na.rm = TRUE) # aggregate by computing average temperature per month.
              }, .groups = 'drop')
  
# Make it spatial data
variable_data_sf <- st_as_sf(variable_data, 
                      coords = c("LONG", # X axis 
                                 "LAT"), # Y axis
                      crs= 4326) %>%
  st_transform(crs = 3414) # To project this into SVY21

# Create grid data object / Raster data
# Extract bounding box variables from mpsz2019 
bbox <- as.list(st_bbox(mpsz2019))

# Assume user is able to indicate resolution
res = 50 # User Input. The smaller the value the higher the resolution, but the higher the computational requirement (slower processing).
nrows =  (bbox$ymax - bbox$ymin)/res
ncols = (bbox$xmax - bbox$xmin)/res
# Create a raster layer, `grid`,  from an existing spatial object `mpsz2019`
grid <- terra::rast(mpsz2019, 
                   nrows = nrows, 
                   ncols = ncols)
# Generate coordinates for each cell of the raster 
xy <- terra::xyFromCell(grid,
                         1:ncell(grid))
# Converting coordinates of raster into a spatial (sf) object
coop <- st_as_sf(as.data.frame(xy),
                 coords = c("x","y"),
                 crs = st_crs(mpsz2019)) # Assign CRS based on mpsz2019
coop <- st_filter(coop, mpsz2019) # Filter to only only includes points within mpsz2019
```

This is the “experimental” variogram that should be shown to user, for user to assess what variables to adjust when fitting the variogram.

```{r}
#| code-fold: true
#| code-summary: "show code"
v <- variogram(MonthlyValue ~ 1, 
               data = variable_data_sf)
plot(v, cex = 1.5) # Experimental plot that should be shown to user
```

The below code chunk is used to tweak the fitted variogram:

```{r}
#| code-fold: true
#| code-summary: "show code"


# The different parameters have been tweaked and the results are presented below. 
# User will choose based on below variables 
fv <- fit.variogram(object = v,
                    model = vgm(psill = 5, model = "Gau", range = 5000, nugget = 0.1))

fv
plot(v, fv, cex = 1.5) #  visualise how well the observed data fit the model by plotting fv 


```

Final surface map

```{r}
#| code-fold: true
#| code-summary: "show code"
# perform spatial interpolation by using the newly derived model 
k <- gstat(formula = MonthlyValue ~ 1,
           data = variable_data_sf,
           model = fv)

# estimate the unknown grids
resp <- predict(k,coop)

resp$x <- st_coordinates(resp)[,1]
resp$y <- st_coordinates(resp)[,2]
resp$pred <- resp$var1.pred

# create a raster surface data object
kpred <- terra::rasterize(resp, grid,
                          field = "pred")

# map the interpolated variable raster, kpred
main_title <- ifelse(selected_var == "Daily Rainfall Total (mm)",
                paste("Distribution for Total Rainfall (mm) for", selected_month, selected_year),
                paste("Distribution for Average Mean Temperature (°C) for", selected_month, selected_year))
raster_title <- ifelse(selected_var == "Daily Rainfall Total (mm)",
                paste("Total Monthly Rainfall (mm)"),
                paste("Average Mean Temperature (°C)"))
tmap_options(check.and.fix = TRUE)
tmap_mode("plot")
tm_shape(kpred) + 
  tm_raster(alpha = 0.6, 
            palette = "viridis",
            title = raster_title) +
  tm_layout(main.title = main_title,
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
```
:::

When varying *model* option, there was noticeable variation in the final surface map:

![](images/clipboard-1050740677.png)

When varying *psill* option for different models, some models showed different surface maps while others looked similar:

![](images/clipboard-1960416952.png)

![](images/clipboard-1460570609.png)

When varying *range* option while keeping the other options static, there were different surface maps:

![](images/clipboard-408425766.png)

#### Automatic variogram modelling

Beside using gstat to perform variogram modelling manually, `autofirVariogram()` of **automap** package can be used to perform varigram modelling.

::: panel-tabset
#### Daily Rainfall Total (mm)

Data Prep:

```{r}
#| code-fold: true
#| code-summary: "show code"

# Indicate user variable
selected_var <- "Daily Rainfall Total (mm)" # "Daily Rainfall Total (mm)" or "Mean Temperature (°C)"
selected_month <- c("February") # User to specify Month
selected_year <- c("2023") # User to specify Year
interpolation_method <- "IDW"

# Create table for chosen variable and time period
variable_data <- weather_data %>%
  filter(Year %in% selected_year,
         Month_Name %in% selected_month) %>%
  group_by(Station, Year, Month, LAT, LONG) %>%
  summarise(MonthlyValue = if(selected_var == "Daily Rainfall Total (mm)") {
                sum(.data[[selected_var]], na.rm = TRUE) # aggregate by computing sum of rainfall for the month. 
              } else if (selected_var == "Mean Temperature (°C)") {
                mean(.data[[selected_var]], na.rm = TRUE) # aggregate by computing average temperature per month.
              }, .groups = 'drop')
  
# Make it spatial data
variable_data_sf <- st_as_sf(variable_data, 
                      coords = c("LONG", # X axis 
                                 "LAT"), # Y axis
                      crs= 4326) %>%
  st_transform(crs = 3414) # To project this into SVY21

# Create grid data object / Raster data
# Extract bounding box variables from mpsz2019 
bbox <- as.list(st_bbox(mpsz2019))

# Assume user is able to indicate resolution
res = 50 # User Input. The smaller the value the higher the resolution, but the higher the computational requirement (slower processing).
nrows =  (bbox$ymax - bbox$ymin)/res
ncols = (bbox$xmax - bbox$xmin)/res
# Create a raster layer, `grid`,  from an existing spatial object `mpsz2019`
grid <- terra::rast(mpsz2019, 
                   nrows = nrows, 
                   ncols = ncols)
# Generate coordinates for each cell of the raster 
xy <- terra::xyFromCell(grid,
                         1:ncell(grid))
# Converting coordinates of raster into a spatial (sf) object
coop <- st_as_sf(as.data.frame(xy),
                 coords = c("x","y"),
                 crs = st_crs(mpsz2019)) # Assign CRS based on mpsz2019
coop <- st_filter(coop, mpsz2019) # Filter to only only includes points within mpsz2019
```

Auto fit variogram:

```{r}
#| code-fold: true
#| code-summary: "show code"

v_auto <- autofitVariogram(MonthlyValue ~ 1, 
                           variable_data_sf)
plot(v_auto)
v_auto

```

Final surface map: 

```{r}
#| code-fold: true
#| code-summary: "show code"
# perform spatial interpolation by using the newly derived model 
k <- gstat(formula = MonthlyValue ~ 1, 
           model = v_auto$var_model,
           data = variable_data_sf)
k

# estimate the unknown grids
resp <- predict(k, coop)
resp$x <- st_coordinates(resp)[,1]
resp$y <- st_coordinates(resp)[,2]
resp$pred <- resp$var1.pred
resp$pred <- resp$pred

# create a raster surface data object
kpred <- terra::rasterize(resp, grid, 
                         field = "pred")

# map the interpolated variable raster, kpred
main_title <- ifelse(selected_var == "Daily Rainfall Total (mm)",
                paste("Distribution for Total Rainfall (mm) for", selected_month, selected_year),
                paste("Distribution for Average Mean Temperature (°C) for", selected_month, selected_year))
raster_title <- ifelse(selected_var == "Daily Rainfall Total (mm)",
                paste("Total Monthly Rainfall (mm)"),
                paste("Average Mean Temperature (°C)"))
tmap_options(check.and.fix = TRUE)
tmap_mode("plot")
tm_shape(kpred) + 
  tm_raster(alpha = 0.6, 
            palette = "viridis",
            title = raster_title) +
  tm_layout(main.title = main_title,
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
```

#### Mean Temperature (°C)

Data Prep:

```{r}
#| code-fold: true
# Indicate user variable
selected_var <- "Mean Temperature (°C)" # "Daily Rainfall Total (mm)" or "Mean Temperature (°C)"
selected_month <- c("February") # User to specify Month
selected_year <- c("2023") # User to specify Year
interpolation_method <- "IDW"

# Create table for chosen variable and time period
variable_data <- weather_data %>%
  filter(Year %in% selected_year,
         Month_Name %in% selected_month) %>%
  group_by(Station, Year, Month, LAT, LONG) %>%
  summarise(MonthlyValue = if(selected_var == "Daily Rainfall Total (mm)") {
                sum(.data[[selected_var]], na.rm = TRUE) # aggregate by computing sum of rainfall for the month. 
              } else if (selected_var == "Mean Temperature (°C)") {
                mean(.data[[selected_var]], na.rm = TRUE) # aggregate by computing average temperature per month.
              }, .groups = 'drop')
  
# Make it spatial data
variable_data_sf <- st_as_sf(variable_data, 
                      coords = c("LONG", # X axis 
                                 "LAT"), # Y axis
                      crs= 4326) %>%
  st_transform(crs = 3414) # To project this into SVY21

# Create grid data object / Raster data
# Extract bounding box variables from mpsz2019 
bbox <- as.list(st_bbox(mpsz2019))

# Assume user is able to indicate resolution
res = 50 # User Input. The smaller the value the higher the resolution, but the higher the computational requirement (slower processing).
nrows =  (bbox$ymax - bbox$ymin)/res
ncols = (bbox$xmax - bbox$xmin)/res
# Create a raster layer, `grid`,  from an existing spatial object `mpsz2019`
grid <- terra::rast(mpsz2019, 
                   nrows = nrows, 
                   ncols = ncols)
# Generate coordinates for each cell of the raster 
xy <- terra::xyFromCell(grid,
                         1:ncell(grid))
# Converting coordinates of raster into a spatial (sf) object
coop <- st_as_sf(as.data.frame(xy),
                 coords = c("x","y"),
                 crs = st_crs(mpsz2019)) # Assign CRS based on mpsz2019
coop <- st_filter(coop, mpsz2019) # Filter to only only includes points within mpsz2019
```

Auto fit variogram:

```{r}
#| code-fold: true
#| code-summary: "show code"

v_auto <- autofitVariogram(MonthlyValue ~ 1, 
                           variable_data_sf)
plot(v_auto)
v_auto

```

Final surface map: 

```{r}
#| code-fold: true
#| code-summary: "show code"
# perform spatial interpolation by using the newly derived model 
k <- gstat(formula = MonthlyValue ~ 1, 
           model = v_auto$var_model,
           data = variable_data_sf)
k

# estimate the unknown grids
resp <- predict(k, coop)
resp$x <- st_coordinates(resp)[,1]
resp$y <- st_coordinates(resp)[,2]
resp$pred <- resp$var1.pred
resp$pred <- resp$pred

# create a raster surface data object
kpred <- terra::rasterize(resp, grid, 
                         field = "pred")

# map the interpolated variable raster, kpred
main_title <- ifelse(selected_var == "Daily Rainfall Total (mm)",
                paste("Distribution for Total Rainfall (mm) for", selected_month, selected_year),
                paste("Distribution for Average Mean Temperature (°C) for", selected_month, selected_year))
raster_title <- ifelse(selected_var == "Daily Rainfall Total (mm)",
                paste("Total Monthly Rainfall (mm)"),
                paste("Average Mean Temperature (°C)"))
tmap_options(check.and.fix = TRUE)
tmap_mode("plot")
tm_shape(kpred) + 
  tm_raster(alpha = 0.6, 
            palette = "viridis",
            title = raster_title) +
  tm_layout(main.title = main_title,
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
```
:::

# UI Design (Work in Progress)

## Clustering and Group Forecasting module

This module will contain three tabs:

-   Horizon Plot
-   Time Series Clustering
-   Forecasting Grouped Time Series

### Tab 1: Horizon Plot

\<to be updated\>

![](images/clipboard-3800135536.png)

### Tab 2: Time Series Clustering

\<to be updated\>

![](images/clipboard-2326084344.png)

### Tab 3: Forecasting Grouped Time Series

\<to be updated\>

## Spatial Interpolation Module

\<to be updated\>

# References

-   [Time series clustering](https://geomoer.github.io/moer-mpg-data-analysis/unit10/unit10-03_time_series_clustering.html)
-   https://plotly.com/ggplot2/dendrogram/
-   https://www.rdocumentation.org/packages/factoextra/versions/1.0.7/topics/fviz_dend
-   [In-class Exercise 7](https://isss608-vaa-demo.netlify.app/in-class_ex/in-class_ex07/in-class_ex07-isomap)
