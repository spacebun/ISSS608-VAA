[
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "In this exercise, I will use a selected module of the Shiny application for the Group Project component to fulfill the following objectives:\n\nTo evaluate and determine what are the necessary R packages needed for the Shiny application,\nTo prepare and test the specific R codes can be run and returned the correct output as expected,\nTo determine the parameters and outputs that will be exposed on the Shiny applications, and\nTo select the appropriate Shiny UI components for exposing the parameters determined above.\n\n\n\nFor our project, we aim to create a Shiny app with user-friendly functionalities, to effectively visualize and analyze climate data.\nThe R Shiny app will consists of three modules:\n\nEDA and CDA module\nUnivariate Forecasting module\nSpatial Interpolation module\n\nFor this exercise, we will focus on the last Spatial Interpolation module. In this geospatial module, two spatial interpolation techniques are presented for user to estimate weather conditions (Rainfall or Temperature) at unmonitored locations. The module is designed to allow users to interactively adjust the inputs for each technique, providing a hands-on opportunity to explore and understand the impact of different parameters on the interpolation results."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#load-packages",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#load-packages",
    "title": "Take-home Exercise 4",
    "section": "2.1 Load packages",
    "text": "2.1 Load packages\n\npacman::p_load(gstat)\n\n\nlibrary(tmap)\n\n\npacman::p_load(tidyverse, naniar, imputeTS, DT, knitr, lubridate,\n               sf, terra, viridis, automap,\n               ggplot2, patchwork, ggthemes, ggiraph, plotly)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#import-data",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#import-data",
    "title": "Take-home Exercise 4",
    "section": "3.1 Import data",
    "text": "3.1 Import data\n\nraw_weather_data &lt;- read_csv(\"data/climate_historical_daily_records.csv\")\n\nDetails of dataset:\n\n\n\n\n\n\n\n\n\nDataset\nDescription\nPeriod\nSource\n\n\n\n\nraw_weather_data\nClimate Historical Daily Records for 63 stations in Singapore\n2014-2023\nhttp://www.weather.gov.sg/climate-historical-daily/\n\n\n\nThis dataset was retrieved from the Meteorological Service Singapore site, and had some basic pre-processing steps performed in python due to the large amount of files:\n\nCombine all downloaded CSV files into one dataframe.\nPerforming cleaning to merge data of columns with slightly different names due to case sensitivity (e.g., “min” vs. “Min”)\n(‘Highest 30 Min Rainfall (mm)’, ‘Highest 30 min Rainfall (mm)’)\n(‘Highest 60 Min Rainfall (mm)’, ‘Highest 60 min Rainfall (mm)’)\n(‘Highest 120 Min Rainfall (mm)’, ‘Highest 120 min Rainfall (mm)’)\nAdd the latitude and longitude of each station to the dataframe."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#check-structure-with-glimpse",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#check-structure-with-glimpse",
    "title": "Take-home Exercise 4",
    "section": "3.2 Check structure with glimpse()",
    "text": "3.2 Check structure with glimpse()\n\nglimpse(raw_weather_data)\n\nRows: 202,976\nColumns: 15\n$ Station                         &lt;chr&gt; \"Paya Lebar\", \"Paya Lebar\", \"Paya Leba…\n$ Year                            &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 20…\n$ Month                           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Day                             &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,…\n$ `Daily Rainfall Total (mm)`     &lt;chr&gt; \"0.0\", \"0.0\", \"2.2\", \"0.6\", \"10.5\", \"3…\n$ `Highest 30 Min Rainfall (mm)`  &lt;chr&gt; \"\\u0097\", \"\\u0097\", \"\\u0097\", \"\\u0097\"…\n$ `Highest 60 Min Rainfall (mm)`  &lt;chr&gt; \"\\u0097\", \"\\u0097\", \"\\u0097\", \"\\u0097\"…\n$ `Highest 120 Min Rainfall (mm)` &lt;chr&gt; \"\\u0097\", \"\\u0097\", \"\\u0097\", \"\\u0097\"…\n$ `Mean Temperature (°C)`         &lt;chr&gt; \"\\u0097\", \"\\u0097\", \"\\u0097\", \"\\u0097\"…\n$ `Maximum Temperature (°C)`      &lt;chr&gt; \"29.5\", \"31.7\", \"31.1\", \"32.3\", \"27.0\"…\n$ `Minimum Temperature (°C)`      &lt;chr&gt; \"24.8\", \"25.0\", \"25.1\", \"23.7\", \"23.8\"…\n$ `Mean Wind Speed (km/h)`        &lt;chr&gt; \"15.8\", \"16.5\", \"14.9\", \"8.9\", \"11.9\",…\n$ `Max Wind Speed (km/h)`         &lt;chr&gt; \"35.3\", \"37.1\", \"33.5\", \"35.3\", \"33.5\"…\n$ LAT                             &lt;dbl&gt; 1.3524, 1.3524, 1.3524, 1.3524, 1.3524…\n$ LONG                            &lt;dbl&gt; 103.9007, 103.9007, 103.9007, 103.9007…\n\n\nThere are 202, 976 rows, and 15 columns in the dataset. In the next few steps, we will drop specific columns and rows based on the project focus."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#filter-dataset-for-desired-period",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#filter-dataset-for-desired-period",
    "title": "Take-home Exercise 4",
    "section": "3.3 Filter dataset for desired period",
    "text": "3.3 Filter dataset for desired period\nWhile the dataset contains 10 years of data from 2014 to 2023, we will focus on the most recent dataset for a 3 year period, from 2021 to 2023. This period was chosen to maximise the overall availability of data across the stations.\n\nraw_weather_data &lt;- raw_weather_data %&gt;%\n  filter(Year &gt;= 2021)\nprint(paste(\"The dataset covers the period from\", min(raw_weather_data$Year, na.rm = TRUE), \"to\", max(raw_weather_data$Year, na.rm = TRUE), \".\"))\n\n[1] \"The dataset covers the period from 2021 to 2023 .\""
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#drop-unused-columns",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#drop-unused-columns",
    "title": "Take-home Exercise 4",
    "section": "3.4 Drop unused columns",
    "text": "3.4 Drop unused columns\nWe will not be using all 15 columns for this project. The following columns will be dropped:\n\nHighest 30 Min Rainfall (mm)\nHighest 60 Min Rainfall (mm)\nHighest 1200 Min Rainfall (mm)\nMean Wind Speed (km/h)\nMax Wind Speed (km/h)\n\n\n# Drop columns\nraw_weather_data &lt;- raw_weather_data %&gt;%\n  select(-c(`Highest 30 Min Rainfall (mm)`, \n            `Highest 60 Min Rainfall (mm)`, \n            `Highest 120 Min Rainfall (mm)`,\n            `Mean Wind Speed (km/h)`,\n            `Max Wind Speed (km/h)`))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#remove-rows-for-specific-stations",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#remove-rows-for-specific-stations",
    "title": "Take-home Exercise 4",
    "section": "3.5 Remove rows for specific Stations",
    "text": "3.5 Remove rows for specific Stations\nThe Meteorological Service Singapore also provides a file, Station Records that has some information on the availability of data for each station. After examining the station records file, we found that 41 stations had missing information for some variables. We will hence drop rows for these stations.\n\n\nshow code\n# Drop rows of 41 stations\n# Define the station names to remove\nstations_to_remove &lt;- c(\"Macritchie Reservoir\", \"Lower Peirce Reservoir\", \"Pasir Ris (West)\", \"Kampong Bahru\", \"Jurong Pier\", \"Ulu Pandan\", \"Serangoon\", \"Jurong (East)\", \"Mandai\", \"Upper Thomson\", \"Buangkok\", \"Boon Lay (West)\", \"Bukit Panjang\", \"Kranji Reservoir\", \"Tanjong Pagar\", \"Admiralty West\", \"Queenstown\", \"Tanjong Katong\", \"Chai Chee\", \"Upper Peirce Reservoir\", \"Kent Ridge\", \"Somerset (Road)\", \"Punggol\", \"Tuas West\", \"Simei\", \"Toa Payoh\", \"Tuas\", \"Bukit Timah\", \"Yishun\", \"Buona Vista\", \"Pasir Ris (Central)\", \"Jurong (North)\", \"Choa Chu Kang (West)\", \"Serangoon North\", \"Lim Chu Kang\", \"Marine Parade\", \"Choa Chu Kang (Central)\", \"Dhoby Ghaut\", \"Nicoll Highway\", \"Botanic Garden\", \"Whampoa\")\n\n# Remove rows with the specified station names\nraw_weather_data &lt;- raw_weather_data[!raw_weather_data$Station %in% stations_to_remove, ]\n\n# Print the number of stations left\nprint(sprintf(\"There were %d stations removed.There are %d stations left.\", length(stations_to_remove), n_distinct(raw_weather_data$Station)))\n\n\n[1] \"There were 41 stations removed.There are 21 stations left.\""
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#check-for-duplicates",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#check-for-duplicates",
    "title": "Take-home Exercise 4",
    "section": "3.6 Check for duplicates",
    "text": "3.6 Check for duplicates\n\n\nshow code\n# Identify duplicates\nduplicates &lt;- raw_weather_data[duplicated(raw_weather_data[c(\"Station\", \"Year\", \"Month\", \"Day\")]) | duplicated(raw_weather_data[c(\"Station\", \"Year\", \"Month\", \"Day\")], fromLast = TRUE), ]\n\n# Check if 'duplicates' dataframe is empty\nif (nrow(duplicates) == 0) {\n  print(\"The combination of Station Name, Year, Month, and Day is unique.\")\n} else {\n  print(\"There are duplicates in the combination of Station Name, Year, Month, and Day. Showing duplicated rows:\")\n  print(duplicates)\n}\n\n\n[1] \"The combination of Station Name, Year, Month, and Day is unique.\""
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#check-and-handle-missing-values",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#check-and-handle-missing-values",
    "title": "Take-home Exercise 4",
    "section": "3.7 Check and handle missing values",
    "text": "3.7 Check and handle missing values\n\n3.7.1 First check for missing values\nMissing values in this dataset can be represented by:\n\n\\u0097\nNA\n-\n\nWe first replace these values with actual NA values:\n\nraw_weather_data &lt;- raw_weather_data %&gt;%\n  mutate(across(where(is.character), ~na_if(.x, \"\\u0097\"))) %&gt;%\n  mutate(across(where(is.character), ~na_if(.x, \"NA\"))) %&gt;%\n  mutate(across(where(is.character), ~na_if(.x, \"-\")))\n\nNext, we visualize the missing values in the dataset:\n\n\nshow code\nvis_miss(raw_weather_data)\n\n\n\n\n\n\n\n\n\nWe will take steps to handle the missing data.\n\n\n3.7.2 Remove Stations with significant missing data\nWe have identified two checks to make:\n\nCheck which stations have no recorded data for entire months.\nCheck which stations have more than 7 consecutive days of missing data\n\nFor both these checks, we will remove the entire station from the dataset as it would not be practical to impute such large amounts of missing values.\n\n3.7.2.1 Identify and remove Stations with no recorded data for entire months\nSome stations have no recorded data for entire months, as summarised in the table below:\n\n\nshow code\n# Create complete combination of Station, Year, and Month\nall_combinations &lt;- expand.grid(\n  Station = unique(raw_weather_data$Station),\n  Year = 2021:2023,\n  Month = 1:12\n)\n\n# Left join this with the original weather data to identify missing entries\nmissing_months &lt;- all_combinations %&gt;%\n  left_join(raw_weather_data, by = c(\"Station\", \"Year\", \"Month\")) %&gt;%\n  # Use is.na() to check for rows that didn't have a match in the original data\n  filter(is.na(Day)) %&gt;%\n  # Select only the relevant columns for the final output\n  select(Station, Year, Month)\n\n# Create a summary table that lists out the missing months\nmissing_months_summary &lt;- missing_months %&gt;%\n  group_by(Station, Year) %&gt;%\n  summarise(MissingMonths = toString(sort(unique(Month))), .groups = 'drop')\n\nkable(missing_months_summary)\n\n\n\n\n\nStation\nYear\nMissingMonths\n\n\n\n\nKhatib\n2022\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n\n\nKhatib\n2023\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n\n\n\n\n\nWe hence drop these stations from our dataset:\n\n\nshow code\nraw_weather_data &lt;- anti_join(raw_weather_data, missing_months, by = \"Station\")\n\nprint(sprintf(\"The folowing %d stations were dropped: %s\", n_distinct(missing_months$Station), paste(unique(missing_months$Station), collapse = \", \")))\n\n\n[1] \"The folowing 1 stations were dropped: Khatib\"\n\n\nshow code\nprint(sprintf(\"There are %d stations left: \", n_distinct(raw_weather_data$Station)))\n\n\n[1] \"There are 20 stations left: \"\n\n\nshow code\nkable(unique(raw_weather_data$Station),\n      row.names = TRUE,\n      col.names = \"Station\",\n      caption = \"List of Remaining Stations\")\n\n\n\nList of Remaining Stations\n\n\n\nStation\n\n\n\n\n1\nPaya Lebar\n\n\n2\nSemakau Island\n\n\n3\nAdmiralty\n\n\n4\nPulau Ubin\n\n\n5\nEast Coast Parkway\n\n\n6\nMarina Barrage\n\n\n7\nAng Mo Kio\n\n\n8\nNewton\n\n\n9\nTuas South\n\n\n10\nPasir Panjang\n\n\n11\nJurong Island\n\n\n12\nChoa Chu Kang (South)\n\n\n13\nTengah\n\n\n14\nChangi\n\n\n15\nSeletar\n\n\n16\nTai Seng\n\n\n17\nJurong (West)\n\n\n18\nClementi\n\n\n19\nSentosa Island\n\n\n20\nSembawang\n\n\n\n\n\n\n\n3.7.2.2 Identify and remove Stations with excessive missing values\nIf there are any missing values, we can try to impute these missing values. However, if there are 7 or more consecutive values missing, we will remove these stations first.\n\n\nshow code\n# Define a helper function to count the number of 7 or more consecutive NAs\ncount_seven_consecutive_NAs &lt;- function(x) {\n  na_runs &lt;- rle(is.na(x))\n  total_consecutive_NAs &lt;- sum(na_runs$lengths[na_runs$values & na_runs$lengths &gt;= 7])\n  return(total_consecutive_NAs)\n}\n\n# Apply the helper function to each relevant column within grouped data\nweather_summary &lt;- raw_weather_data %&gt;%\n  group_by(Station, Year, Month) %&gt;%\n  summarise(across(-Day, ~ count_seven_consecutive_NAs(.x), .names = \"count_consec_NAs_{.col}\"), .groups = \"drop\")\n\n# Filter to keep only rows where there is at least one column with 7 or more consecutive missing values\nweather_summary_with_consecutive_NAs &lt;- weather_summary %&gt;%\n  filter(if_any(starts_with(\"count_consec_NAs_\"), ~ . &gt; 0))\n\n# View the result\nprint(sprintf(\"There are %d stations with 7 or more consecutive missing values.\", n_distinct(weather_summary_with_consecutive_NAs$Station)))\n\n\n[1] \"There are 9 stations with 7 or more consecutive missing values.\"\n\n\nshow code\n# kable(weather_summary_with_consecutive_NAs)\ndatatable(weather_summary_with_consecutive_NAs, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 10, scrollX=T),\n          caption = 'Details of stations with &gt;=7 missing values')\n\n\n\n\n\n\nWe hence drop these stations from our dataset:\n\n\nshow code\nraw_weather_data &lt;- anti_join(raw_weather_data, weather_summary_with_consecutive_NAs, by = \"Station\")\n\nprint(sprintf(\"The folowing %d stations were dropped: %s\", n_distinct(weather_summary_with_consecutive_NAs$Station), paste(unique(weather_summary_with_consecutive_NAs$Station), collapse = \", \")))\n\n\n[1] \"The folowing 9 stations were dropped: Admiralty, Clementi, Jurong Island, Marina Barrage, Paya Lebar, Semakau Island, Sembawang, Sentosa Island, Tengah\"\n\n\nshow code\nprint(sprintf(\"There are %d stations left: \", n_distinct(raw_weather_data$Station)))\n\n\n[1] \"There are 11 stations left: \"\n\n\nshow code\nkable(unique(raw_weather_data$Station),\n      row.names = TRUE,\n      col.names = \"Station\",\n      caption = \"List of Remaining Stations\")\n\n\n\nList of Remaining Stations\n\n\n\nStation\n\n\n\n\n1\nPulau Ubin\n\n\n2\nEast Coast Parkway\n\n\n3\nAng Mo Kio\n\n\n4\nNewton\n\n\n5\nTuas South\n\n\n6\nPasir Panjang\n\n\n7\nChoa Chu Kang (South)\n\n\n8\nChangi\n\n\n9\nSeletar\n\n\n10\nTai Seng\n\n\n11\nJurong (West)\n\n\n\n\n\n\n\n\n3.7.3 Second check for missing values\nFrom the check below we see there are still missing values in our data. We will impute these values in the next step.\n\n\nshow code\nvis_miss(raw_weather_data)\n\n\n\n\n\n\n\n\n\n\n\n3.7.4 Impute missing values\nTo handle the missing values for the remaining Stations, we will impute missing values using simple moving average from imputeTS package.\n\n3.7.4.1 Create Date column\n\nraw_weather_data &lt;- raw_weather_data %&gt;%\n  mutate(Date = as.Date(paste(Year, Month, Day, sep = \"-\"))) %&gt;%\n  relocate(Date, .after = 1)\n\n\n\n3.7.4.2 Using imputeTS package\n\n# Define the weather variables to loop through\nweather_variables &lt;- c(\"Daily Rainfall Total (mm)\", \"Mean Temperature (°C)\", \"Maximum Temperature (°C)\", \"Minimum Temperature (°C)\")\n\n# Ensure raw_weather_data is correctly copied to a new data frame for imputation\nweather_data_imputed &lt;- raw_weather_data\n\n# Loop through each weather variable to impute missing values\nfor(variable in weather_variables) {\n  # Convert variable to numeric, ensuring that the conversion warnings are handled if necessary\n  weather_data_imputed[[variable]] &lt;- as.numeric(as.character(weather_data_imputed[[variable]]))\n  \n  # Impute missing values using a moving average\n  weather_data_imputed &lt;- weather_data_imputed %&gt;%\n    group_by(Station) %&gt;%\n    arrange(Station, Date) %&gt;%\n    mutate(\"{variable}\" := round(na_ma(.data[[variable]], k = 7, weighting = \"simple\"), 1)) %&gt;%\n    ungroup()\n}\n\n\n\n\n3.7.5 Final visual check for missing values\n\n\nshow code\nvis_miss(weather_data_imputed)\n\n\n\n\n\n\n\n\n\n\n\n3.7.6 Add specific columns to data\nThese columns are added as they may be used in plots later.\n\nweather_data_imputed &lt;- weather_data_imputed %&gt;% \n  mutate(Date_mine = make_date(2023, month(Date), day(Date)),\n         Month_Name = factor(months(Date), levels = month.name),\n         Week = isoweek(Date),\n         Weekday = wday(Date)) %&gt;%\n  select(1:5, Date_mine, Month_Name, Week, Weekday, everything()) # Re-order columns"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#sec-viewcleaneddataset",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#sec-viewcleaneddataset",
    "title": "Take-home Exercise 4",
    "section": "3.8 Summary of cleaned data",
    "text": "3.8 Summary of cleaned data\n\n3.8.1 Details of stations and time period of data\n\n\nshow code\ntime_period_start &lt;- min(weather_data_imputed$Date)\ntime_period_end &lt;- max(weather_data_imputed$Date)\ncat(\"\\nThe time period of the dataset is from\", format(time_period_start, \"%Y-%m-%d\"),\"to\", format(time_period_end, \"%Y-%m-%d\"), \"\\n\")\n\n\n\nThe time period of the dataset is from 2021-01-01 to 2023-12-31 \n\n\nshow code\nprint(sprintf(\"There are %d stations: \", n_distinct(weather_data_imputed$Station)))\n\n\n[1] \"There are 11 stations: \"\n\n\nshow code\nkable(unique(weather_data_imputed$Station),\n      row.names = TRUE,\n      col.names = \"Station\",\n      caption = \"List of Stations\")\n\n\n\nList of Stations\n\n\n\nStation\n\n\n\n\n1\nAng Mo Kio\n\n\n2\nChangi\n\n\n3\nChoa Chu Kang (South)\n\n\n4\nEast Coast Parkway\n\n\n5\nJurong (West)\n\n\n6\nNewton\n\n\n7\nPasir Panjang\n\n\n8\nPulau Ubin\n\n\n9\nSeletar\n\n\n10\nTai Seng\n\n\n11\nTuas South\n\n\n\n\n\n\n\n3.8.2 Check structure with glimpse()\n\nglimpse(weather_data_imputed)\n\nRows: 12,045\nColumns: 15\n$ Station                     &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", …\n$ Date                        &lt;date&gt; 2021-01-01, 2021-01-02, 2021-01-03, 2021-…\n$ Year                        &lt;dbl&gt; 2021, 2021, 2021, 2021, 2021, 2021, 2021, …\n$ Month                       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Day                         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,…\n$ Date_mine                   &lt;date&gt; 2023-01-01, 2023-01-02, 2023-01-03, 2023-…\n$ Month_Name                  &lt;fct&gt; January, January, January, January, Januar…\n$ Week                        &lt;dbl&gt; 53, 53, 53, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, …\n$ Weekday                     &lt;dbl&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, …\n$ `Daily Rainfall Total (mm)` &lt;dbl&gt; 94.4, 114.4, 5.2, 0.0, 0.0, 0.0, 1.6, 12.6…\n$ `Mean Temperature (°C)`     &lt;dbl&gt; 24.0, 23.0, 23.9, 25.1, 26.9, 26.9, 24.4, …\n$ `Maximum Temperature (°C)`  &lt;dbl&gt; 26.2, 24.5, 25.3, 27.9, 31.6, 30.3, 26.0, …\n$ `Minimum Temperature (°C)`  &lt;dbl&gt; 21.5, 21.6, 23.2, 23.1, 24.1, 25.1, 23.8, …\n$ LAT                         &lt;dbl&gt; 1.3764, 1.3764, 1.3764, 1.3764, 1.3764, 1.…\n$ LONG                        &lt;dbl&gt; 103.8492, 103.8492, 103.8492, 103.8492, 10…\n\n\n\n\n3.8.3 View dataset as interactive table\n\n\nshow code\ndatatable(weather_data_imputed, \n            class= \"compact\",\n            rownames = FALSE,\n            width=\"100%\", \n            options = list(pageLength = 10, scrollX=T),\n          caption = 'Cleaned and imputed weather dataset')"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#save-cleaned-data-to-.rds",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#save-cleaned-data-to-.rds",
    "title": "Take-home Exercise 4",
    "section": "3.9 Save cleaned data to .rds",
    "text": "3.9 Save cleaned data to .rds\n\nwrite_rds(weather_data_imputed, \"data/weather_imputed_11stations.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#import-cleaned-data",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#import-cleaned-data",
    "title": "Take-home Exercise 4",
    "section": "3.10 Import cleaned data",
    "text": "3.10 Import cleaned data\nThe below code can be used to import the cleaned data.\n\nweather_data &lt;- read_rds(\"data/weather_imputed_11stations.rds\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#overview-of-exploratory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#overview-of-exploratory-data-analysis",
    "title": "Take-home Exercise 4 (Work in Progress)",
    "section": "5.1 Overview of Exploratory Data Analysis",
    "text": "5.1 Overview of Exploratory Data Analysis\nThere are 4 variables in the dataset, weather_data, that we can use for time series analysis:\n\nDaily Rainfall Total (mm)\nMean Temperature (°C)\nMinimum Temperature (°C)\nMaximum Temperature (°C)\n\nFor this exercise, we will utilise Horizon Plots to visualize multiple time series data. Future work on the project may include other exploratory plots for visualizing multiple time series data."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#horizon-plots",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#horizon-plots",
    "title": "Take-home Exercise 4 (Work in Progress)",
    "section": "5.1 Horizon Plots",
    "text": "5.1 Horizon Plots\nWe will utilise Horizon Plots to explore and visualize multiple time series data at a glance.\nHorizon plots are a visualization technique used for displaying multiple time series data. Since the data contains data across multiple years for multiple stations, it would be valuable to allow users to use horizon plots to visually explore patterns or trends across multiple time series data.\nOverview of user options:\n\nSelect daily variable to plot\nPlot across stations for different time granularity\nChoose months to plot (Compare 1 month, Compare 3 months, Compare 6 months)\nChoose years to plot (Compare 1 year, Compare 2 years, Compare 3 years)\nPlot across different time periods for a single stations\n\n\n5.1.1 Horizon Plot with reproducible code\n\n\nshow code\ncompareAcrossHorizon = \"Stations\"\nselectedYear = 2021 # Need to update below code for selection of year\nselected_var = \"Mean Temperature (°C)\"\ndate_breaks = \"3 month\"\ntitle &lt;- ifelse(compareAcrossHorizon == \"Years\",\n                paste(selected_var, \"for\", selectedStation, \"across the years 2021 to 2023\"),\n                paste(selected_var, \"for\", selectedYear, \"across station(s)\"))\n\n# Compute origin and  horizon scale cutpoints: \ncutpoints &lt;- weather_data %&gt;% \n  mutate(\n    outlier = between(\n      .data[[selected_var]], \n      quantile(.data[[selected_var]], 0.25, na.rm = TRUE) -\n        1.5 * IQR(.data[[selected_var]], na.rm = TRUE),\n      quantile(.data[[selected_var]], 0.75, na.rm = TRUE) +\n        1.5 * IQR(.data[[selected_var]], na.rm = TRUE))) %&gt;% \n  filter(outlier)\n\n# ori &lt;- sum(range(cutpoints[[selected_var]]))/2\n# sca &lt;- seq(range(cutpoints[[selected_var]])[1], range(cutpoints[[selected_var]])[2], length.out = seq_length)[-seq_length/2]\n\nori &lt;- sum(range(cutpoints[[selected_var]]))/2\nsca &lt;- seq(range(cutpoints[[selected_var]])[1], range(cutpoints[[selected_var]])[2], length.out = 7)[-4]\n\n\nori &lt;- round(ori, 2) # The origin, rounded to 2 decimal places\nsca &lt;- round(sca, 2) # The horizon scale cutpoints\n\nweather_data %&gt;% ggplot() +\n  geom_horizon(aes(x = Date,\n                   y = .data[[selected_var]],\n                   fill = after_stat(Cutpoints)),\n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = T) +\n  facet_grid(`Station`~.) +\n  theme_few() +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = date_breaks, date_labels = \"%b%y\") +\n  ggtitle(title)\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Horizon Plot of Mean Temperature (°C) across different weather stations, for 2021 - 2023\n\n\nshow code\n# Step 1: compute origin and  horizon scale cutpoints: \ncutpoints &lt;- weather_data %&gt;% \n  mutate(\n    outlier = between(\n      `Mean Temperature (°C)`, \n      quantile(`Mean Temperature (°C)`, 0.25, na.rm = TRUE) -\n        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE),\n      quantile(`Mean Temperature (°C)`, 0.75, na.rm = TRUE) +\n        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE))) %&gt;% \n  filter(outlier)\nori &lt;- sum(range(cutpoints$`Mean Temperature (°C)`))/2\nsca &lt;- seq(range(cutpoints$`Mean Temperature (°C)`)[1], \n           range(cutpoints$`Mean Temperature (°C)`)[2], \n           length.out = 7)[-4]\nori &lt;- round(ori, 2) # The origin, rounded to 2 decimal places\nsca &lt;- round(sca, 2) # The horizon scale cutpoints\nweather_data %&gt;% ggplot() +\n  geom_horizon(aes(x = Date,\n                   y = `Mean Temperature (°C)`,\n                   fill = after_stat(Cutpoints)),\n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = T) +\n  facet_grid(`Station`~.) +\n  theme_few() +\n  theme(panel.spacing.y=unit(0, \"lines\"), strip.text.y = element_text(\n    size = 5, angle = 0, hjust = 0),\n    # legend.position = 'none',\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size=7),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n    scale_x_date(expand=c(0,0), date_breaks = \"3 month\", date_labels = \"%b%y\") +\n  ggtitle('Mean Temperature (°C) across different weather stations (2021 - 2023)')\n\n\n\n\n\n\n\n\n\n\n\n5.1.3 Horizon Plot of Mean Temperature (°C) across different weather stations for a specified year, 2023\n\n\nshow code\n# Ensure data has Year, Month, Date column + Date_mine column\nweather_data &lt;- weather_data %&gt;% \n  mutate(Year = year(Date)) %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  mutate(Day = day(Date)) %&gt;%\n  mutate(Date_mine = make_date(2023, month(Date), day(Date)))\n\n# Filter for specified year only\nweather_data_2023 &lt;- weather_data %&gt;% \n  filter(Year == 2023) \n\n# Step 1: compute origin and  horizon scale cutpoints: \ncutpoints &lt;- weather_data_2023 %&gt;% \n  mutate(\n    outlier = between(\n      `Mean Temperature (°C)`, \n      quantile(`Mean Temperature (°C)`, 0.25, na.rm = TRUE) -\n        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE),\n      quantile(`Mean Temperature (°C)`, 0.75, na.rm = TRUE) +\n        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE))) %&gt;% \n  filter(outlier)\n\nori &lt;- sum(range(cutpoints$`Mean Temperature (°C)`))/2\nsca &lt;- seq(range(cutpoints$`Mean Temperature (°C)`)[1], \n           range(cutpoints$`Mean Temperature (°C)`)[2], \n           length.out = 7)[-4]\n\nori &lt;- round(ori, 2) # The origin, rounded to 2 decimal places\nsca &lt;- round(sca, 2) # The horizon scale cutpoints\n\n# Plot horizon plot\nweather_data_2023 %&gt;% ggplot() +\n  geom_horizon(aes(x = Date_mine, \n                   y = `Mean Temperature (°C)`,\n                   fill = after_stat(Cutpoints)), \n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = T) +\n  facet_grid(~Station ~ .) +\n  theme_few() +\n  theme(\n    panel.spacing.y = unit(0, \"lines\"),\n    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n  scale_x_date(expand = c(0, 0), \n               date_breaks = \"1 month\", \n               date_labels = \"%b\") +\n  xlab('Date') +\n  ggtitle('Mean Temperature (°C) for the year 2023',\n          'across the stations')\n\n\n\n\n\n\n\n\n\n\n\n5.1.4 Horizon Plot of Mean Temperature (°C) For Ang Mo Kio across the years\n\n\nshow code\n# Step 0: Filter for specified station only \nang_mo_kio_data &lt;- weather_data %&gt;% \n  filter(Station == \"Seletar\")\n\n# Step 1: compute origin and  horizon scale cutpoints: \ncutpoints &lt;- ang_mo_kio_data %&gt;% \n  mutate(\n    outlier = between(\n      `Mean Temperature (°C)`, \n      quantile(`Mean Temperature (°C)`, 0.25, na.rm = TRUE) -\n        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE),\n      quantile(`Mean Temperature (°C)`, 0.75, na.rm = TRUE) +\n        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE))) %&gt;% \n  filter(outlier)\n\nori &lt;- sum(range(cutpoints$`Mean Temperature (°C)`))/2\nsca &lt;- seq(range(cutpoints$`Mean Temperature (°C)`)[1], \n           range(cutpoints$`Mean Temperature (°C)`)[2], \n           length.out = 7)[-4]\n\nori &lt;- round(ori, 2) # The origin, rounded to 2 decimal places\nsca &lt;- round(sca, 2) # The horizon scale cutpoints\n\n# Step 2: Ensure data has Year, Month, Date column. Also create a Date_mine column for comparing across years\nang_mo_kio_data &lt;- ang_mo_kio_data %&gt;% \n  mutate(Year = year(Date)) %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  mutate(Day = day(Date)) %&gt;%\n  mutate(Date_mine = make_date(2023, month(Date), day(Date)))\n\n# Step 3: Plot horizon plot\nang_mo_kio_data %&gt;% ggplot() +\n  geom_horizon(aes(x = Date_mine, \n                   y = `Mean Temperature (°C)`,\n                   fill = after_stat(Cutpoints)), \n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = T) +\n  facet_grid(~Year ~ .) +\n  theme_few() +\n  theme(\n    panel.spacing.y = unit(0, \"lines\"),\n    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n  scale_x_date(expand = c(0, 0), \n               date_breaks = \"1 month\", \n               date_labels = \"%b\") +\n  xlab('Date') +\n  ggtitle('Mean Temperature (°C) For Ang Mo Kio',\n          'across the years 2021 to 2023')\n\n\n\n\n\n\n\n\n\n\n\n5.1.5 Horizon Plot of Mean Temperature (°C) across average of all weather stations from 2021 to 2023\n\n\nshow code\n# Step 0: Compute average of Mean Temperature (°C) across all stations\nweather_data_all_stations &lt;- weather_data %&gt;%\n  group_by(Date) %&gt;%\n  summarise(`Mean Temperature (°C)` = mean(`Mean Temperature (°C)`))\n\n# Step 1: compute origin and  horizon scale cutpoints: \ncutpoints &lt;- weather_data_all_stations %&gt;% \n  mutate(\n    outlier = between(\n      `Mean Temperature (°C)`, \n      quantile(`Mean Temperature (°C)`, 0.25, na.rm = TRUE) -\n        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE),\n      quantile(`Mean Temperature (°C)`, 0.75, na.rm = TRUE) +\n        1.5 * IQR(`Mean Temperature (°C)`, na.rm = TRUE))) %&gt;% \n  filter(outlier)\n\nori &lt;- sum(range(cutpoints$`Mean Temperature (°C)`))/2\nsca &lt;- seq(range(cutpoints$`Mean Temperature (°C)`)[1], \n           range(cutpoints$`Mean Temperature (°C)`)[2], \n           length.out = 7)[-4]\n\nori &lt;- round(ori, 2) # The origin, rounded to 2 decimal places\nsca &lt;- round(sca, 2) # The horizon scale cutpoints\n\n# Step 2: Ensure data has Year, Month, Date column. Also create a Date_mine column for comparing across years\nweather_data_all_stations &lt;- weather_data_all_stations %&gt;% \n  mutate(Year = year(Date)) %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  mutate(Day = day(Date)) %&gt;%\n  mutate(Date_mine = make_date(2023, month(Date), day(Date)))\n\n# Step 3: Plot horizon plot\nweather_data_all_stations %&gt;% ggplot() +\n  geom_horizon(aes(x = Date_mine, \n                   y = `Mean Temperature (°C)`,\n                   fill = after_stat(Cutpoints)), \n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = T) +\n  facet_grid(~Year ~ .) +\n  theme_few() +\n  theme(\n    panel.spacing.y = unit(0, \"lines\"),\n    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n  scale_x_date(expand = c(0, 0), \n               date_breaks = \"1 month\", \n               date_labels = \"%b\") +\n  xlab('Date') +\n  ggtitle('Mean Temperature (°C) across across all weather stations', \n          'from 2021 to 2023')\n\n\n\n\n\n\n\n\n\n\n\n5.1.6 Horizon Plot of Daily Rainfall Total across all weather stations from 2021 to 2023\nThis plot may not make sense as we may need to sum up rainfall for each day across all weather stations first before plotting.\n\n\nshow code\n# Step 1: compute origin and  horizon scale cutpoints: \ncutpoints &lt;- weather_data %&gt;% \n  mutate(\n    outlier = between(\n      `Daily Rainfall Total (mm)`, \n      quantile(`Daily Rainfall Total (mm)`, 0.25, na.rm = TRUE) -\n        1.5 * IQR(`Daily Rainfall Total (mm)`, na.rm = TRUE),\n      quantile(`Daily Rainfall Total (mm)`, 0.75, na.rm = TRUE) +\n        1.5 * IQR(`Daily Rainfall Total (mm)`, na.rm = TRUE))) %&gt;% \n  filter(outlier)\n\nori &lt;- sum(range(cutpoints$`Daily Rainfall Total (mm)`))/2\nsca &lt;- seq(range(cutpoints$`Daily Rainfall Total (mm)`)[1], \n           range(cutpoints$`Daily Rainfall Total (mm)`)[2], \n           length.out = 7)[-4]\n\nori &lt;- round(ori, 2) # The origin, rounded to 2 decimal places\nsca &lt;- round(sca, 2) # The horizon scale cutpoints\n\n\n# Step 2: Ensure data has Year, Month, Date column. Also create a Date_mine column for comparing across years\nweather_data &lt;- weather_data %&gt;% \n  mutate(Year = year(Date)) %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  mutate(Day = day(Date)) %&gt;%\n  mutate(Date_mine = make_date(2023, month(Date), day(Date)))\n\n# Step 3: Plot horizon plot\nweather_data %&gt;% ggplot() +\n  geom_horizon(aes(x = Date_mine, \n                   y = `Daily Rainfall Total (mm)`,\n                   fill = after_stat(Cutpoints)), \n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = T) +\n  facet_grid(~Year ~ .) +\n  theme_few() +\n  theme(\n    panel.spacing.y = unit(0, \"lines\"),\n    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n  scale_x_date(expand = c(0, 0), \n               date_breaks = \"1 month\", \n               date_labels = \"%b\") +\n  xlab('Date') +\n  ggtitle('Daily Rainfall Total across all weather stations', \n          'from 2021 to 2023')\n\n\n\n\n\n\n\n\n\n\n\n5.1.7 Horizon Plot of Daily Rainfall Total (mm) across all the stations for a specified year\n\n\nshow code\n# Ensure data has Year, Month, Date column\nweather_data &lt;- weather_data %&gt;% \n  mutate(Year = year(Date)) %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  mutate(Day = day(Date)) %&gt;%\n  mutate(Date_mine = make_date(2023, month(Date), day(Date)))\n\n# Filter for specified year only\nweather_data_2023 &lt;- weather_data %&gt;% \n  filter(Year == 2023)\n\n# Step 1: compute origin and  horizon scale cutpoints: \ncutpoints &lt;- weather_data_2023 %&gt;% \n  mutate(\n    outlier = between(\n      `Daily Rainfall Total (mm)`, \n      quantile(`Daily Rainfall Total (mm)`, 0.25, na.rm = TRUE) -\n        1.5 * IQR(`Daily Rainfall Total (mm)`, na.rm = TRUE),\n      quantile(`Daily Rainfall Total (mm)`, 0.75, na.rm = TRUE) +\n        1.5 * IQR(`Daily Rainfall Total (mm)`, na.rm = TRUE))) %&gt;% \n  filter(outlier)\n\nori &lt;- sum(range(cutpoints$`Daily Rainfall Total (mm)`))/2\nsca &lt;- seq(range(cutpoints$`Daily Rainfall Total (mm)`)[1], \n           range(cutpoints$`Daily Rainfall Total (mm)`)[2], \n           length.out = 7)[-4]\n\nori &lt;- round(ori, 2) # The origin, rounded to 2 decimal places\nsca &lt;- round(sca, 2) # The horizon scale cutpoints\n\n# Plot horizon plot\nweather_data_2023 %&gt;% ggplot() +\n  geom_horizon(aes(x = Date_mine, \n                   y = `Daily Rainfall Total (mm)`,\n                   fill = after_stat(Cutpoints)), \n               origin = ori, horizonscale = sca) +\n  scale_fill_hcl(palette = 'RdBu', reverse = T) +\n  facet_grid(~Station ~ .) +\n  theme_few() +\n  theme(\n    panel.spacing.y = unit(0, \"lines\"),\n    strip.text.y = element_text(size = 7, angle = 0, hjust = 0),\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.border = element_blank()\n    ) +\n  scale_x_date(expand = c(0, 0), \n               date_breaks = \"1 month\", \n               date_labels = \"%b\") +\n  xlab('Date') +\n  ggtitle('Daily Rainfall Total (mm) for the year 2023',\n          'across the stations')"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#overview-1",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#overview-1",
    "title": "Take-home Exercise 4 (Work in Progress)",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nClustering is the practice of finding hidden patterns or similar groups in data.\nTo perform time series clustering, we will use the function tsclust from dtwclust.\nhttps://cran.r-project.org/web/packages/dtwclust/vignettes/dtwclust.pdf\nWe will divide cluster analysis into the following parts:\n\nFormat data into list of series for input to clustering function\nChoosing the clustering method: “partitional”, “hierarchical”, “tadpole” or “fuzzy”\nChoosing distance computation: Ignored for type = “tadpole”.\nDetermining a measure to quantify the similarity between observations\nSelecting the desired number of clusters"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#clustering-of-stations",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#clustering-of-stations",
    "title": "Take-home Exercise 4 (Work in Progress)",
    "section": "6.2 Clustering of stations",
    "text": "6.2 Clustering of stations\nWe will use the variable temp_data , and perform cluster analysis of time series of daily temperature readings from different weather stations for the same period.\n\n6.2.1 Format data into list of series\nUsers should be allowed to cluster by:\n\nStations, for multiple stations and multiple years\nMonths, for a single station and a given year\n\n\nFor a single selected station and year, by monthsFor multiple stations and multiple years, by station\n\n\n\n\nshow code\n# Extract the target variable, temperature data into a variable\ntemp_data &lt;- weather_data %&gt;%\n  select(c(Station, Date, Year, Month, Month_Name, Day, `Mean Temperature (°C)`))\n\n# Filter for station(s) and year(s) e.g., \"Changi\", for 2023\nstations_selected = \"Changi\"\nyears_selected = \"2021\"\nstation_temp_data &lt;- temp_data %&gt;% \n  filter(Station %in% stations_selected) %&gt;%\n  filter(Year %in% years_selected)\n\n# Ensure the data is ordered by Month_Name and by Date\nstation_temp_data &lt;- station_temp_data %&gt;%\n  arrange(Month_Name, Date)\n\n# Format data into list of series, by Month_Name\nlist_of_series &lt;- split(station_temp_data$`Mean Temperature (°C)`, station_temp_data$Month_Name)\n\n\n\n\n\n\nshow code\n# Extract the target variable, temperature data into a variable\ntemp_data &lt;- weather_data %&gt;%\n  select(c(Station, Date, Year, Month, Day, `Mean Temperature (°C)`))\n\n# Filter for all stations and years \nstations_selected = unique(weather_data$Station) # note: not station_selected\nyears_selected = unique(weather_data$Year) # note: not stations_selected\nstation_temp_data &lt;- temp_data %&gt;% \n  filter(Station %in% stations_selected) %&gt;%\n  filter(Year %in% years_selected)\n\n\n# Group-wise (Station-by-Station) Normalization, applying Min-Max scaling within each station group\nstation_temp_data &lt;- station_temp_data %&gt;%\n  group_by(Station) %&gt;%\n  mutate(Normalized_Temp = (`Mean Temperature (°C)` - min(`Mean Temperature (°C)`, na.rm = TRUE)) / \n                            (max(`Mean Temperature (°C)`, na.rm = TRUE) - min(`Mean Temperature (°C)`, na.rm = TRUE))) %&gt;%\n  ungroup()\n\n\n# Ensure the data is ordered by Station and by Date\nstation_temp_data &lt;- station_temp_data %&gt;%\n  arrange(Station, Date)\n\n# Format data into list of series, by Station\nlist_of_series &lt;- split(station_temp_data$Normalized_Temp, station_temp_data$Station)\n\n\n\n\n\n\n\n6.2.2 Exploring clustering methods\n\n6.2.2.1 Hierarchical\nAssume clustering for several stations across 2021 - 2023.\n\n\nshow code\nn_cluster = 2 # Number of desired clusters. Should not show if cluster_type == \"partitional\"\ncluster_type = \"hierarchical\" #  \"partitional\", \"hierarchical\", \"tadpole\" or \"fuzzy\"\ndistance = \"dtw_basic\" # Ignored for type = \"tadpole\"\ncontrol = hierarchical_control(method = \"complete\") \n\nc &lt;- tsclust(series = list_of_series, \n        type = cluster_type,  \n        k = n_cluster, \n        distance = distance, \n        control = control\n        )\n\np &lt;- fviz_dend(c, k = n_cluster, # Cut in n_cluster groups\n          cex = 0.6, # label size\n          k_colors = c(\"jco\"),\n          color_labels_by_k = FALSE,  # color labels by groups\n          rect_border = \"jco\",\n          rect = TRUE,\n          rect_fill = TRUE,\n          horiz = TRUE)\np\n\n\n\n\n\n\n\n\n\nVarying number of desired clusters, k, produces the following:\n&lt;to be updated&gt;\nKeeping k at x and varying the distance computation produces the following:\n&lt;to be updated&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#tab-1-horizon-plot",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#tab-1-horizon-plot",
    "title": "Take-home Exercise 4 (Work in Progress)",
    "section": "8.1 Tab 1: Horizon Plot",
    "text": "8.1 Tab 1: Horizon Plot\n&lt;to be updated&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#tab-2-clustering",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#tab-2-clustering",
    "title": "Take-home Exercise 4 (Work in Progress)",
    "section": "8.2 Tab 2: Clustering",
    "text": "8.2 Tab 2: Clustering\n&lt;to be updated&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#tab-3-forecasting",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#tab-3-forecasting",
    "title": "Take-home Exercise 4 (Work in Progress)",
    "section": "8.3 Tab 3: Forecasting",
    "text": "8.3 Tab 3: Forecasting\n&lt;to be updated&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608 Visual Analytics and Applications",
    "section": "",
    "text": "ISSS608 Visual Analytics and Applications\nWelcome to my portfolio for the Visual Analytics and Applications course, taught by Prof. Kam Tin Seong. This website is a collection of exercises and assignments completed throughout the course."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex7/In-class_Ex7.html",
    "href": "In-class_Ex/In-class_Ex7/In-class_Ex7.html",
    "title": "In-class Exercise 7",
    "section": "",
    "text": "Objective of this exercise is to create interpolation for rainfall data."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex7/In-class_Ex7.html#overview",
    "href": "In-class_Ex/In-class_Ex7/In-class_Ex7.html#overview",
    "title": "In-class Exercise 7",
    "section": "",
    "text": "Objective of this exercise is to create interpolation for rainfall data."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex7/In-class_Ex7.html#load-libraries",
    "href": "In-class_Ex/In-class_Ex7/In-class_Ex7.html#load-libraries",
    "title": "In-class Exercise 7",
    "section": "Load libraries",
    "text": "Load libraries\n\npacman::p_load(sf, \n               terra, \n               gstat,# used for interpolation\n               tmap, \n               viridis, \n               tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex7/In-class_Ex7.html#import-data",
    "href": "In-class_Ex/In-class_Ex7/In-class_Ex7.html#import-data",
    "title": "In-class Exercise 7",
    "section": "Import data",
    "text": "Import data\nRead in aspatial data, where latitude and longitude of the data is provided for each station:\n\nrfstations &lt;- read_csv(\"data/aspatial/RainfallStation.csv\")\n\n\nglimpse(rfstations)\n\nRows: 63\nColumns: 3\n$ Station   &lt;chr&gt; \"Admiralty\", \"Admiralty (West)\", \"Ang Mo Kio\", \"Boon Lay (Ea…\n$ Latitude  &lt;dbl&gt; 1.4439, 1.4582, 1.3764, 1.3302, 1.3275, 1.3087, 1.3837, 1.38…\n$ Longitude &lt;dbl&gt; 103.7854, 103.7953, 103.8492, 103.7205, 103.7042, 103.8180, …\n\n\nRead in attribute data and select for desired variables:\n\nrfdata&lt;- read_csv(\"data/aspatial/DAILYDATA_202402.csv\") %&gt;% \n  select(c(1,5)) %&gt;% # select column 1 to 5\n  group_by(Station) %&gt;% # Group by rainfall station\n  summarise(MONTHSUM = sum(`Daily Rainfall Total (mm)`)) %&gt;%\n  ungroup()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex7/In-class_Ex7.html#data-preparation",
    "href": "In-class_Ex/In-class_Ex7/In-class_Ex7.html#data-preparation",
    "title": "In-class Exercise 7",
    "section": "Data preparation",
    "text": "Data preparation\nDo a left join to add the latitude and longitude to our data.\n\nrfdata &lt;- rfdata %&gt;%\n  left_join(rfstations)\n\nMake it spatial data:\n\nrfdata_sf &lt;- st_as_sf(rfdata,\n                      coords = c(\"Longitude\", # X axis must be called first \n                                 \"Latitude\"),\n                      crs = 4236) %&gt;% # Y axis\n              st_transform(crs = 3414) # To project this into SVY21\n\nUse tmap to plot\n\n# This is a shape file, read in with st_read\nmpsz2019 &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\spacebun\\ISSS608-VAA\\In-class_Ex\\In-class_Ex7\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\nNotice it has a MULTIPOLYGON for the geometry column:\n\nWithin each there are multple polygon pairs:\n\nUse tmap function:\n\ntmap_options(check.and.fix = TRUE) # Some geo data may be topological data. Use this code to fix topo error without changing the data. \ntmap_mode(\"view\") # Make your map interactive\n\n# Instead of using polygon we use tm_shape\ntm_shape(mpsz2019) + # Plot boundary map\n  tm_borders() +\n  tm_shape(rfdata_sf) + # Plot rainfall station \n  tm_dots(col = \"MONTHSUM\") # Use color to differentiate the total rainfall data\n\n\n\n\ntmap_mode(\"plot\")\n\nYou can call tm_polygon() and put the fill or you can do tm_fill(). (for your consideration\nIn order to generate interpolation, we need a raster layer. We need to define the number of cols and rows in this later.\n\nmpsz2019\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\nInputs required for raster layer:\n\ncalculate difference between xmax, xmin and ymax, ymin\nchoose resolutions (e.g. 50m by 50m)\n\nUse these inputs to calculate the number of rows and number columns:\n\nnrows = (ymax - ymin)/res\nncol = (xmax - xmin)/res\n\nFront end user sees the option to adjust resolution (res) of the grid.\ntake ymax - ymin divided by the raster grid resolution (e.g. 50 meters or 100 meters)\nWe will create this raster layer:\n\n# Create a raster layer, grid,  from an existing spatial object mpsz2019\ngrid &lt;- terra::rast(mpsz2019, \n                   nrows = 690, # (ymax - ymin)/50 (where 50 is chosen res)\n                   ncols = 1075) # (xmax - xmin)/50 (where 50 is chosen res)\n\n# Generate coordinates for each cell of the raster \nxy &lt;- terra::xyFromCell(grid,\n                         1:ncell(grid))\n\n\n# Converting coordinates of raster into a spatial (sf) object\ncoop &lt;- st_as_sf(as.data.frame(xy),\n                 coords = c(\"x\",\"y\"),\n                 crs = st_crs(mpsz2019)) # Assign CRS based on mpsz2019\ncoop &lt;- st_filter(coop,mpsz2019) # Filter to only only includes points within mpsz2019"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex7/In-class_Ex7.html#method-1-inverse-distance-weighted-interpolation",
    "href": "In-class_Ex/In-class_Ex7/In-class_Ex7.html#method-1-inverse-distance-weighted-interpolation",
    "title": "In-class Exercise 7",
    "section": "Method 1: inverse distance weighted interpolation",
    "text": "Method 1: inverse distance weighted interpolation\n\nres &lt;- gstat(formula = MONTHSUM ~ 1, # specify  dependent variable\n             locations = rfdata_sf,\n             nmax= 15, # number of nearest neighbors considered for interpolation. Parameter to expose for UI\n             set = list(idp = 0))\n\n# Predict values at new locations (coop) based on the spatial data (rfdata_sf)\nresp &lt;- predict(res,coop)\n\n[inverse distance weighted interpolation]\n\n# Extract x and y coordinates and the predicted values into resp\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\n\n\n# Predictions are rasterized over a predefined grid, using the mean of predictions where multiple values fall into a single grid cell\n# This results in a raster layer pred representing spatially interpolated values of MONTHSUM.\npred &lt;- terra:: rasterize(resp, grid, # resp$pred contains the spatially interpolated variables. grid is raster layer.\n                         field=\"pred\",\n                         fun=\"mean\")\n\nPlot the result\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(pred) +\n  tm_raster(alpha=0.6,\n            palette = \"viridis\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex7/In-class_Ex7.html#method-2-ordinary-kriging",
    "href": "In-class_Ex/In-class_Ex7/In-class_Ex7.html#method-2-ordinary-kriging",
    "title": "In-class Exercise 7",
    "section": "Method 2: ordinary kriging",
    "text": "Method 2: ordinary kriging\nVariogram model, no covariates\nWe are going to use three parameters of the gstat function:\n\nformula—The prediction “formula” specifying the dependent and the independent variables (covariates)\ndata—The calibration data\nmodel—The variogram model\n\n\nv &lt;- variogram(log(MONTHSUM) ~ 1,\n               data = rfdata_sf)\nplot(v)\n\n\n\n\n\n\n\n\n\nfv &lt;- fit.variogram(object = v,\n                    model = vgm(psill = 0.5, model = \"Sph\",\n                                range = 900, nugget = 0.1))\n\nfv\n\n  model psill range\n1   Nug   0.1     0\n2   Sph   0.5   900\n\nplot(v, fv, cex = 1.5)\n\n\n\n\n\n\n\n\n\nk &lt;- gstat(formula = log(MONTHSUM) ~ 1,\n           data = rfdata_sf,\n           model = fv)\n\n\nresp &lt;- predict(k,coop)\n\n[using ordinary kriging]\n\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\n\nkpred &lt;- terra::rasterize(resp, grid,\n                          field = \"pred\")\n\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) +\n  tm_raster(alpha = 0.6,\n            palette = \"viridis\")\n\n\n\n\n\n\n\n\nYou don’t need to expose every option."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#data-preparation-1",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#data-preparation-1",
    "title": "Take-home Exercise 4 (Work in Progress)",
    "section": "8.1 Data Preparation",
    "text": "8.1 Data Preparation\nDesign considerations for user control:\n\nUse radioButtons function to select only one variable (“Daily Rainfall Total (mm)”, “Mean Temperature (°C)”)\nUse radioButtons function to select only one type of time window\nThis function should be conditional.\n\nFirst, read in shape file for Singapore. This step should be done before UI and server code.\n\n# This is a shape file, read in with st_read\nmpsz2019 &lt;- st_read(dsn = \"data/geospatial\",\n                    layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\spacebun\\ISSS608-VAA\\Take-home_Ex\\Take-home_Ex4\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\nNext, aggregate weather data for selected variable and time window\nFor rainfall, we should aggregate by computing sum of rainfall for the month. For temperature, we should aggregate by computing average temperature per month.\n\n# Indicate user variable\nselected_var &lt;- \"Daily Rainfall Total (mm)\" # \"Daily Rainfall Total (mm)\" or \"Mean Temperature (°C)\"\nselected_month &lt;- c(\"January\")\nselected_year &lt;- c(\"2021\")\n\n# Create tbl for chosen variable and time period\nvariable_data &lt;- weather_data %&gt;%\n  filter(Year %in% selected_year,\n         Month_Name %in% selected_month) %&gt;%\n  group_by(Station, Year, Month, LAT, LONG) %&gt;%\n  summarise(MonthlyValue = if(selected_var == \"Daily Rainfall Total (mm)\") {\n                sum(.data[[selected_var]], na.rm = TRUE)\n              } else {\n                mean(.data[[selected_var]], na.rm = TRUE)\n              }, .groups = 'drop')\n  \n# Make it spatial data\nvariable_data_sf &lt;- st_as_sf(variable_data,\n                      coords = c(\"LONG\", # X axis  \n                                 \"LAT\"),\n                      crs = 4236) %&gt;% # Y axis\n              st_transform(crs = 3414) # To project this into SVY21\n\n# Plot variable with tmap\ntmap_options(check.and.fix = TRUE) # Use this code to fix topo error without changing the data. \ntmap_mode(\"view\") # Make map interactive\n\n\ntm_shape(mpsz2019) + # Plot boundary map\n  tm_borders() +\n  tm_shape(variable_data_sf) + # Plot station \n  tm_dots(col = \"MonthlyValue\") # Use color to differentiate\n\n\n\n\ntmap_mode(\"plot\") # Change mode back\n\n\n# Create Raster Data\n# Extract bounding box variables from mpsz2019 \nbbox &lt;- as.list(st_bbox(mpsz2019))\n\n# Assume user is able to indicate resolution\nres = 50 # User Input\n\nnrows =  (bbox$ymax - bbox$ymin)/res\nncols = (bbox$xmax - bbox$xmin)/res\n\n# Create a raster layer, `grid`,  from an existing spatial object `mpsz2019`\ngrid &lt;- terra::rast(mpsz2019, \n                   nrows = nrows, \n                   ncols = ncols)\n\n# Generate coordinates for each cell of the raster \nxy &lt;- terra::xyFromCell(grid,\n                         1:ncell(grid))\n\n# Converting coordinates of raster into a spatial (sf) object\ncoop &lt;- st_as_sf(as.data.frame(xy),\n                 coords = c(\"x\",\"y\"),\n                 crs = st_crs(mpsz2019)) # Assign CRS based on mpsz2019\n\ncoop &lt;- st_filter(coop, mpsz2019) # Filter to only only includes points within mpsz2019"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#method-1-inverse-distance-weighted-interpolation",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#method-1-inverse-distance-weighted-interpolation",
    "title": "Take-home Exercise 4 (Work in Progress)",
    "section": "8.2 Method 1: inverse distance weighted interpolation",
    "text": "8.2 Method 1: inverse distance weighted interpolation\n\n8.2.1 Varying nmax\n\nnmax = 1nmax = 3\n\n\n\n\nCode\n# Set value\nnmax = 1\n\n# Create gstat object\nres &lt;- gstat(formula = MonthlyValue ~ 1, # specify  dependent variable\n             locations = variable_data_sf,\n             nmax = nmax, # number of nearest neighbors considered for interpolation. Parameter to expose for UI\n             set = list(idp = 0))\n\n# Predict values at new locations (coop) based on the spatial data (rfdata_sf)\nresp &lt;- predict(res,coop)\n\n\n[inverse distance weighted interpolation]\n\n\nCode\n# Plot The Result\n\n# Extract x and y coordinates and the predicted values into resp\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\n\n\n# Predictions are rasterized over a predefined grid, using the mean of predictions where multiple values fall into a single grid cell\n# This results in a raster layer pred representing spatially interpolated values of MONTHSUM.\npred &lt;- terra:: rasterize(resp, grid, # resp$pred contains the spatially interpolated variables. grid is raster layer.\n                         field=\"pred\",\n                         fun=\"mean\")\n\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(pred) +\n  tm_raster(alpha=0.6,\n            palette = \"viridis\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Set value\nnmax = 3\n\n# Create gstat object\nres &lt;- gstat(formula = MonthlyValue ~ 1, # specify  dependent variable\n             locations = variable_data_sf,\n             nmax = nmax, # number of nearest neighbors considered for interpolation. Parameter to expose for UI\n             set = list(idp = 0))\n\n# Predict values at new locations (coop) based on the spatial data (rfdata_sf)\nresp &lt;- predict(res,coop)\n\n\n[inverse distance weighted interpolation]\n\n\nCode\n# Plot The Result\n\n# Extract x and y coordinates and the predicted values into resp\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\n\n\n# Predictions are rasterized over a predefined grid, using the mean of predictions where multiple values fall into a single grid cell\n# This results in a raster layer pred representing spatially interpolated values of MONTHSUM.\npred &lt;- terra:: rasterize(resp, grid, # resp$pred contains the spatially interpolated variables. grid is raster layer.\n                         field=\"pred\",\n                         fun=\"mean\")\n\n\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(pred) +\n  tm_raster(alpha=0.6,\n            palette = \"viridis\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#clustering-and-group-forecasting-module",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#clustering-and-group-forecasting-module",
    "title": "Take-home Exercise 4 (Work in Progress)",
    "section": "9.1 Clustering and Group Forecasting module",
    "text": "9.1 Clustering and Group Forecasting module\nThis module will contain three tabs:\n\nHorizon Plot\nTime Series Clustering\nForecasting Grouped Time Series\n\n\n9.1.1 Tab 1: Horizon Plot\n&lt;to be updated&gt;\n\n\n\n9.1.2 Tab 2: Time Series Clustering\n&lt;to be updated&gt;\n\n\n\n9.1.3 Tab 3: Forecasting Grouped Time Series\n&lt;to be updated&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#geospatial-module",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#geospatial-module",
    "title": "Take-home Exercise 4 (Work in Progress)",
    "section": "9.2 Geospatial Module",
    "text": "9.2 Geospatial Module\n&lt;to be updated&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#spatial-interpolation-module-1",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#spatial-interpolation-module-1",
    "title": "Take-home Exercise 4 (Work in Progress)",
    "section": "9.2 Spatial Interpolation Module",
    "text": "9.2 Spatial Interpolation Module\n&lt;to be updated&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#project-details",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#project-details",
    "title": "Take-home Exercise 4",
    "section": "",
    "text": "For our project, we aim to create a Shiny app with user-friendly functionalities, to effectively visualize and analyze climate data.\nThe R Shiny app will consists of three modules:\n\nEDA and CDA module\nUnivariate Forecasting module\nSpatial Interpolation module\n\nFor this exercise, we will focus on the last Spatial Interpolation module. In this geospatial module, two spatial interpolation techniques are presented for user to estimate weather conditions (Rainfall or Temperature) at unmonitored locations. The module is designed to allow users to interactively adjust the inputs for each technique, providing a hands-on opportunity to explore and understand the impact of different parameters on the interpolation results."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#prototype-module-report",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#prototype-module-report",
    "title": "Take-home Exercise 4",
    "section": "4.1 Prototype module report",
    "text": "4.1 Prototype module report\n\n4.1.1 Read in shape file\nFirst, read in planning subzone boundary data (shape file) for Singapore.\nThis step should be done before running UI and server code.\n\nmpsz2019 &lt;- st_read(dsn = \"data/geospatial\",## This is a shape file, read in with st_read\n                    layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\spacebun\\ISSS608-VAA\\Take-home_Ex\\Take-home_Ex4\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n4.1.2 Data preparation\n\n4.1.2.1 Select for variable and time resolution\nWe will first focus on Rainfall data over a one-month period, assessing how various model parameters influence this variable.\nThe code chunk below selects monthly Rainfall data for analysis, and aggregates it to compute monthly totals or averages for a specific year and month.\n\n# User-defined criteria\nselected_var &lt;- \"Daily Rainfall Total (mm)\" # \"Daily Rainfall Total (mm)\" or \"Mean Temperature (°C)\"\ntime_resolution &lt;- \"Month\" # Options: \"Day\", \"Month\", \"Year\"\nselected_date &lt;- \"2023-02-15\" # To be single select and reactive (only show if Day)\nselected_month &lt;- c(\"February\") # To be single select and reactive (only show if Month)\nselected_year &lt;- c(\"2023\") # To be single select and reactive (only show if Month or Year)\n\n# Adjusting filter based on time_resolution\nif (time_resolution == \"Day\") {\n  # Filter for a specific day, assuming Date is in YYYY-MM-DD format\n  variable_data &lt;- weather_data %&gt;%\n    filter(Date == as.Date(selected_date))\n} else if (time_resolution == \"Month\") {\n  # Filter for a specific month\n  variable_data &lt;- weather_data %&gt;%\n    filter(Year == selected_year,\n           Month_Name == selected_month)\n} else if (time_resolution == \"Year\") {\n  # Filter for an entire year\n  variable_data &lt;- weather_data %&gt;%\n    filter(Year == selected_year)\n}\n\n# Group and summarise data, selecting for the correctly variable\nvariable_data &lt;- variable_data %&gt;%\n  group_by(Station, Year, LAT, LONG) %&gt;%\n  summarise(MonthlyValue = if(grepl(\"Rainfall\", selected_var)) {\n                sum(.data[[selected_var]], na.rm = TRUE) # Sum or average based on the variable\n              } else if (grepl(\"Temperature\", selected_var)) {\n                mean(.data[[selected_var]], na.rm = TRUE)\n              }, .groups = 'drop')\n\n\n\n4.1.2.2 Visualize data on a map\nNext, we will visualize the data on a map:\n\n\nshow code\n# Make it spatial data\nvariable_data_sf &lt;- st_as_sf(variable_data, \n                      coords = c(\"LONG\", # X axis \n                                 \"LAT\"), # Y axis\n                      crs= 4326) %&gt;%\n  st_transform(crs = 3414) # To project this into SVY21\n\n# Visualize variable with tmap\ncol_title = \ntmap_options(check.and.fix = TRUE) # Use this code to fix topo error without changing the data. \ntmap_mode(\"view\") # Make map interactive\ntm_shape(mpsz2019) + # Plot boundary map\n  tm_borders() +\ntm_shape(variable_data_sf) + \n  tm_dots(col = 'MonthlyValue') # Use color to differentiate\n\n\n\n\n\n\n\n\n4.1.2.3 Create raster\n\n# Create grid data object / Raster data\n# Extract bounding box variables from mpsz2019 \nbbox &lt;- as.list(st_bbox(mpsz2019))\n\n# Assume user is able to indicate resolution\nres = 50 # User Input. The smaller the value the higher the resolution, but the higher the computational requirement (slower processing).\nnrows =  (bbox$ymax - bbox$ymin)/res\nncols = (bbox$xmax - bbox$xmin)/res\n# Create a raster layer, `grid`,  from an existing spatial object `mpsz2019`\ngrid &lt;- rast(mpsz2019, \n                   nrows = nrows, \n                   ncols = ncols)\n# Generate coordinates for each cell of the raster \nxy &lt;- xyFromCell(grid,\n                         1:ncell(grid))\n# Converting coordinates of raster into a spatial (sf) object\ncoop &lt;- st_as_sf(as.data.frame(xy),\n                 coords = c(\"x\",\"y\"),\n                 crs = st_crs(mpsz2019)) # Assign CRS based on mpsz2019\ncoop &lt;- st_filter(coop, mpsz2019) # Filter to only only includes points within mpsz2019\n\n\n\n\n4.1.3 Method 1: Inverse Distance Weighted interpolation\n\n\nshow code\n# IDW Method\n\nnmax = 3 # Set value\n\n# Create gstat object\nres &lt;- gstat(formula = MonthlyValue ~ 1, # specify  dependent variable\n             locations = variable_data_sf,\n             nmax = nmax, # number of nearest neighbors considered for interpolation. Parameter to expose for UI\n             set = list(idp = 0))\n\n# Predict values at new locations (coop) based on the spatial data (rfdata_sf)\nresp &lt;- predict(res,coop)\n\n\n[inverse distance weighted interpolation]\n\n\nshow code\n# Plot The Result\n# Extract x and y coordinates and the predicted values into resp\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\n\n# Predictions are rasterized over a predefined grid, using the mean of predictions where multiple values fall into a single grid cell\n# This results in a raster layer pred representing spatially interpolated values\npred &lt;- rasterize(resp, grid, field=\"pred\", fun=\"mean\") # resp$pred contains the spatially interpolated variables. grid is raster layer.\n\n\n# Map the interpolated variable raster, pred\nmain_title &lt;- ifelse(selected_var == \"Daily Rainfall Total (mm)\",\n                paste(\"Distribution for Total Rainfall (mm) for\", time_resolution, \"of\", selected_month, selected_year),\n                paste(\"Distribution for Average Mean Temperature (°C) for\", time_resolution, \"of\", selected_month, selected_year))\nraster_title &lt;- ifelse(selected_var == \"Daily Rainfall Total (mm)\",\n                paste(\"Total Rainfall (mm)\"),\n                paste(\"Average \", selected_var))\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(pred) + \n  tm_raster(title = raster_title, \n            alpha = 0.6, palette = \"viridis\") +\n  tm_layout(main.title = main_title, main.title.position = \"center\", main.title.size = 1.2, \n            legend.height = 0.45, legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) + tm_scale_bar() + tm_grid(alpha =0.2)\n\n\n\n\n\n\n\n\n\nVarying nmax produced the following results:\n\n\n\n4.1.4 Method 2: Kriging\nKriging can be understood as a two-step process:\n\nfirst, the spatial covariance structure of the sampled points is determined by fitting a variogram; and\nsecond, weights derived from this covariance structure are used to interpolate values for unsampled points or blocks across the spatial field.\n\nThe below code chunk show the available variogram models in gstat.\n\nshow.vgms(par.strip.text = list(cex = 0.75))\n\n\n\n\n\n\n\n\n\n4.1.4.1 Experimental variogram\nThis is the “experimental” variogram that should be shown to user, for user to assess what variables to adjust when fitting the variogram.\n\n\nshow code\nv &lt;- variogram(MonthlyValue ~ 1, \n               data = variable_data_sf)\nplot(v, cex = 1.5) # Experimental plot that should be shown to user\n\n\n\n\n\n\n\n\n\n\n\n4.1.4.2 Fitted variogram\nThe below code chunk is used to tweak the fitted variogram:\n\n\nshow code\n# The different parameters have been tweaked and the results are presented below. \n# User will choose based on below variables \nfv &lt;- fit.variogram(object = v,\n                    model = vgm(psill = 10, model = \"Gau\", range = 5000, nugget = 0.1))\n\nfv\n\n\n  model    psill    range\n1   Nug    0.000    0.000\n2   Gau 1794.622 4644.178\n\n\nshow code\nplot(v, fv, cex = 1.5) #  visualise how well the observed data fit the model by plotting fv \n\n\n\n\n\n\n\n\n\n\n\n4.1.4.3 Result: Isohyet map\n\n\nshow code\n# perform spatial interpolation by using the newly derived model \nk &lt;- gstat(formula = MonthlyValue ~ 1,\n           data = variable_data_sf,\n           model = fv)\n\n# estimate the unknown grids\nresp &lt;- predict(k,coop)\n\n\n[using ordinary kriging]\n\n\nshow code\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\n\n# create a raster surface data object\nkpred &lt;- rasterize(resp, grid,\n                          field = \"pred\")\n\n# map the interpolated variable raster, kpred\nmain_title &lt;- ifelse(selected_var == \"Daily Rainfall Total (mm)\",\n                paste(\"Distribution for Total Rainfall (mm) for\", selected_month, selected_year),\n                paste(\"Distribution for Average Mean Temperature (°C) for\", selected_month, selected_year))\nraster_title &lt;- ifelse(selected_var == \"Daily Rainfall Total (mm)\",\n                paste(\"Total Monthly Rainfall (mm)\"),\n                paste(\"Average Mean Temperature (°C)\"))\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(title = raster_title,\n            alpha = 0.6, palette = \"viridis\") +\n  tm_layout(main.title = main_title, main.title.position = \"center\", main.title.size = 1.2, \n            legend.height = 0.45, legend.width = 0.35, frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) + tm_scale_bar() + tm_grid(alpha =0.2)\n\n\n\n\n\n\n\n\n\n\n\n4.1.4.4 Prediction variance\n\n\nshow code\n# Assuming resp already contains the predictions and variance from the predict function\n# Extracting the variance\nresp$variance &lt;- resp$var1.var\n\n# Create a raster surface data object for variance\nkvar &lt;- rasterize(resp, grid, field = \"variance\")\n\n# Map the interpolated variance raster, kvar\nvariance_title &lt;- \"Prediction Variance\"\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kvar) + \n  tm_raster(title = variance_title,\n            alpha = 0.6, palette = \"viridis\") +\n  tm_layout(main.title = \"Kriging Prediction Variance\", main.title.position = \"center\", main.title.size = 1.2, \n            legend.height = 0.45, legend.width = 0.35, frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) + tm_scale_bar() + tm_grid(alpha =0.2)\n\n\n\n\n\n\n\n\n\nWhen varying model option, there was noticeable variation in the final surface map:\n\nWhen varying psill option for different models, some models showed different surface maps while others looked similar:\n\n\nWhen varying range option while keeping the other options static, there were different surface maps:\n\n\n\n\n4.1.5 Automatic variogram modelling\nBeside using gstat to perform variogram modelling manually, autofitVariogram() of automap package can be used to perform varigram modelling.\n\n4.1.5.1 Auto fit variogram\n\n\nshow code\nv_auto &lt;- autofitVariogram(MonthlyValue ~ 1, variable_data_sf)\nplot(v_auto)\n\n\n\n\n\n\n\n\n\n\n\n4.1.5.2 Result: Isohyet map\n\n\nshow code\n# perform spatial interpolation by using the newly derived model \nk &lt;- gstat(formula = MonthlyValue ~ 1, \n           model = v_auto$var_model,\n           data = variable_data_sf)\nk\n\n\ndata:\nvar1 : formula = MonthlyValue`~`1 ; data dim = 11 x 3\nvariograms:\n        model     psill    range\nvar1[1]   Nug 1491.0299    0.000\nvar1[2]   Exp  693.4758 8155.059\n\n\nshow code\n# estimate the unknown grids\nresp &lt;- predict(k, coop)\n\n\n[using ordinary kriging]\n\n\nshow code\nresp$x &lt;- st_coordinates(resp)[,1]\nresp$y &lt;- st_coordinates(resp)[,2]\nresp$pred &lt;- resp$var1.pred\nresp$pred &lt;- resp$pred\n\n# create a raster surface data object\nkpred &lt;- rasterize(resp, grid, \n                         field = \"pred\")\n\n# map the interpolated variable raster, kpred\nmain_title &lt;- ifelse(selected_var == \"Daily Rainfall Total (mm)\",\n                paste(\"Distribution for Total Rainfall (mm) for\", selected_month, selected_year),\n                paste(\"Distribution for Average Mean Temperature (°C) for\", selected_month, selected_year))\nraster_title &lt;- ifelse(selected_var == \"Daily Rainfall Total (mm)\",\n                paste(\"Total Monthly Rainfall (mm)\"),\n                paste(\"Average Mean Temperature (°C)\"))\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kpred) + \n  tm_raster(alpha = 0.6, \n            palette = \"viridis\",\n            title = raster_title) +\n  tm_layout(main.title = main_title,\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n\n\n\n\n\n\n4.1.5.3 Prediction variance\n\n\nshow code\n# Assuming resp already contains the predictions and variance from the predict function\n# Extracting the variance\nresp$variance &lt;- resp$var1.var\n\n# Create a raster surface data object for variance\nkvar &lt;- rasterize(resp, grid, field = \"variance\")\n\n# Map the interpolated variance raster, kvar\nvariance_title &lt;- \"Prediction Variance\"\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\ntm_shape(kvar) + \n  tm_raster(title = variance_title,\n            alpha = 0.6, palette = \"viridis\") +\n  tm_layout(main.title = \"Kriging Prediction Variance\", main.title.position = \"center\", main.title.size = 1.2, \n            legend.height = 0.45, legend.width = 0.35, frame = TRUE) +\n  tm_compass(type=\"8star\", size = 2) + tm_scale_bar() + tm_grid(alpha =0.2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#ui-design",
    "href": "Take-home_Ex/Take-home_Ex4/Take-home_Ex4.html#ui-design",
    "title": "Take-home Exercise 4",
    "section": "4.2 UI Design",
    "text": "4.2 UI Design\nIn this section we will describe the parameters to expose, the output to show, while suggesting the Shiny UI components to use.\n\n4.2.1 Parameters to expose\nThe parameters to be exposed can be categorized into 2 types.\n\nData Selection Parameters:\n\n\nChoose Variable: Single select option to choose between the variables to analyse (Daily Rainfall Total (mm), Mean Temperature (°C))\nTime Resolution: Single select option for temporal resolution for analysis (Day, Month, Year).\n\nThe date picker should show based on the Time Resolution selected.\n\n\n\nModel Parameters for Spatial Interpolation:\n\nIDW:\n\nSelect resolution\nSelect nmax\n\nOrdinary Kriging:\n\nSelect resolution\nSelect model\nSelect psill\nSelect range\nSelect nugget\n\n\n\n\n\n4.2.2 Proposed layout (with parameters and outputs)\nThe module will have a single page, with option to scroll down to test both spatial interpolation methods. Below we show screenshot of the layout, describe the parameters and description of the Shiny UI function selected, and describe the output plots to be shown for each method.\n\n4.2.2.1 Map of Data\n\nParameters\n(1) Choose variable: Selection of variable to plot. Use radioButton function.\n(2) Time resolution: Choose either Day, Month, Year using radioButton function.\n(3) This parameter is a Dynamic UI, based on the output of the previous parameter. It will use uiOutput in UI code and renderUI in server to achieve this conditional display of date selection.\nDetails in the code chunk below:\n\nui &lt;- fluidPage(\n  fluidRow(\n    box(title = \"Data Selection Parameters\",  width = 2, status = \"primary\", solidHeader = TRUE,\n        radioButtons(\"GS_selected_var\", \"Choose variable\", variables),\n        radioButtons(\"GS_time_resolution\", \"Time resolution\", c(\"Day\", \"Month\", \"Year\")),\n        uiOutput(\"GS_dynamic_time_resolution\"), \n        actionButton(\"GS_updatetmap\", \"Show map\")\n      \n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n    output$GS_dynamic_time_resolution &lt;- renderUI({\n    if (input$GS_time_resolution == \"Day\") {\n        airDatepickerInput(\"GS_selected_date\", label = \"Select date\", value = \"2021-01-01\", maxDate = \"2023-12-31\", minDate = \"2021-01-01\", dateFormat = \"yyyy-MM-dd\")\n    } else if (input$GS_time_resolution == \"Month\") {\n        airMonthpickerInput(\"GS_selected_month\", label = \"Select month and year\", value = \"2021-01-01\", maxDate = \"2023-12-31\", minDate = \"2021-01-01\", dateFormat = \"yyyy-MM\")\n    } else if (input$GS_time_resolution == \"Year\") {\n      airYearpickerInput(\"GS_selected_year\", label = \"Select year\", value = \"2021-01-01\", maxDate = \"2023-12-31\", minDate = \"2021-01-01\", dateFormat = \"yyyy\")\n    }\n  })\n}\n\nOutput\n(A) An interactive tmap() showing the data collected for the value at each of the stations is displayed.\n\n\n4.2.2.2 IDW Method\n\nParameters\n(1) Show IDW?: This option will trigger whether to display the below parameters for the method and the results of the method.\n(2) Resolution: Choose resolution of the map using sliderInput function.\n(3) nmax: Choose nmax of the map using sliderInput function.\nDetails of the above are in the code chunk below:\n\nui &lt;- fluidPage(\n  fluidRow(\n    box(title = \"IDW Selection Parameters\",  width = 2, status = \"primary\", solidHeader = TRUE,\n        radioButtons(\"GS_show_IDW\", \"Show IDW?\", c(\"Yes\", \"No\"), selected = \"Yes\", inline = TRUE),\n        uiOutput(\"GS_dynamic_IDW\"),\n        actionButton(\"GS_updateIDW\", \"Show Result\")\n        \n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n    output$GS_dynamic_IDW &lt;- renderUI({\n    if (input$GS_show_IDW == \"Yes\") {\n      list(\n        sliderInput(\"GS_IDW_res\", \"Resolution\", \n                    min = 30, max = 80, value = 50),\n        sliderInput(\"GS_nmax\" , \"nmax\",\n                    min = 1, max = 10, value = 3)\n      )\n    }\n    \n  })\n}\n\nOutput\n(A) A static map showing the result of the interpolation method is displayed.\n\n\n4.2.2.3 Ordinary Kriging Method\n\nParameters\n(1) Show Kriging?: This option will trigger whether to display the below parameters for the method and the results of the method.\n(2) Resolution: Choose resolution of the map using sliderInput function.\n(3) model: Choose model of the map using selectInput function.\n(4) psill: Choose psill of the map using sliderInput function.\n(5) range: Choose range of the map using sliderInput function.\n(6) nugget: Choose nugget of the map using sliderInput function.\nDetails of the above are in the code chunk below:\n\nui &lt;- fluidPage(\n  fluidRow(\n    box(title = \"Kriging Selection Parameters\",  width = 2, status = \"primary\", solidHeader = TRUE,\n        radioButtons(\"GS_show_OK\", \"Show Kriging?\", c(\"Yes\", \"No\"), selected = \"Yes\", inline = TRUE),\n        uiOutput(\"GS_dynamic_OK\"),\n        actionButton(\"GS_updateOK\", \"Show Result\")\n        \n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  # OK\n  output$GS_dynamic_OK &lt;- renderUI({\n    if (input$GS_show_OK == \"Yes\") {\n      model_choices &lt;- c(\"Nug\", \"Exp\",\"Sph\",\"Gau\",\"Exc\",\"Mat\",\"Ste\",\"Cir\",\"Lin\",\"Bes\",\"Pen\",\"Per\",\"Wav\",\"Hol\",\"Log\",\"Pow\",\"Spl\")\n      # min_nugget &lt;- min(variable_data$ValueToPlot, na.rm = TRUE)\n      # max_nugget &lt;- max(variable_data$ValueToPlot, na.rm = TRUE)\n      # mid_nugget  &lt;- mean(variable_data$ValueToPlot, na.rm = TRUE)\n      list(\n        sliderInput(\"GS_OK_res\", \"Resolution\", min = 30, max = 80, value = 50),\n        selectInput(\"GS_model\" , \"model\", model_choices),\n        sliderInput(\"GS_OK_psill\", \"psill\", min = 0.5, max = 10, value = 0.5, step = 0.5),\n        sliderInput(\"GS_OK_range\", \"range\", min = 500, max = 10000, value = 8000, step = 500),\n        sliderInput(\"GS_OK_nugget\", \"nugget\", min = 0.1, max = 10, value = 0.1, step = 0.1)\n      )\n    }\n    \n  })\n}\n\nOutput\n(A) A static plot showing the experimental variogram displayed.\n(B) A static plot showing the fitted variogram displayed. This will vary based on the user selection of method parameters.\n(C) A static map showing the result of the interpolation method is displayed. This will vary based on the user selection of method parameters.\n(D) A static map showing the prediction variance of the interpolation method is displayed. This will vary based on the user selection of method parameters.\nThings to work on\n\nShould show result of using autofitVariogram() of automap package so that user can tweak their parameters to obtain result\nMay need to further refine the selection of range of values and steps for the nugget, psill, range parameters\nShould show the table of fv generated when fitting variogram, like in the screenshot below:"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex9/In-class_Ex9.html",
    "href": "In-class_Ex/In-class_Ex9/In-class_Ex9.html",
    "title": "In-class Exercise 9",
    "section": "",
    "text": "To visualize and analyse network data, using R and appropriate R packages."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#overview",
    "href": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#overview",
    "title": "In-class Exercise 9",
    "section": "",
    "text": "To visualize and analyse network data, using R and appropriate R packages."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#load-libraries",
    "href": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#load-libraries",
    "title": "In-class Exercise 9",
    "section": "Load libraries",
    "text": "Load libraries\n\npacman::p_load(igraph, tidygraph, ggraph, visNetwork, lubridate, \n               clock, # only for time\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#import-data",
    "href": "In-class_Ex/In-class_Ex9/In-class_Ex9.html#import-data",
    "title": "In-class Exercise 9",
    "section": "Import data",
    "text": "Import data\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\nTransform SentDate column to Date type and create a new Weekday column:\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(Weekday = wday(SentDate, # Create new column , Weekday\n                        label = TRUE,\n                        abbr = FALSE)) %&gt;%\n  mutate(SendDate = dmy(SentDate))  # Change type to date and create a new column, Senddate\n\nAggregate the individual by date, senders, receivers, main subject and day of the week:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;% # Focus only on rows that are 'Work related'\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;% # Add new field, Weight. Assigned based on summarise function..?\n  filter(source!=target) %&gt;% # Exclude emails where recipient and sender are the same. \n  filter(Weight &gt; 1) %&gt;% \n  ungroup()\n\nUse tbl_graph() to build tidygraph data model:\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes, edges = GAStech_edges_aggregated, directed = TRUE)\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 Sunday       5\n2     1     2 Monday       2\n3     1     2 Tuesday      3\n# ℹ 1,369 more rows\n\n\nPlot basic network graph:\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\n\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\n\nTry tweaking the layout as per this documentation.\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"kk\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\ng &lt;- ggraph(GAStech_graph,\n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, size = 3))\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\ng &lt;-ggraph(GAStech_graph,\n           layout = \"nicely\") +\n  geom_edge_link(aes(width = Weight),\n                 alpha = 0.2) +\n  scale_edge_width(range = c(0.1,5)) +\n  geom_node_point(aes(colour = Department),\n                  size = 3)\n\n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\n\nAnalysis: Computing centrality indices:\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "In this exercise I will:\n\ncreate bullet chart by using ggplot2,\ncreate sparklines by using ggplot2 ,\nbuild industry standard dashboard by using R Shiny."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#overview",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#overview",
    "title": "Hands-on Exercise 10",
    "section": "",
    "text": "In this exercise I will:\n\ncreate bullet chart by using ggplot2,\ncreate sparklines by using ggplot2 ,\nbuild industry standard dashboard by using R Shiny."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#loading-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#loading-libraries",
    "title": "Hands-on Exercise 10",
    "section": "2 Loading libraries",
    "text": "2 Loading libraries\n\npacman::p_load(lubridate, ggthemes, reactable,\nreactablefmtr, gt, gtExtras, tidyverse, svglite)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#the-data-set",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#the-data-set",
    "title": "Hands-on Exercise 10",
    "section": "3 The data set",
    "text": "3 The data set\n\n3.1 Importing data\n\ncoffeechain &lt;- read_rds(\"data/rds/CoffeeChain.rds\")\n\n\n\n3.2 Data preparation\nAggregate Sales and Budgeted Sales at the Product level.\n\nproduct &lt;- coffeechain %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`target` = sum(`Budget Sales`),\n            `current` = sum(`Sales`)) %&gt;%\n  ungroup()\n\n\n\n3.3 Bullet chart in ggplot2\n\nggplot(product, aes(Product, current)) + \n  geom_col(aes(Product, max(target) * 1.01),\n           fill=\"grey85\", width=0.85) +\n  geom_col(aes(Product, target * 0.75),\n           fill=\"grey60\", width=0.85) +\n  geom_col(aes(Product, target * 0.5),\n           fill=\"grey50\", width=0.85) +\n  geom_col(aes(Product, current), \n           width=0.35,\n           fill = \"black\") + \n  geom_errorbar(aes(y = target,\n                    x = Product, \n                    ymin = target,\n                    ymax= target), \n                width = .4,\n                colour = \"red\",\n                size = 1) +\n  coord_flip()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#plotting-sparklines-using-ggplot2",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#plotting-sparklines-using-ggplot2",
    "title": "Hands-on Exercise 10",
    "section": "4 Plotting sparklines using ggplot2",
    "text": "4 Plotting sparklines using ggplot2\n\n4.1 Prepare data\n\nsales_report &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  mutate(Month = month(Date)) %&gt;%\n  group_by(Month, Product) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup() %&gt;%\n  select(Month, Product, Sales)\n\nCompute the minimum, maximum and end of the month sales:\n\nmins &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.min(Sales))\nmaxs &lt;- group_by(sales_report, Product) %&gt;% \n  slice(which.max(Sales))\nends &lt;- group_by(sales_report, Product) %&gt;% \n  filter(Month == max(Month))\n\nCompute the 25 and 75 quantiles:\n\nquarts &lt;- sales_report %&gt;%\n  group_by(Product) %&gt;%\n  summarise(quart1 = quantile(Sales, \n                              0.25),\n            quart2 = quantile(Sales, \n                              0.75)) %&gt;%\n  right_join(sales_report)\n\n\n\n4.2 Sparklines in ggplot2\n\nggplot(sales_report, aes(x=Month, y=Sales)) + \n  facet_grid(Product ~ ., scales = \"free_y\") + \n  geom_ribbon(data = quarts, aes(ymin = quart1, max = quart2), \n              fill = 'grey90') +\n  geom_line(size=0.3) +\n  geom_point(data = mins, col = 'red') +\n  geom_point(data = maxs, col = 'blue') +\n  geom_text(data = mins, aes(label = Sales), vjust = -1) +\n  geom_text(data = maxs, aes(label = Sales), vjust = 2.5) +\n  geom_text(data = ends, aes(label = Sales), hjust = 0, nudge_x = 0.5) +\n  geom_text(data = ends, aes(label = Product), hjust = 0, nudge_x = 1.0) +\n  expand_limits(x = max(sales_report$Month) + \n                  (0.25 * (max(sales_report$Month) - min(sales_report$Month)))) +\n  scale_x_continuous(breaks = seq(1, 12, 1)) +\n  scale_y_continuous(expand = c(0.1, 0)) +\n  theme_tufte(base_size = 3, base_family = \"Helvetica\") +\n  theme(axis.title=element_blank(), axis.text.y = element_blank(), \n        axis.ticks = element_blank(), strip.text = element_blank())"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#static-information-dashboard-design-gt-and-gtextras-methods",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#static-information-dashboard-design-gt-and-gtextras-methods",
    "title": "Hands-on Exercise 10",
    "section": "5 Static Information Dashboard Design: gt and gtExtras methods",
    "text": "5 Static Information Dashboard Design: gt and gtExtras methods\n\n5.1 Plotting a simple bullet chart\n\nproduct %&gt;%\n  gt::gt() %&gt;%\n  gt_plt_bullet(column = current, \n              target = target, \n              width = 60,\n              palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\nProduct\ncurrent\n\n\n\n\nAmaretto\n\n\n\n   \n\n\n\nCaffe Latte\n\n\n\n   \n\n\n\nCaffe Mocha\n\n\n\n   \n\n\n\nChamomile\n\n\n\n   \n\n\n\nColombian\n\n\n\n   \n\n\n\nDarjeeling\n\n\n\n   \n\n\n\nDecaf Espresso\n\n\n\n   \n\n\n\nDecaf Irish Cream\n\n\n\n   \n\n\n\nEarl Grey\n\n\n\n   \n\n\n\nGreen Tea\n\n\n\n   \n\n\n\nLemon\n\n\n\n   \n\n\n\nMint\n\n\n\n   \n\n\n\nRegular Espresso"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#sparklines-gtextras-method",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#sparklines-gtextras-method",
    "title": "Hands-on Exercise 10",
    "section": "6 sparklines: gtExtras method",
    "text": "6 sparklines: gtExtras method\nPrepare data:\n\nreport &lt;- coffeechain %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  filter(Year == \"2013\") %&gt;%\n  mutate (Month = month(Date, \n                        label = TRUE, \n                        abbr = TRUE)) %&gt;%\n  group_by(Product, Month) %&gt;%\n  summarise(Sales = sum(Sales)) %&gt;%\n  ungroup()\n\nIt is important to note that one of the requirement of gtExtras functions is that almost exclusively they require you to pass data.frame with list columns. In view of this, code chunk below will be used to convert the report data.frame into list columns.\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\n# A tibble: 13 × 2\n   Product           `Monthly Sales`\n   &lt;chr&gt;             &lt;list&gt;         \n 1 Amaretto          &lt;dbl [12]&gt;     \n 2 Caffe Latte       &lt;dbl [12]&gt;     \n 3 Caffe Mocha       &lt;dbl [12]&gt;     \n 4 Chamomile         &lt;dbl [12]&gt;     \n 5 Colombian         &lt;dbl [12]&gt;     \n 6 Darjeeling        &lt;dbl [12]&gt;     \n 7 Decaf Espresso    &lt;dbl [12]&gt;     \n 8 Decaf Irish Cream &lt;dbl [12]&gt;     \n 9 Earl Grey         &lt;dbl [12]&gt;     \n10 Green Tea         &lt;dbl [12]&gt;     \n11 Lemon             &lt;dbl [12]&gt;     \n12 Mint              &lt;dbl [12]&gt;     \n13 Regular Espresso  &lt;dbl [12]&gt;     \n\n\n\n6.1 Plotting coffee chain sales report\n\nreport %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\") %&gt;%\n   gt() %&gt;%\n   gt_plt_sparkline('Monthly Sales',\n                    same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMonthly Sales\n\n\n\n\nAmaretto\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n\n\n\n   3.7K\n\n\n\nChamomile\n\n\n\n   3.3K\n\n\n\nColombian\n\n\n\n   5.5K\n\n\n\nDarjeeling\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n\n\n\n   2.7K\n\n\n\nEarl Grey\n\n\n\n   3.0K\n\n\n\nGreen Tea\n\n\n\n   1.5K\n\n\n\nLemon\n\n\n\n   4.4K\n\n\n\nMint\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n6.2 Adding statistics\nCalculate summary statistics:\n\nreport %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            ) %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = 4,\n    decimals = 2)\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\n\n\n\n\nAmaretto\n1016\n1210\n1,119.00\n\n\nCaffe Latte\n1398\n1653\n1,528.33\n\n\nCaffe Mocha\n3322\n3828\n3,613.92\n\n\nChamomile\n2967\n3395\n3,217.42\n\n\nColombian\n5132\n5961\n5,457.25\n\n\nDarjeeling\n2926\n3281\n3,112.67\n\n\nDecaf Espresso\n3181\n3493\n3,326.83\n\n\nDecaf Irish Cream\n2463\n2901\n2,648.25\n\n\nEarl Grey\n2730\n3005\n2,841.83\n\n\nGreen Tea\n1339\n1476\n1,398.75\n\n\nLemon\n3851\n4418\n4,080.83\n\n\nMint\n1388\n1669\n1,519.17\n\n\nRegular Espresso\n890\n1218\n1,023.42\n\n\n\n\n\n\n\n\n\n6.3 Combining the data frame\n\nspark &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize('Monthly Sales' = list(Sales), \n            .groups = \"drop\")\n\nsales &lt;- report %&gt;% \n  group_by(Product) %&gt;% \n  summarise(\"Min\" = min(Sales, na.rm = T),\n            \"Max\" = max(Sales, na.rm = T),\n            \"Average\" = mean(Sales, na.rm = T)\n            )\n\nsales_data = left_join(sales, spark)\n\n\n\n6.4 Plot updated data.table\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales',\n                   same_limit = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K\n\n\n\n\n\n\n\n\n\n\n6.5 Combining bullet chart and sparklines\n\nbullet &lt;- coffeechain %&gt;%\n  filter(Date &gt;= \"2013-01-01\") %&gt;%\n  group_by(`Product`) %&gt;%\n  summarise(`Target` = sum(`Budget Sales`),\n            `Actual` = sum(`Sales`)) %&gt;%\n  ungroup() \n\nsales_data = sales_data %&gt;%\n  left_join(bullet)\n\nsales_data %&gt;%\n  gt() %&gt;%\n  gt_plt_sparkline('Monthly Sales') %&gt;%\n  gt_plt_bullet(column = Actual, \n                target = Target, \n                width = 28,\n                palette = c(\"lightblue\", \n                          \"black\")) %&gt;%\n  gt_theme_538()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProduct\nMin\nMax\nAverage\nMonthly Sales\nActual\n\n\n\n\nAmaretto\n1016\n1210\n1119.000\n\n\n\n   1.2K\n\n\n\n\n   \n\n\n\nCaffe Latte\n1398\n1653\n1528.333\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nCaffe Mocha\n3322\n3828\n3613.917\n\n\n\n   3.7K\n\n\n\n\n   \n\n\n\nChamomile\n2967\n3395\n3217.417\n\n\n\n   3.3K\n\n\n\n\n   \n\n\n\nColombian\n5132\n5961\n5457.250\n\n\n\n   5.5K\n\n\n\n\n   \n\n\n\nDarjeeling\n2926\n3281\n3112.667\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nDecaf Espresso\n3181\n3493\n3326.833\n\n\n\n   3.2K\n\n\n\n\n   \n\n\n\nDecaf Irish Cream\n2463\n2901\n2648.250\n\n\n\n   2.7K\n\n\n\n\n   \n\n\n\nEarl Grey\n2730\n3005\n2841.833\n\n\n\n   3.0K\n\n\n\n\n   \n\n\n\nGreen Tea\n1339\n1476\n1398.750\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nLemon\n3851\n4418\n4080.833\n\n\n\n   4.4K\n\n\n\n\n   \n\n\n\nMint\n1388\n1669\n1519.167\n\n\n\n   1.5K\n\n\n\n\n   \n\n\n\nRegular Espresso\n890\n1218\n1023.417\n\n\n\n   1.1K"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-methods",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#interactive-information-dashboard-design-reactable-and-reactablefmtr-methods",
    "title": "Hands-on Exercise 10",
    "section": "7 Interactive Information Dashboard Design: reactable and reactablefmtr methods",
    "text": "7 Interactive Information Dashboard Design: reactable and reactablefmtr methods\n\n7.1 Load package\n\nlibrary(dataui)\n\n\n\n7.2 Plotting interactive sparklines\n\n\nCode\n# Prepare list field\nreport &lt;- report %&gt;%\n  group_by(Product) %&gt;%\n  summarize(`Monthly Sales` = list(Sales))\n\n# Plot sparklines\nreactable(\n  report,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\n\n\n7.3 Changing the page size\nDefault page size is 10, we can change it as follows:\n\n\nCode\nreactable(\n  report,\n  defaultPageSize = 13, # Change page size\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(report)\n    )\n  )\n)\n\n\n\n\n\n\n\n\n7.4 Adding points and labels\nhighlight_points argument is used to show the minimum and maximum values points and label argument is used to label first and last values:\n\n\nCode\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        labels = c(\"first\", \"last\")\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n7.5 Adding reference line\nstatline argument is used to show the mean line:\n\n\nCode\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        statline = \"mean\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n7.6 Adding bandline\n\n\nCode\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkline(\n        report,\n        highlight_points = highlight_points(\n          min = \"red\", max = \"blue\"),\n        line_width = 1,\n        bandline = \"innerquartiles\",\n        bandline_color = \"green\"\n        )\n    )\n  )\n)\n\n\n\n\n\n\n\n\n7.7 Changing from sparkline to sparkbar\n\n\nCode\nreactable(\n  report,\n  defaultPageSize = 13,\n  columns = list(\n    Product = colDef(maxWidth = 200),\n    `Monthly Sales` = colDef(\n      cell = react_sparkbar(\n        report,\n        highlight_bars = highlight_bars(\n          min = \"red\", max = \"blue\"),\n        bandline = \"innerquartiles\",\n        statline = \"mean\")\n    )\n  )\n)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#references",
    "href": "Hands-on_Ex/Hands-on_Ex10/Hands-on_Ex10.html#references",
    "title": "Hands-on Exercise 10",
    "section": "8 References",
    "text": "8 References\n\nR for Visual Analytics: Chapter 31: Information Dashboard Design: R methods"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\n\ntidyverse, and\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven)\n\n\n\n\n\n\n\nNote\n\n\n\npacman::p_load(tidyverse, haven) is calling the pacman package, since we have not yet loaded this package. If pacman package was already loaded, we can load tidyverse and haven with p_load(tidyverse, haven)."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#loading-r-packages",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#loading-r-packages",
    "title": "In-class Exercise 1",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\n\ntidyverse, and\nhaven\n\nThe code chunk used is as follows:\n\npacman::p_load(tidyverse, haven)\n\n\n\n\n\n\n\nNote\n\n\n\npacman::p_load(tidyverse, haven) is calling the pacman package, since we have not yet loaded this package. If pacman package was already loaded, we can load tidyverse and haven with p_load(tidyverse, haven)."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-pisa-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-pisa-data",
    "title": "In-class Exercise 1",
    "section": "2 Importing PISA data",
    "text": "2 Importing PISA data\nThe code chunk below uses read_sas() of haven to import PISA data into R environment.\n\nstu_qqq &lt;- read_sas(\"data/cy08msp_stu_qqq.sas7bdat\")\n\nThe code chunk below filters the data where country code (CNT) is Singapore (SGP).\n\nstu_qqq_SG &lt;- stu_qqq %&gt;% \n  filter(CNT == \"SGP\")\n\nThe code chunk below then saves this filtered data as an RData file. This smaller file can be used in the future.\n\nwrite_rds(stu_qqq_SG, \"data/stu_qqq_SG.rds\")\n\nTo load this file in the future, follow the code in the code chunk below.\n\nstu_qqq_SG &lt;- read_rds(\"data/stu_qqq_SG.rds\")"
  }
]